{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Duckalog Documentation","text":"<p>Welcome to the Duckalog documentation. Duckalog is a Python library and CLI for building DuckDB catalogs from declarative YAML/JSON configuration files.</p> <p>Use these docs to:</p> <ul> <li>Understand the core concepts behind Duckalog configs.</li> <li>Get started with the CLI and Python API.</li> <li>Find API reference information generated from the source code.</li> <li>Learn about the system architecture and design patterns.</li> </ul> <p>For a deeper product and technical description, see the Architecture in <code>architecture.md</code>.</p>"},{"location":"#quick-start-guide-a-realistic-example","title":"Quick Start Guide: A Realistic Example","text":"<p>Let's walk through a typical analytics scenario where you need to combine data from multiple sources: Parquet files in S3, a reference database (DuckDB), and an Iceberg table. We'll create a unified catalog that joins these datasources.</p>"},{"location":"#step-1-set-up-your-configuration","title":"Step 1: Set up your configuration","text":"<p>First, create a comprehensive config file called <code>analytics_catalog.yaml</code>:</p> <pre><code>version: 1\n\nduckdb:\n  database: analytics.duckdb\n  pragmas:\n    - \"SET memory_limit='2GB'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n\n# Attach our reference data database\nattachments:\n  duckdb:\n    - alias: refdata\n      path: ./reference_data.duckdb\n      read_only: true\n\n# Configure Iceberg catalog access\niceberg_catalogs:\n  - name: analytics_warehouse\n    catalog_type: rest\n    uri: \"https://iceberg-catalog.example.com\"\n    warehouse: \"s3://our-warehouse/analytics/\"\n    options:\n      token: \"${env:ICEBERG_TOKEN}\"\n\nviews:\n  # Raw events from S3 Parquet files\n  - name: raw_events\n    source: parquet\n    uri: \"s3://our-bucket/events/*.parquet\"\n\n  # Product data from Iceberg\n  - name: products\n    source: iceberg\n    catalog: analytics_warehouse\n    table: analytics.products\n\n  # Customer reference data from attached database\n  - name: customers\n    source: duckdb\n    database: refdata\n    table: customers\n\n  # Enhanced events with joined data\n  - name: enhanced_events\n    sql: |\n      SELECT \n        e.event_id,\n        e.timestamp,\n        e.user_id,\n        e.event_type,\n        c.name as customer_name,\n        c.segment as customer_segment,\n        p.category as product_category,\n        p.price as product_price\n      FROM raw_events e\n      JOIN customers c ON e.customer_id = c.id\n      LEFT JOIN products p ON e.product_id = p.id\n\n  # Daily aggregation\n  - name: daily_metrics\n    sql: |\n      SELECT \n        DATE(timestamp) as event_date,\n        event_type,\n        COUNT(*) as event_count,\n        COUNT(DISTINCT user_id) as unique_users,\n        AVG(product_price) as avg_product_value\n      FROM enhanced_events\n      GROUP BY DATE(timestamp), event_type\n</code></pre>"},{"location":"#step-2-set-environment-variables","title":"Step 2: Set environment variables","text":"<p>Before running Duckalog, set the required environment variables:</p> <pre><code>export AWS_ACCESS_KEY_ID=\"your_aws_key\"\nexport AWS_SECRET_ACCESS_KEY=\"your_aws_secret\"\nexport ICEBERG_TOKEN=\"your_iceberg_token\"\n</code></pre>"},{"location":"#step-3-validate-and-build-your-catalog","title":"Step 3: Validate and build your catalog","text":"<p>First, let's validate the configuration to catch any issues early:</p> <pre><code>duckalog validate analytics_catalog.yaml\n</code></pre> <p>If validation passes, build the catalog:</p> <pre><code>duckalog build analytics_catalog.yaml\n</code></pre> <p>This will create the <code>analytics.duckdb</code> file with all your views properly configured.</p>"},{"location":"#step-4-use-your-catalog","title":"Step 4: Use your catalog","text":"<p>Now you can query your unified data source:</p> <pre><code># Connect directly with DuckDB CLI\nduckdb analytics.duckdb -c \"SELECT * FROM daily_metrics ORDER BY event_date DESC LIMIT 10\"\n\n# Or use duckalog to generate SQL for inspection\nduckalog generate-sql analytics_catalog.yaml --output create_views.sql\n</code></pre>"},{"location":"#step-5-programmatically-interact-with-your-catalog","title":"Step 5: Programmatically interact with your catalog","text":"<p>You can also use the Python API for more advanced workflows:</p> <pre><code>from duckalog import build_catalog, generate_sql, load_config\nimport polars as pl\nimport duckdb\n\n# Load configuration\nconfig = load_config(\"analytics_catalog.yaml\")\n\n# Generate SQL without execution\nsql = generate_sql(\"analytics_catalog.yaml\")\nprint(\"SQL to be executed:\")\nprint(sql)\n\n# Build catalog programmatically\nbuild_catalog(\"analytics_catalog.yaml\")\n\n# Query with DuckDB directly\ncon = duckdb.connect(\"analytics.duckdb\")\n\n# Get daily metrics as DataFrame\nmetrics_df = con.execute(\"SELECT * FROM daily_metrics WHERE event_date &gt;= CURRENT_DATE - INTERVAL 7 DAYS\").df()\nprint(f\"Last 7 days of metrics: {metrics_df}\")\n\n# Get enhanced events for a specific user\nuser_events = con.execute(\"SELECT * FROM enhanced_events WHERE user_id = 'user123'\").df()\nprint(f\"User events: {user_events}\")\n\ncon.close()\n</code></pre> <p>This example demonstrates how Duckalog can help you create a cohesive analytics layer that combines data from multiple sources, applies business logic, and provides a single point of access for your data consumers.</p>"},{"location":"#path-resolution-feature","title":"Path Resolution Feature","text":"<p>Duckalog includes automatic path resolution that converts relative file paths to absolute paths relative to the configuration file location. This provides consistent behavior across different working directories while maintaining security validation.</p>"},{"location":"#key-benefits","title":"Key Benefits","text":"<ul> <li>Portability: Configurations can be moved between environments without breaking file references</li> <li>Security: Built-in validation prevents directory traversal attacks while allowing reasonable parent directory access</li> <li>Consistency: Paths are resolved consistently regardless of the current working directory</li> <li>Flexibility: Works with relative paths, absolute paths, and remote URIs</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n\nviews:\n  - name: events\n    source: parquet\n    uri: data/events.parquet        # Automatically resolved to absolute path\n\n  - name: reference\n    source: parquet  \n    uri: ../shared/data/users.parquet  # Parent directory access allowed\n</code></pre> <p>The paths above are automatically resolved relative to the configuration file location when loaded.</p> <p>Learn more: Path Resolution Guide</p>"},{"location":"#quick-reference","title":"Quick Reference","text":""},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>System Architecture - Understanding Duckalog's design, components, and patterns</li> <li>Path Resolution Guide - Automatic path resolution and security features</li> <li>User Guide - Configuration patterns and troubleshooting</li> </ul>"},{"location":"#install","title":"Install","text":"<pre><code>pip install duckalog\n</code></pre>"},{"location":"#basic-commands","title":"Basic Commands","text":"<pre><code># Build a catalog from configuration\nduckalog build catalog.yaml\n\n# Generate SQL without executing it\nduckalog generate-sql catalog.yaml --output create_views.sql\n\n# Validate configuration syntax\nduckalog validate catalog.yaml\n</code></pre>"},{"location":"#python-api","title":"Python API","text":"<pre><code>from duckalog import build_catalog, generate_sql, validate_config\n\n# Build or update a catalog\nbuild_catalog(\"catalog.yaml\")\n\n# Generate SQL without execution\nsql = generate_sql(\"catalog.yaml\")\n\n# Validate configuration\nvalidate_config(\"catalog.yaml\")\n</code></pre> <p>For a deeper product and technical description, see the PRD in <code>docs/PRD_Spec.md</code>.</p>"},{"location":"SECURITY/","title":"Security Documentation","text":"<p>This document outlines the security features and best practices for Duckalog, including both web UI security and path resolution security features.</p>"},{"location":"SECURITY/#overview","title":"Overview","text":"<p>Duckalog includes comprehensive security hardening across multiple components:</p> <ul> <li>Web UI Security: Read-only SQL enforcement, authentication, and CORS protection</li> <li>Path Resolution Security: Validation against directory traversal and system access</li> <li>Configuration Security: Secure handling of credentials and file operations</li> </ul>"},{"location":"SECURITY/#sql-injection-protection","title":"\ud83d\udee1\ufe0f SQL Injection Protection","text":"<p>Duckalog implements comprehensive SQL injection protection through canonical quoting helpers and strict input validation.</p>"},{"location":"SECURITY/#sql-quoting-strategy","title":"SQL Quoting Strategy","text":"<p>Duckalog uses two canonical functions for safe SQL construction:</p>"},{"location":"SECURITY/#quote_ident-for-database-identifiers","title":"quote_ident() - For Database Identifiers","text":"<ul> <li>Purpose: Quote database, table, column, and view names</li> <li>Protection: Prevents identifier injection through proper escaping</li> <li>Usage: <code>view.database</code>, <code>view.table</code>, attachment aliases, catalog names</li> </ul> <pre><code>from duckalog import quote_ident\n\n# Safe identifier quoting\nquote_ident(\"my_table\")         # Returns: \"my_table\"\nquote_ident(\"table; DROP...\")   # Returns: \"table; DROP...\" (escaped, not executed)\nquote_ident('user \"events\"')    # Returns: \"user \"\"events\"\"\" (quotes escaped)\n</code></pre>"},{"location":"SECURITY/#quote_literal-for-string-values","title":"quote_literal() - For String Values","text":"<ul> <li>Purpose: Quote string literals in SQL (paths, secrets, connection strings)</li> <li>Protection: Prevents SQL injection through proper string escaping</li> <li>Usage: File paths, secret values, connection strings, scope values</li> </ul> <pre><code>from duckalog import quote_literal\n\n# Safe literal quoting\nquote_literal(\"user's data\")      # Returns: \"'user''s data'\"\nquote_literal(\"path/to/file\")     # Returns: \"'path/to/file'\"\nquote_literal(\"SELECT * FROM...\") # Returns: \"'SELECT * FROM...'\" (not executed)\n</code></pre>"},{"location":"SECURITY/#injection-attack-prevention","title":"Injection Attack Prevention","text":""},{"location":"SECURITY/#view-sql-injection-protection","title":"View SQL Injection Protection","text":"<pre><code># \u274c MALICIOUS INPUT - BLOCKED\nviews:\n  - name: evil\n    source: duckdb\n    database: '\"; DROP TABLE users; --'\n    table: 'bad_table'\n    # Generated SQL: SELECT * FROM \"\"; DROP TABLE users; --\".\"bad_table\"\n    # The malicious SQL is safely quoted as identifiers, not executed\n\n# \u2705 LEGITIMATE INPUT - ALLOWED  \nviews:\n  - name: sales_data\n    source: duckdb\n    database: my_database\n    table: orders_2024\n    # Generated SQL: SELECT * FROM \"my_database\".\"orders_2024\"\n</code></pre>"},{"location":"SECURITY/#secret-sql-injection-protection","title":"Secret SQL Injection Protection","text":"<pre><code># \u274c MALICIOUS SECRET VALUE - BLOCKED\nsecrets:\n  - type: s3\n    name: evil_secret\n    key_id: 'user'' OR 1=1 --'\n    secret: 'malicious_value'\n    # Generated SQL safely escapes quotes:\n    # KEY_ID 'user'' OR 1=1 --' (not executed as SQL)\n    # SECRET 'malicious_value' (safe)\n\n# \u2705 LEGITIMATE SECRET - ALLOWED\nsecrets:\n  - type: s3\n    name: prod_s3\n    key_id: AKIA123456789\n    secret: secret_key_abc123\n    # Generated SQL: KEY_ID 'AKIA123456789', SECRET 'secret_key_abc123'\n</code></pre>"},{"location":"SECURITY/#path-sql-injection-protection","title":"Path SQL Injection Protection","text":"<pre><code># \u274c MALICIOUS PATH - BLOCKED\npath = \"../../../etc/passwd\"\nsql_path = normalize_path_for_sql(path)\n# Returns: \"'../../../etc/passwd'\" (safe string literal)\n\n# \u2705 LEGITIMATE PATH - ALLOWED\npath = \"data/file.parquet\"  \nsql_path = normalize_path_for_sql(path)\n# Returns: \"'data/file.parquet'\" (safe)\n</code></pre>"},{"location":"SECURITY/#strict-type-enforcement","title":"Strict Type Enforcement","text":"<p>Duckalog enforces strict type checking for secret options to prevent unsafe object serialization:</p> <pre><code>from duckalog import SecretConfig, generate_secret_sql\n\n# \u2705 ALLOWED TYPES\nsecret = SecretConfig(\n    type=\"s3\",\n    options={\n        \"use_ssl\": True,        # bool\n        \"timeout\": 30,          # int  \n        \"rate_limit\": 0.5,      # float\n        \"region\": \"us-west-2\"   # str\n    }\n)\n# This works fine\n\n# \u274c BLOCKED TYPES\nsecret = SecretConfig(\n    type=\"s3\", \n    options={\n        \"bad_option\": [1, 2, 3],      # list - BLOCKED\n        \"worse_option\": {\"key\": \"val\"} # dict - BLOCKED\n    }\n)\ngenerate_secret_sql(secret)  # Raises TypeError\n</code></pre>"},{"location":"SECURITY/#security-guarantees","title":"Security Guarantees","text":"<ol> <li>Canonical API: All SQL construction uses safe quoting helpers</li> <li>No Ad-hoc Quoting: Prevents inconsistent or unsafe quoting patterns</li> <li>Type Safety: Strict validation prevents unsafe object serialization</li> <li>Consistent Behavior: Same quoting rules across all SQL generation</li> <li>Clear Error Messages: Detailed TypeError messages for violations</li> </ol>"},{"location":"SECURITY/#security-testing","title":"Security Testing","text":"<p>Test SQL injection protection:</p> <pre><code># Test malicious inputs are safely quoted\nfrom duckalog import quote_ident, quote_literal, generate_view_sql, ViewConfig\n\n# Test identifier injection\nmalicious_db = '\"; DROP TABLE users; --'\nsql = generate_view_sql(ViewConfig(\n    name=\"test\", source=\"duckdb\", database=malicious_db, table=\"test_table\"\n))\nassert \"DROP TABLE\" not in sql  # Injection blocked\n\n# Test literal injection  \nmalicious_secret = \"user' OR 1=1 --\"\nquoted = quote_literal(malicious_secret)\nassert \"OR 1=1\" not in quoted  # Injection blocked\n</code></pre>"},{"location":"SECURITY/#root-based-path-resolution-security","title":"\ud83d\udd12 Root-Based Path Resolution Security","text":"<p>Duckalog implements a robust root-based path security model that replaces heuristic-based traversal protection with clear, enforceable boundaries.</p>"},{"location":"SECURITY/#root-based-security-model","title":"Root-Based Security Model","text":""},{"location":"SECURITY/#core-security-principle","title":"Core Security Principle","text":"<p>The new security model enforces a simple but powerful invariant:</p> <p>Any local file path resolved from configuration MUST result in an absolute path that is located under at least one allowed root.</p> <p>Default Allowed Roots: - The directory containing the main configuration file - Any additional roots specified in configuration options</p>"},{"location":"SECURITY/#security-threats-mitigated","title":"Security Threats Mitigated","text":"<p>All Forms of Path Traversal: <pre><code># \u274c BLOCKED - Any traversal attempt is rejected\nviews:\n  - name: malicious1\n    source: parquet\n    uri: ../../../../../../etc/passwd\n\n  - name: malicious2  \n    source: parquet\n    uri: ..\\\\..\\\\..\\\\windows\\\\system32\\\\config\\\\sam\n\n  - name: malicious3\n    source: parquet\n    uri: ../..//../etc/passwd\n</code></pre></p> <p>System Directory Access: <pre><code># \u274c BLOCKED - All system directories are outside allowed roots\nviews:\n  - name: system_config\n    source: parquet\n    uri: /etc/config.parquet\n\n  - name: usr_access\n    source: parquet\n    uri: /usr/local/data/parquet\n</code></pre></p> <p>Invalid Path Encodings: <pre><code># \u274c BLOCKED - All path encoding bypass attempts\n\"..%2F..%2F..%2Fetc%2Fpasswd\"     # URL encoded\n\"..\\..\\..\\windows\\system32\\config\" # Mixed separators  \n\"../../../etc/passwd\"              # Standard traversal\n</code></pre></p>"},{"location":"SECURITY/#allowed-usage-patterns","title":"Allowed Usage Patterns","text":"<p>The security model allows legitimate file access within project boundaries:</p> <pre><code># \u2705 ALLOWED - Files within configuration directory tree\nviews:\n  - name: local_data\n    source: parquet\n    uri: ./data/processed/events.parquet      # Same directory\n\n  - name: subdirectory\n    source: parquet\n    uri: data/raw/customers.parquet           # Subdirectory\n\n  - name: sibling_project\n    source: parquet\n    uri: ../shared/common-data/users.parquet # Parent sibling (within bounds)\n</code></pre> <p>System Directory Access: <pre><code># \u274c BLOCKED - Attempts to access system directories\nviews:\n  - name: system_config\n    source: parquet\n    uri: ../etc/config.parquet\n    # Resolves to /etc/config.parquet - BLOCKED\n</code></pre></p> <p>Security Violations Blocked: - <code>/etc/</code>, <code>/usr/</code>, <code>/bin/</code>, <code>/sbin/</code>, <code>/var/log/</code>, <code>/sys/</code>, <code>/proc/</code> - Excessive parent directory traversal (more than 3 levels) - Paths that resolve to system locations - Invalid or malformed path sequences</p>"},{"location":"SECURITY/#reasonable-traversal-allowed","title":"Reasonable Traversal Allowed","text":"<p>The security model allows legitimate parent directory access within reasonable bounds:</p> <pre><code># \u2705 ALLOWED - Reasonable parent directory access\nviews:\n  - name: shared_data\n    source: parquet\n    uri: ../shared/data.parquet  # 1 level up - allowed\n\n  - name: project_common\n    source: parquet\n    uri: ../../project/common.parquet  # 2 levels up - allowed\n\n  - name: enterprise_data\n    source: parquet\n    uri: ../../../enterprise/data.parquet  # 3 levels up - allowed\n</code></pre>"},{"location":"SECURITY/#technical-implementation","title":"Technical Implementation","text":""},{"location":"SECURITY/#robust-cross-platform-validation","title":"Robust Cross-Platform Validation","text":"<p>The security model uses platform-independent primitives for maximum reliability:</p> <pre><code>from duckalog.path_resolution import is_within_allowed_roots\nfrom pathlib import Path\n\n# Uses Path.resolve() and os.path.commonpath() for validation\nconfig_dir = Path(\"/project/config\")\nallowed_roots = [config_dir]\n\nresult = is_within_allowed_roots(\"/project/config/data/file.parquet\", allowed_roots)\n# Returns: True (safe)\n</code></pre> <p>Key Technical Features: - <code>Path.resolve()</code>: Follows symlinks and resolves to canonical absolute paths - <code>os.path.commonpath()</code>: Finds common path prefix robustly across platforms - Cross-platform support: Handles Windows drive letters, UNC paths, Unix absolute paths - No magic numbers: Eliminates heuristic thresholds and pattern matching</p>"},{"location":"SECURITY/#path-type-handling","title":"Path Type Handling","text":"Path Type Security Treatment Local Relative Paths Resolved and validated against allowed roots Local Absolute Paths Validated to ensure they're within allowed roots Remote URIs (s3://, https://, etc.) Not subject to local path validation Windows Paths Full support for drive letters and UNC paths"},{"location":"SECURITY/#security-validation-process","title":"Security Validation Process","text":"<ol> <li>Path Detection: Classify path as relative, absolute, or remote</li> <li>Resolution: Convert relative paths to absolute paths</li> <li>Traversal Analysis: Count and validate parent directory traversals</li> <li>Pattern Matching: Check against dangerous location patterns</li> <li>File Access: Validate file exists and is accessible</li> <li>Error Reporting: Provide detailed security violation messages</li> </ol>"},{"location":"SECURITY/#security-error-handling","title":"Security Error Handling","text":"<p>Directory Traversal Violation: <pre><code>{\n  \"error\": \"Path resolution violates security rules: '../../../etc/passwd' resolves to '/etc/passwd' which is outside reasonable bounds\"\n}\n</code></pre></p> <p>System Directory Access Violation: <pre><code>{\n  \"error\": \"Path resolution violates security rules: '../etc/config.parquet' resolves to dangerous location\"\n}\n</code></pre></p> <p>File Access Issues: <pre><code>{\n  \"error\": \"File does not exist: /path/to/missing.parquet\"\n}\n</code></pre></p>"},{"location":"SECURITY/#security-configuration","title":"Security Configuration","text":"<p>Path resolution security is always enabled when path resolution is active:</p> <pre><code>from duckalog import load_config\n\n# Path resolution with security validation (default)\nconfig = load_config(\"catalog.yaml\", resolve_paths=True)\n\n# Disable path resolution (no security validation needed)\nconfig = load_config(\"catalog.yaml\", resolve_paths=False)\n</code></pre>"},{"location":"SECURITY/#security-best-practices-for-paths","title":"Security Best Practices for Paths","text":""},{"location":"SECURITY/#project-structure-recommendations","title":"Project Structure Recommendations","text":"<pre><code>project/\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 catalog.yaml\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2514\u2500\u2500 external/\n\u251c\u2500\u2500 databases/\n\u2502   \u2514\u2500\u2500 reference.duckdb\n\u2514\u2500\u2500 shared/\n    \u2514\u2500\u2500 enterprise/\n        \u2514\u2500\u2500 common_data/\n</code></pre>"},{"location":"SECURITY/#secure-configuration-patterns","title":"Secure Configuration Patterns","text":"<pre><code># \u2705 GOOD - Clear project structure\nviews:\n  - name: local_data\n    source: parquet\n    uri: ./data/processed/events.parquet\n\n  - name: shared_reference\n    source: parquet\n    uri: ../shared/enterprise/common_data/customers.parquet\n\n# \u274c AVOID - Deep or unclear traversal\nviews:\n  - name: unclear_data\n    source: parquet\n    uri: ../../../../some/deep/structure/data.parquet\n</code></pre>"},{"location":"SECURITY/#environment-specific-security","title":"Environment-Specific Security","text":"<p>Development Environment: - Allow broader file access for development convenience - Use relative paths for portable development setups</p> <p>Production Environment: - Restrict to well-defined data directories - Use absolute paths or controlled relative paths - Implement additional monitoring and logging</p>"},{"location":"SECURITY/#security-testing_1","title":"Security Testing","text":""},{"location":"SECURITY/#manual-security-testing","title":"Manual Security Testing","text":"<pre><code>from duckalog.path_resolution import validate_path_security\nfrom pathlib import Path\n\n# Test dangerous patterns\nconfig_dir = Path(\"/project/config\")\n\n# Test cases that should fail\nassert not validate_path_security(\"../../../etc/passwd\", config_dir)\nassert not validate_path_security(\"../usr/local/data.parquet\", config_dir)\n\n# Test cases that should succeed\nassert validate_path_security(\"../shared/data.parquet\", config_dir)\nassert validate_path_security(\"./local/data.parquet\", config_dir)\n</code></pre>"},{"location":"SECURITY/#automated-security-tests","title":"Automated Security Tests","text":"<p>The test suite includes comprehensive security validation:</p> <pre><code># Run path resolution security tests\npytest tests/test_path_resolution.py -k \"security\"\n\n# Test specific security scenarios\npytest tests/test_path_resolution.py::TestPathValidation\n</code></pre>"},{"location":"SECURITY/#security-auditing","title":"Security Auditing","text":""},{"location":"SECURITY/#logging","title":"Logging","text":"<p>Path resolution security violations are logged with detailed information:</p> <pre><code># Security logging includes:\n# - Original path that violated security\n# - Resolved path that was blocked\n# - Security rule that was violated\n# - Configuration file location\n</code></pre>"},{"location":"SECURITY/#monitoring","title":"Monitoring","text":"<p>Monitor for security violations in production:</p> <ol> <li>Log Analysis: Look for path resolution security errors</li> <li>Access Patterns: Monitor for unusual file access attempts</li> <li>Configuration Changes: Track path configuration modifications</li> <li>Error Rates: Alert on increased security violation rates</li> </ol>"},{"location":"SECURITY/#read-only-sql-enforcement","title":"\ud83d\udee1\ufe0f Read-Only SQL Enforcement","text":""},{"location":"SECURITY/#sql-query-validation","title":"SQL Query Validation","text":"<p>The UI enforces strict read-only SQL execution to prevent data modification and database attacks:</p>"},{"location":"SECURITY/#allowed-operations","title":"Allowed Operations","text":"<ul> <li><code>SELECT</code> statements (single statement only)</li> <li><code>WITH</code> clauses (Common Table Expressions)</li> <li><code>JOIN</code>, <code>UNION</code>, <code>INTERSECT</code>, <code>EXCEPT</code> operations</li> <li><code>WHERE</code>, <code>GROUP BY</code>, <code>HAVING</code>, <code>ORDER BY</code> clauses</li> <li><code>LIMIT</code> and <code>OFFSET</code> clauses</li> <li>Subqueries and nested queries</li> <li>Window functions and aggregate functions</li> </ul>"},{"location":"SECURITY/#blocked-operations","title":"Blocked Operations","text":"<p>DDL (Data Definition Language) - <code>CREATE</code> (TABLE, VIEW, INDEX, etc.) - <code>DROP</code> (TABLE, VIEW, INDEX, etc.) - <code>ALTER</code> (TABLE, etc.) - <code>TRUNCATE</code> - <code>RENAME</code></p> <p>DML (Data Manipulation Language) - <code>INSERT</code> - <code>UPDATE</code> - <code>DELETE</code> - <code>MERGE</code> / <code>UPSERT</code> / <code>REPLACE</code> - <code>CALL</code> (stored procedures)</p> <p>Administrative Commands - <code>GRANT</code> / <code>REVOKE</code> - <code>COMMENT</code> - <code>EXPLAIN</code> / <code>DESCRIBE</code> - <code>EXECUTE</code></p> <p>Multi-Statement Queries - Any query containing multiple statements separated by semicolons - Attempted SQL injection using statement chaining</p>"},{"location":"SECURITY/#security-error-messages","title":"Security Error Messages","text":"<p>When queries are blocked, the system returns descriptive error messages:</p> <pre><code>{\n  \"error\": \"Invalid query: DDL statements are not allowed for security reasons\"\n}\n</code></pre> <pre><code>{\n  \"error\": \"Invalid query: Only single SELECT statements are allowed\"\n}\n</code></pre>"},{"location":"SECURITY/#examples","title":"Examples","text":"<p>\u2705 Allowed Queries: <pre><code>SELECT * FROM my_view WHERE id &gt; 100\nSELECT * FROM users JOIN orders ON users.id = orders.user_id\nWITH ranked_data AS (SELECT *, ROW_NUMBER() OVER (ORDER BY created_at) as rn FROM my_table) SELECT * FROM ranked_data\n</code></pre></p> <p>\u274c Blocked Queries: <pre><code>DROP TABLE users\nSELECT * FROM users; DELETE FROM users;\nINSERT INTO logs VALUES ('hack attempt')\nCALL malicious_procedure()\n</code></pre></p>"},{"location":"SECURITY/#authentication-and-authorization","title":"\ud83d\udd10 Authentication and Authorization","text":""},{"location":"SECURITY/#admin-token-protection","title":"Admin Token Protection","text":"<p>Mutating operations (POST, PUT, DELETE) require authentication in production mode:</p> <pre><code># Set admin token for production\nexport DUCKALOG_ADMIN_TOKEN=\"your-secure-random-token\"\n</code></pre>"},{"location":"SECURITY/#protected-endpoints","title":"Protected Endpoints","text":"<ul> <li><code>/api/config</code> (POST)</li> <li><code>/api/views</code> (POST, PUT, DELETE)</li> <li><code>/api/rebuild</code> (POST)</li> </ul>"},{"location":"SECURITY/#local-mode-development","title":"Local Mode (Development)","text":"<p>When running locally without an admin token, the UI operates in a permissive mode suitable for development.</p>"},{"location":"SECURITY/#production-mode","title":"Production Mode","text":"<p>When <code>DUCKALOG_ADMIN_TOKEN</code> is set, all mutating endpoints require: <pre><code>Authorization: Bearer your-secure-random-token\n</code></pre></p>"},{"location":"SECURITY/#token-security-best-practices","title":"Token Security Best Practices","text":"<ol> <li>Use strong, random tokens (minimum 32 characters)</li> <li>Rotate tokens regularly</li> <li>Never commit tokens to version control</li> <li>Use environment variables or secure secret management</li> <li>Monitor for unauthorized access attempts</li> </ol>"},{"location":"SECURITY/#cors-policy","title":"\ud83c\udf10 CORS Policy","text":""},{"location":"SECURITY/#default-configuration","title":"Default Configuration","text":"<p>The UI implements restrictive CORS policies by default:</p>"},{"location":"SECURITY/#allowed-origins","title":"Allowed Origins","text":"<ul> <li><code>http://localhost</code></li> <li><code>http://127.0.0.1</code></li> <li>Specific localhost ports (3000, 8000, 8080, 9000, 5173)</li> </ul>"},{"location":"SECURITY/#security-settings","title":"Security Settings","text":"<ul> <li>Credentials: Disabled by default (<code>Access-Control-Allow-Credentials: false</code>)</li> <li>Methods: GET, POST, PUT, DELETE, OPTIONS</li> <li>Headers: Content-Type, Authorization</li> </ul>"},{"location":"SECURITY/#cross-origin-protection","title":"Cross-Origin Protection","text":"<p>External domains are automatically blocked: - \u274c <code>https://evil-site.com</code> - \u274c <code>https://malicious-domain.net</code> - \u274c Any non-localhost origin</p>"},{"location":"SECURITY/#customization","title":"Customization","text":"<p>For production deployments, customize CORS settings in your deployment configuration:</p> <pre><code># Example: Custom allowed origins\ncors_origins = [\"https://your-trusted-domain.com\"]\n</code></pre>"},{"location":"SECURITY/#configuration-security","title":"\ud83d\udcc1 Configuration Security","text":""},{"location":"SECURITY/#format-preservation","title":"Format Preservation","text":"<p>The UI preserves the original configuration file format (YAML/JSON) when making updates:</p> <ul> <li>YAML files: Maintain comments, formatting, and structure</li> <li>JSON files: Preserve pretty-printing and organization</li> <li>Atomic writes: Prevent configuration corruption</li> </ul>"},{"location":"SECURITY/#atomic-operations","title":"Atomic Operations","text":"<p>All configuration updates use atomic file operations: 1. Write to temporary file 2. Validate the temporary file 3. Atomically move to target location 4. Clean up on failure</p>"},{"location":"SECURITY/#in-memory-reload","title":"In-Memory Reload","text":"<p>Configuration changes are immediately reflected in memory without requiring server restart.</p>"},{"location":"SECURITY/#background-task-security","title":"\u26a1 Background Task Security","text":""},{"location":"SECURITY/#task-isolation","title":"Task Isolation","text":"<p>Database operations run in isolated background threads: - Prevents UI blocking during long-running queries - Isolates failures between concurrent operations - Prevents resource exhaustion</p>"},{"location":"SECURITY/#task-result-management","title":"Task Result Management","text":"<ul> <li>Unique task IDs: Prevent result collisions</li> <li>Automatic cleanup: Prevents memory leaks</li> <li>Error isolation: Failed tasks don't affect others</li> <li>Timeout handling: Prevents hanging operations</li> </ul>"},{"location":"SECURITY/#concurrent-operation-safety","title":"Concurrent Operation Safety","text":"<p>Multiple users can safely: - Run queries simultaneously - Export data concurrently - Rebuild catalogs without interference - View and modify configurations independently</p>"},{"location":"SECURITY/#security-monitoring","title":"\ud83d\udd0d Security Monitoring","text":""},{"location":"SECURITY/#request-logging","title":"Request Logging","text":"<p>All security-relevant actions are logged: - Failed authentication attempts - Blocked SQL queries - CORS policy violations - Configuration changes</p>"},{"location":"SECURITY/#error-handling","title":"Error Handling","text":"<p>Security-sensitive errors don't expose internal details: - Generic error messages for security violations - No stack traces in production responses - Proper HTTP status codes (401, 403, 400)</p>"},{"location":"SECURITY/#production-deployment-security","title":"\ud83d\ude80 Production Deployment Security","text":""},{"location":"SECURITY/#environment-variables","title":"Environment Variables","text":"<pre><code># Required for production security\nexport DUCKALOG_ADMIN_TOKEN=\"your-secure-random-token\"\n\n# Optional: Custom CORS origins\nexport DUCKALOG_CORS_ORIGINS=\"https://yourdomain.com,https://app.yourdomain.com\"\n</code></pre>"},{"location":"SECURITY/#network-security","title":"Network Security","text":"<ol> <li>Firewall: Restrict access to UI port (default: 8000)</li> <li>HTTPS: Use reverse proxy with SSL/TLS termination</li> <li>VPN: Consider VPN access for internal deployments</li> <li>Network segmentation: Isolate from sensitive systems</li> </ol>"},{"location":"SECURITY/#reverse-proxy-configuration","title":"Reverse Proxy Configuration","text":"<p>Example using nginx:</p> <pre><code>server {\n    listen 443 ssl;\n    server_name your-domain.com;\n\n    # SSL configuration\n    ssl_certificate /path/to/cert.pem;\n    ssl_certificate_key /path/to/key.pem;\n\n    # Security headers\n    add_header X-Frame-Options DENY;\n    add_header X-Content-Type-Options nosniff;\n    add_header X-XSS-Protection \"1; mode=block\";\n\n    location / {\n        proxy_pass http://127.0.0.1:8000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre>"},{"location":"SECURITY/#security-testing_2","title":"\ud83d\udd27 Security Testing","text":"<p>The comprehensive test suite validates security features:</p> <ul> <li>Read-only SQL enforcement (<code>TestReadOnlySQLEnforcement</code>)</li> <li>CORS policy validation (<code>TestCORSPolicy</code>)</li> <li>Authentication verification (integrated tests)</li> <li>Configuration security (<code>TestConfigFormatPreservation</code>)</li> <li>Background task isolation (<code>TestBackgroundTaskConcurrency</code>)</li> </ul> <p>Run security tests:</p> <pre><code># Run all security-related tests\npytest tests/test_ui.py::TestReadOnlySQLEnforcement\npytest tests/test_ui.py::TestCORSPolicy\npytest tests/test_ui.py::TestConfigFormatPreservation\n\n# Run all UI security tests\npytest tests/test_ui.py -k \"security or auth or cors or read_only\"\n</code></pre>"},{"location":"SECURITY/#security-considerations","title":"\ud83d\udea8 Security Considerations","text":""},{"location":"SECURITY/#database-security","title":"Database Security","text":"<ul> <li>Read-only database user: Consider using read-only database credentials</li> <li>Connection security: Use SSL/TLS for database connections</li> <li>Access control: Limit database access to the UI server only</li> </ul>"},{"location":"SECURITY/#data-privacy","title":"Data Privacy","text":"<ul> <li>Sensitive data: Avoid exposing PII in view definitions</li> <li>Query logs: Be aware that queries may be logged</li> <li>Export limits: Consider implementing export size limits</li> </ul>"},{"location":"SECURITY/#operational-security","title":"Operational Security","text":"<ul> <li>Regular updates: Keep dependencies updated</li> <li>Monitoring: Monitor for unusual activity patterns</li> <li>Backup security: Secure configuration backups</li> <li>Access audit: Regularly review access logs</li> </ul>"},{"location":"SECURITY/#incident-response","title":"\ud83c\udd98 Incident Response","text":""},{"location":"SECURITY/#security-incident-response","title":"Security Incident Response","text":"<ol> <li>Containment: Immediately rotate admin tokens</li> <li>Analysis: Review logs for unauthorized access</li> <li>Recovery: Restart services with clean configuration</li> <li>Prevention: Update security configurations</li> <li>Reporting: Document and learn from incidents</li> </ol>"},{"location":"SECURITY/#contact-information","title":"Contact Information","text":"<p>Report security vulnerabilities: - Create an issue with \"SECURITY\" label - Email security contacts (if provided) - Follow responsible disclosure practices</p> <p>Last updated: Duckalog v0.1.0</p>"},{"location":"architecture/","title":"Duckalog Architecture","text":"<p>Duckalog is a Python library and CLI for building DuckDB catalogs from declarative YAML/JSON configuration files. This document provides a comprehensive architectural overview of the system, its components, and how they work together to transform configuration files into functional DuckDB catalogs.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>Duckalog follows a config-driven, idempotent architecture that transforms declarative configuration into functional DuckDB catalogs. The system is designed around separation of concerns, with clear boundaries between configuration loading, validation, SQL generation, and catalog execution.</p>"},{"location":"architecture/#core-philosophy","title":"Core Philosophy","text":"<ul> <li>Configuration as Code: Catalogs are defined in version-controllable YAML/JSON files</li> <li>Idempotent Operations: Running the same config produces the same catalog every time</li> <li>Multi-Source Integration: Unified interface for S3 Parquet, Delta Lake, Iceberg, and relational databases</li> <li>Environment-Driven: Credentials and connection details sourced from environment variables</li> <li>Separation of Concerns: Clear boundaries between configuration, validation, generation, and execution</li> </ul>"},{"location":"architecture/#high-level-system-architecture","title":"High-Level System Architecture","text":"<pre><code>graph TB\n    subgraph \"External Systems\"\n        S3[S3 Object Storage&lt;br/&gt;Parquet Files]\n        DELTA[Delta Lake&lt;br/&gt;Tables]\n        ICEBERG[Iceberg&lt;br/&gt;Catalog/Tables]\n        DUCKDB_EXT[External DuckDB&lt;br/&gt;Files]\n        SQLITE[SQLite&lt;br/&gt;Databases]\n        POSTGRES[PostgreSQL&lt;br/&gt;Databases]\n    end\n\n    subgraph \"Duckalog System\"\n        CLI[CLI Interface&lt;br/&gt;Commands &amp; Validation]\n        CONFIG[Config Loading&lt;br/&gt;YAML/JSON + Environment]\n        MODEL[Pydantic Models&lt;br/&gt;Validation &amp; Types]\n        SQLGEN[SQL Generation&lt;br/&gt;CREATE VIEW Statements]\n        ENGINE[DuckDB Engine&lt;br/&gt;Execution &amp; Connections]\n    end\n\n    subgraph \"Output\"\n        CATALOG[(DuckDB Catalog&lt;br/&gt;Views &amp; Attachments)]\n    end\n\n    CLI --&gt; CONFIG\n    CONFIG --&gt; MODEL\n    MODEL --&gt; SQLGEN\n    SQLGEN --&gt; ENGINE\n    ENGINE --&gt; CATALOG\n\n    ENGINE -.-&gt;|S3 Parquet Access| S3\n    ENGINE -.-&gt;|Delta Tables| DELTA\n    ENGINE -.-&gt;|Iceberg Integration| ICEBERG\n    ENGINE -.-&gt;|DuckDB Attachments| DUCKDB_EXT\n    ENGINE -.-&gt;|SQLite Connections| SQLITE\n    ENGINE -.-&gt;|PostgreSQL Access| POSTGRES</code></pre>"},{"location":"architecture/#core-architecture-components","title":"Core Architecture Components","text":"<p>The system consists of five primary modules that work together to process configurations and create DuckDB catalogs:</p> <pre><code>graph TB\n    subgraph \"Duckalog Core Components\"\n        CLI[CLI Module&lt;br/&gt;Command parsing &amp; dispatch]\n        CONFIG[Config Module&lt;br/&gt;Loading &amp; env interpolation]\n        MODEL[Model Module&lt;br/&gt;Pydantic validation]\n        SQLGEN[SQL Generation&lt;br/&gt;CREATE VIEW statements]\n        ENGINE[Engine Module&lt;br/&gt;DuckDB execution]\n\n        CLI -.-&gt;|typer commands| CONFIG\n        CONFIG -.-&gt;|validated dict| MODEL\n        MODEL -.-&gt;|typed objects| SQLGEN\n        SQLGEN -.-&gt;|SQL strings| ENGINE\n        ENGINE -.-&gt;|executed SQL| DB[(DuckDB Catalog)]\n    end\n\n    subgraph \"Detailed Component Interactions\"\n        direction LR\n        CONFIG_A[File I/O]\n        CONFIG_B[Env Interpolation]\n        CONFIG_C[Error Handling]\n\n        MODEL_A[Schema Validation]\n        MODEL_B[Type Conversion]\n        MODEL_C[Business Rules]\n\n        SQLGEN_A[Source Detection]\n        SQLGEN_B[Template Selection]\n        SQLGEN_C[Statement Building]\n\n        ENGINE_A[Connection Mgmt]\n        ENGINE_B[Attachment Setup]\n        ENGINE_C[View Creation]\n        ENGINE_D[Transaction Control]\n    end</code></pre>"},{"location":"architecture/#1-configuration-module-configpy","title":"1. Configuration Module (<code>config.py</code>)","text":"<p>Responsibilities: - Load YAML/JSON configuration files - Perform environment variable interpolation - Parse and prepare raw configuration data - Handle file I/O and error management</p> <p>Key Features: - Supports both YAML and JSON formats - Environment variable substitution using <code>${env:VAR_NAME}</code> pattern - Recursive traversal of configuration structures - Comprehensive error handling with descriptive messages</p> <p>Example Flow: <pre><code>sequenceDiagram\n    participant CLI\n    participant Config\n    participant File\n    participant Env\n\n    CLI-&gt;&gt;Config: load_config(\"catalog.yaml\")\n    Config-&gt;&gt;File: Read file content\n    File--&gt;&gt;Config: Raw YAML content\n    Config-&gt;&gt;Config: Parse YAML/JSON\n    Config-&gt;&gt;Config: interpolate_env(raw_dict)\n    Config-&gt;&gt;Env: Get ${env:VAR} values\n    Env--&gt;&gt;Config: Real values\n    Config--&gt;&gt;CLI: Validated Config object</code></pre></p>"},{"location":"architecture/#2-model-module-modelpy","title":"2. Model Module (<code>model.py</code>)","text":"<p>Responsibilities: - Define Pydantic models for configuration schema - Provide data validation and type checking - Ensure configuration consistency - Support extensibility for new configuration types</p> <p>Core Models: - <code>DuckDBConfig</code>: Database file and session settings - <code>AttachmentConfig</code>: External database connections (DuckDB, SQLite, Postgres) - <code>IcebergCatalogConfig</code>: Iceberg catalog connections - <code>ViewConfig</code>: Individual view definitions and source specifications - <code>Config</code>: Root configuration aggregating all components</p> <p>Validation Flow: <pre><code>flowchart LR\n    A[Raw Config Dict] --&gt; B[Pydantic Validation]\n    B --&gt; C{Valid?}\n    C --&gt;|Yes| D[Typed Config Object]\n    C --&gt;|No| E[ConfigError]\n    D --&gt; F[Schema Validation]\n    F --&gt; G[Business Rule Validation]\n    G --&gt; H[Validated Config]</code></pre></p>"},{"location":"architecture/#3-sql-generation-module-sqlgenpy","title":"3. SQL Generation Module (<code>sqlgen.py</code>)","text":"<p>Responsibilities: - Transform typed configuration objects into SQL statements - Generate <code>CREATE VIEW</code> statements for different source types - Handle SQL escaping and identifier quoting - Support complex view definitions with joins and aggregations</p> <p>Source Types Supported: - Parquet: S3-based Parquet file views - Delta Lake: Delta table references - Iceberg: Iceberg table and catalog views - Database: Attached DuckDB/SQLite/Postgres tables - SQL: Raw SQL query views</p> <p>SQL Generation Process: <pre><code>graph TD\n    A[ViewConfig] --&gt; B{Source Type Detection}\n    B --&gt;|parquet| C[Generate Parquet CREATE VIEW]\n    B --&gt;|delta| D[Generate Delta CREATE VIEW]\n    B --&gt;|iceberg| E[Generate Iceberg CREATE VIEW]\n    B --&gt;|duckdb/sqlite/postgres| F[Generate DB CREATE VIEW]\n    B --&gt;|sql| G[Validate &amp; Pass Through SQL]\n\n    C --&gt; H[Add Options &amp; Settings]\n    D --&gt; H\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n    H --&gt; I[Complete SQL Statement]\n\n    subgraph \"Source-Specific Logic\"\n        J[URI Processing]\n        K[Authentication Config]\n        L[Table Reference Resolution]\n        M[Option Normalization]\n    end\n\n    C -.-&gt; J\n    D -.-&gt; J\n    E -.-&gt; L\n    F -.-&gt; L</code></pre></p>"},{"location":"architecture/#4-engine-module-enginepy","title":"4. Engine Module (<code>engine.py</code>)","text":"<p>Responsibilities: - Manage DuckDB connections and sessions - Set up external attachments and catalogs - Execute generated SQL statements - Handle transaction management and error recovery</p> <p>Key Operations: - Connection Management: Open and maintain DuckDB connections - Attachment Setup: Configure DuckDB, SQLite, and Postgres connections - Catalog Configuration: Set up Iceberg catalogs with proper authentication - View Execution: Apply generated SQL to create views in the catalog - Session Management: Configure pragmas and session settings</p> <p>Engine Workflow: <pre><code>sequenceDiagram\n    participant Config\n    participant Engine\n    participant DuckDB\n    participant External\n\n    Config-&gt;&gt;Engine: execute(config)\n    Engine-&gt;&gt;DuckDB: Open connection\n    Engine-&gt;&gt;DuckDB: Apply pragmas\n    Engine-&gt;&gt;DuckDB: Install extensions\n    Config-&gt;&gt;Engine: Process attachments\n    Engine-&gt;&gt;DuckDB: ATTACH external DBs\n    Config-&gt;&gt;Engine: Process catalogs\n    Engine-&gt;&gt;DuckDB: CREATE iceberg catalogs\n    Config-&gt;&gt;Engine: Process views\n    loop For each view\n        Engine-&gt;&gt;SQLGEN: generate_sql(view)\n        SQLGEN--&gt;&gt;Engine: SQL statement\n        Engine-&gt;&gt;DuckDB: Execute SQL\n        DuckDB--&gt;&gt;Engine: Success/Error\n    end\n    Engine--&gt;&gt;Config: Complete catalog</code></pre></p>"},{"location":"architecture/#5-cli-module-clipy","title":"5. CLI Module (<code>cli.py</code>)","text":"<p>Responsibilities: - Provide command-line interface for users - Parse command-line arguments and options - Dispatch to appropriate library functions - Handle user input validation and error reporting</p> <p>Available Commands: - <code>build</code>: Create or update a DuckDB catalog from configuration - <code>generate-sql</code>: Generate SQL statements without executing them - <code>validate</code>: Validate configuration files for syntax and schema correctness</p>"},{"location":"architecture/#data-flow-architecture","title":"Data Flow Architecture","text":"<p>The complete data flow from configuration file to functional catalog follows this pipeline:</p> <pre><code>flowchart TD\n    A[YAML/JSON Config File] --&gt; B[Config Loading]\n    B --&gt; C[Environment Interpolation]\n    C --&gt; D[Pydantic Validation]\n    D --&gt; E[Business Rule Validation]\n    E --&gt; F[SQL Generation]\n    F --&gt; G[Catalog Building]\n    G --&gt; H[(DuckDB Catalog)]\n\n    subgraph \"Validation Stages\"\n        C --&gt; C1[Syntax Check]\n        C1 --&gt; C2[Schema Validation]\n        C2 --&gt; C3[Reference Validation]\n    end\n\n    subgraph \"SQL Generation\"\n        F --&gt; F1[Source Type Detection]\n        F1 --&gt; F2[Template Selection]\n        F2 --&gt; F3[Statement Building]\n        F3 --&gt; F4[Option Processing]\n    end\n\n    subgraph \"Catalog Building\"\n        G --&gt; G1[Connection Setup]\n        G1 --&gt; G2[Attachment Configuration]\n        G2 --&gt; G3[Catalog Setup]\n        G3 --&gt; G4[View Creation]\n    end</code></pre>"},{"location":"architecture/#complete-data-flow-architecture","title":"Complete Data Flow Architecture","text":"<p>Here's the complete end-to-end data flow from configuration file to functional DuckDB catalog:</p> <pre><code>sequenceDiagram\n    participant User\n    participant CLI\n    participant Config\n    participant Model\n    participant SQLGen\n    participant Engine\n    participant DuckDB\n    participant External\n\n    User-&gt;&gt;CLI: duckalog build config.yaml\n    CLI-&gt;&gt;Config: load_config()\n    Config-&gt;&gt;External: Read file\n    Config-&gt;&gt;External: Environment variables\n    Config-&gt;&gt;Model: Raw config dict\n\n    par Parallel Processing\n        Model-&gt;&gt;Model: Schema validation\n        Model-&gt;&gt;Model: Type conversion\n        Model-&gt;&gt;Model: Business rule validation\n    end\n\n    Model-&gt;&gt;SQLGen: Validated ViewConfig list\n\n    par For Each View\n        SQLGen-&gt;&gt;SQLGen: Detect source type\n        SQLGen-&gt;&gt;SQLGen: Generate SQL statement\n        SQLGen-&gt;&gt;SQLGen: Apply source-specific options\n        SQLGen-&gt;&gt;Engine: SQL string\n    end\n\n    Engine-&gt;&gt;DuckDB: Open connection\n    Engine-&gt;&gt;DuckDB: Apply pragmas\n    Engine-&gt;&gt;External: Setup attachments (if any)\n    Engine-&gt;&gt;External: Setup catalogs (if any)\n\n    par For Each Generated SQL\n        Engine-&gt;&gt;DuckDB: Execute CREATE VIEW\n        DuckDB--&gt;&gt;Engine: Success/Error\n    end\n\n    Engine--&gt;&gt;Model: Completion status\n    Model--&gt;&gt;CLI: Success/Error\n    CLI--&gt;&gt;User: Operation complete</code></pre>"},{"location":"architecture/#design-patterns-and-architectural-decisions","title":"Design Patterns and Architectural Decisions","text":""},{"location":"architecture/#1-separation-of-concerns","title":"1. Separation of Concerns","text":"<p>Duckalog strictly separates different responsibilities into distinct modules:</p> <ul> <li>Configuration Layer: File I/O, parsing, environment interpolation</li> <li>Validation Layer: Schema validation, type checking, business rules</li> <li>Generation Layer: SQL statement creation, template management</li> <li>Execution Layer: Database operations, connection management</li> <li>Interface Layer: CLI commands, user interaction</li> </ul> <p>This separation provides several benefits: - Easier testing and debugging - Independent module evolution - Clear extension points - Reduced coupling between components</p>"},{"location":"architecture/#2-config-driven-design","title":"2. Config-Driven Design","text":"<p>The entire system revolves around declarative configuration:</p> <p>Benefits: - Reproducibility: Same config always produces same results - Version Control: Configuration files can be tracked in Git - Testing: Easy to create test scenarios with different configs - Collaboration: Non-developers can understand and modify configurations - Documentation: Config files serve as living documentation</p> <p>Extensibility: New source types can be added by: 1. Extending the <code>ViewConfig</code> Pydantic model 2. Adding SQL generation logic for the new source type 3. Implementing engine-side setup if needed 4. Updating documentation and examples</p>"},{"location":"architecture/#3-idempotent-operations","title":"3. Idempotent Operations","text":"<p>All catalog building operations are designed to be idempotent:</p> <ul> <li>View Replacement: <code>CREATE OR REPLACE VIEW</code> ensures consistent results</li> <li>Attachment Management: Consistent attachment procedures regardless of current state</li> <li>Schema Evolution: Config changes are applied predictably</li> <li>Rollback Safety: Failed operations leave the catalog in a consistent state</li> </ul>"},{"location":"architecture/#4-layered-architecture","title":"4. Layered Architecture","text":"<p>The system follows a strict layered approach:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   CLI Interface     \u2502  \u2190 User interaction, command parsing\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Config Layer      \u2502  \u2190 File I/O, environment handling\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Validation Layer  \u2502  \u2190 Schema validation, type checking\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Generation Layer  \u2502  \u2190 SQL creation, template management\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Execution Layer   \u2502  \u2190 Database operations, connections\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   External Systems  \u2502  \u2190 DuckDB, S3, databases, catalogs\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Each layer depends only on the layer directly below it, ensuring clear dependencies and testability.</p>"},{"location":"architecture/#5-error-handling-and-logging-patterns","title":"5. Error Handling and Logging Patterns","text":"<p>Error Handling Strategy: - Fail Fast: Validate configuration early to catch issues quickly - Descriptive Errors: Provide actionable error messages with context - Graceful Degradation: Continue processing non-dependent items when possible - Error Categorization: Different exception types for different failure modes</p> <p>Logging Approach: - Structured Logging: Use consistent log formats and levels - Security Conscious: Never log sensitive information (passwords, tokens) - Debug Support: Detailed logging available for troubleshooting - User-Friendly: Important operations logged at appropriate levels</p>"},{"location":"architecture/#configuration-schema-architecture","title":"Configuration Schema Architecture","text":"<p>The configuration schema follows a hierarchical structure:</p> <pre><code>graph TD\n    ROOT[Config Root&lt;br/&gt;version, duckdb, attachments, views] --&gt; DB[DuckDB Config&lt;br/&gt;database, pragmas]\n    ROOT --&gt; ATTACH[Attachments Config&lt;br/&gt;duckdb, sqlite, postgres]\n    ROOT --&gt; ICEBERG[Iceberg Catalogs&lt;br/&gt;catalog configs]\n    ROOT --&gt; VIEWS[Views List&lt;br/&gt;view definitions]\n\n    ATTACH --&gt; DUCKDB_ATTACH[DuckDB Attachment&lt;br/&gt;alias, path, read_only]\n    ATTACH --&gt; SQLITE_ATTACH[SQLite Attachment&lt;br/&gt;alias, path]\n    ATTACH --&gt; POSTGRES_ATTACH[Postgres Attachment&lt;br/&gt;alias, host, port, user, password]\n\n    ICEBERG --&gt; IC_CONFIG[Iceberg Catalog Config&lt;br/&gt;name, catalog_type, uri, warehouse]\n\n    VIEWS --&gt; PARQUET_VIEW[Parquet View&lt;br/&gt;source: parquet, uri, options]\n    VIEWS --&gt; DELTA_VIEW[Delta View&lt;br/&gt;source: delta, uri]\n    VIEWS --&gt; ICEBERG_VIEW[Iceberg View&lt;br/&gt;source: iceberg, catalog, table]\n    VIEWS --&gt; DB_VIEW[Database View&lt;br/&gt;source: duckdb/sqlite/postgres, database, table]\n    VIEWS --&gt; SQL_VIEW[SQL View&lt;br/&gt;source: sql, query]</code></pre>"},{"location":"architecture/#component-dependency-graph","title":"Component Dependency Graph","text":"<p>Understanding the dependencies between components helps in maintenance and extension:</p> <pre><code>graph TD\n    subgraph \"Layer 1: Interface\"\n        CLI[CLI Module&lt;br/&gt;Commands: build, validate, generate-sql]\n    end\n\n    subgraph \"Layer 2: Configuration\"\n        CONFIG[Config Module&lt;br/&gt;File loading &amp; env interpolation]\n    end\n\n    subgraph \"Layer 3: Validation\"\n        MODEL[Model Module&lt;br/&gt;Pydantic validation &amp; types]\n    end\n\n    subgraph \"Layer 4: Generation\"\n        SQLGEN[SQL Generation&lt;br/&gt;Statement creation]\n    end\n\n    subgraph \"Layer 5: Execution\"\n        ENGINE[Engine Module&lt;br/&gt;DuckDB operations]\n    end\n\n    subgraph \"External Dependencies\"\n        DUCKDB[DuckDB Engine]\n        YAML[YAML/JSON Parser]\n        PYDANTIC[Pydantic Validator]\n        TYPER[Typer CLI Framework]\n    end\n\n    CLI --&gt; CONFIG\n    CONFIG --&gt; MODEL\n    MODEL --&gt; SQLGEN\n    SQLGEN --&gt; ENGINE\n\n    CONFIG -.-&gt; YAML\n    MODEL -.-&gt; PYDANTIC\n    CLI -.-&gt; TYPER\n    ENGINE -.-&gt; DUCKDB</code></pre>"},{"location":"architecture/#dependency-rules","title":"Dependency Rules:","text":"<ol> <li>No backward dependencies - higher layers never depend on lower layers</li> <li>Clear interfaces - each layer exposes well-defined interfaces</li> <li>Minimal coupling - components only know about their direct dependencies</li> <li>Testable units - each layer can be tested independently</li> </ol>"},{"location":"architecture/#extension-patterns","title":"Extension Patterns","text":""},{"location":"architecture/#adding-new-source-types","title":"Adding New Source Types","text":"<p>To extend Duckalog with a new data source type:</p> <ol> <li> <p>Model Extension:    <pre><code>class NewSourceViewConfig(BaseModel):\n    name: str\n    source: Literal[\"new_source\"]\n    uri: str\n    options: dict[str, Any] = Field(default_factory=dict)\n</code></pre></p> </li> <li> <p>SQL Generation:    <pre><code>def generate_new_source_sql(view: NewSourceViewConfig) -&gt; str:\n    # Generate appropriate CREATE VIEW statement\n    return f\"CREATE OR REPLACE VIEW {view.name} AS ...\"\n</code></pre></p> </li> <li> <p>Engine Integration (if needed):    <pre><code>def setup_new_source(engine, view: NewSourceViewConfig):\n    # Set up any required connections or configurations\n    pass\n</code></pre></p> </li> </ol>"},{"location":"architecture/#adding-new-attachment-types","title":"Adding New Attachment Types","text":"<p>Similar extension pattern for database attachments:</p> <ol> <li>Extend <code>AttachmentsConfig</code> model</li> <li>Add attachment setup logic in engine</li> <li>Update SQL generation if needed</li> <li>Add validation rules</li> </ol>"},{"location":"architecture/#performance-and-scalability-considerations","title":"Performance and Scalability Considerations","text":""},{"location":"architecture/#current-architecture-supports","title":"Current Architecture Supports:","text":"<ul> <li>Large Configuration Files: Efficient parsing and validation</li> <li>Multiple Views: Batch processing and optimization</li> <li>Concurrent Operations: Thread-safe where appropriate</li> <li>Memory Management: Streaming and chunked processing where needed</li> </ul>"},{"location":"architecture/#scaling-patterns","title":"Scaling Patterns:","text":"<ul> <li>Horizontal Scaling: Multiple catalogs can be processed independently</li> <li>Vertical Scaling: DuckDB's in-memory processing for complex queries</li> <li>External Optimization: Leverage underlying system optimizations (S3, databases)</li> </ul>"},{"location":"architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"architecture/#security-principles","title":"Security Principles:","text":"<ul> <li>Zero Secrets in Config: All sensitive data via environment variables</li> <li>Connection Security: SSL/TLS support for external connections</li> <li>Access Control: DuckDB's built-in security features</li> <li>Audit Trail: Config-driven approach provides built-in change tracking</li> </ul>"},{"location":"architecture/#security-measures","title":"Security Measures:","text":"<ul> <li>Environment variable validation</li> <li>Secure credential handling</li> <li>Connection security options (SSL modes)</li> <li>No credential logging or exposure</li> </ul>"},{"location":"architecture/#development-and-testing-architecture","title":"Development and Testing Architecture","text":""},{"location":"architecture/#testing-strategy","title":"Testing Strategy:","text":"<ul> <li>Unit Tests: Individual module functionality</li> <li>Integration Tests: End-to-end catalog building</li> <li>Configuration Tests: Schema validation and parsing</li> <li>SQL Generation Tests: Output verification for different source types</li> </ul>"},{"location":"architecture/#development-workflow","title":"Development Workflow:","text":"<ol> <li>Configuration-driven development</li> <li>Test-first approach for new features</li> <li>Documentation integration</li> <li>Continuous validation against specifications</li> </ol>"},{"location":"architecture/#future-architecture-considerations","title":"Future Architecture Considerations","text":""},{"location":"architecture/#potential-extensions","title":"Potential Extensions:","text":"<ul> <li>Plugin System: Dynamic loading of new source types</li> <li>Caching Layer: Configuration and SQL result caching</li> <li>Monitoring Integration: Metrics and observability</li> <li>Multi-Catalog Management: Orchestrating multiple catalog deployments</li> </ul>"},{"location":"architecture/#architectural-evolution","title":"Architectural Evolution:","text":"<p>The current architecture is designed to accommodate these future needs without major restructuring, thanks to its separation of concerns and extensibility patterns.</p>"},{"location":"architecture/#conclusion","title":"Conclusion","text":"<p>Duckalog's architecture provides a robust, maintainable, and extensible foundation for building DuckDB catalogs from declarative configurations. The separation of concerns, config-driven design, and idempotent operations make it suitable for both development and production use cases, while the clear extension patterns support future growth and adaptation to new data sources and requirements.</p>"},{"location":"architecture/#getting-started-with-the-architecture","title":"Getting Started with the Architecture","text":""},{"location":"architecture/#for-new-developers","title":"For New Developers","text":"<ol> <li>Start with the system overview to understand Duckalog's purpose and high-level design</li> <li>Review the component descriptions to understand each module's responsibilities</li> <li>Follow the data flow to see how configuration becomes a catalog</li> <li>Examine the design patterns to understand architectural decisions</li> <li>Look at extension examples if you need to add new functionality</li> </ol>"},{"location":"architecture/#for-contributors","title":"For Contributors","text":"<ul> <li>Code contributions should respect the separation of concerns</li> <li>New source types follow the documented extension patterns</li> <li>Architecture changes require OpenSpec proposals</li> <li>Documentation updates should maintain consistency across documents</li> </ul>"},{"location":"architecture/#for-system-integrators","title":"For System Integrators","text":"<ul> <li>API stability is maintained through well-defined interfaces</li> <li>Configuration evolution follows semantic versioning principles</li> <li>Extension points are documented and tested</li> <li>Error handling provides actionable feedback for troubleshooting</li> </ul>"},{"location":"architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>User Guide: How to use Duckalog in practice</li> <li>API Reference: Complete API documentation</li> <li>Examples: See the <code>examples/</code> directory for configuration samples</li> </ul>"},{"location":"best-practices-path-management/","title":"Best Practices: Path Management in Duckalog","text":"<p>This guide provides comprehensive best practices for managing paths in Duckalog configurations to ensure security, portability, maintainability, and cross-platform compatibility.</p>"},{"location":"best-practices-path-management/#executive-summary","title":"Executive Summary","text":"<ul> <li>Use relative paths for local data files</li> <li>Organize data logically relative to your configuration</li> <li>Validate path accessibility regularly</li> <li>Implement security boundaries with reasonable limits</li> <li>Consider cross-platform compatibility in all paths</li> <li>Use environment variables for external dependencies</li> </ul>"},{"location":"best-practices-path-management/#path-strategy","title":"Path Strategy","text":""},{"location":"best-practices-path-management/#recommended-relative-first-approach","title":"\u2705 Recommended: Relative-First Approach","text":"<pre><code># Recommended structure\nversion: 1\n\nduckdb:\n  database: analytics.duckdb\n\nviews:\n  - name: users\n    source: parquet\n    uri: \"data/users.parquet\"  # \u2705 Relative - portable and predictable\n\n  - name: events\n    source: parquet\n    uri: \"../shared/events/*.parquet\"  # \u2705 Reasonable parent traversal\n\n  - name: cloud_backup\n    source: parquet\n    uri: \"s3://company-bucket/data/*.parquet\"  # \u2705 Remote URI unchanged\n</code></pre> <p>Benefits: - \u2705 Works from any working directory - \u2705 Portable across development environments - \u2705 Easy to version control and share - \u2705 Clear project structure</p>"},{"location":"best-practices-path-management/#avoid-absolute-path-dependencies","title":"\u274c Avoid: Absolute Path Dependencies","text":"<pre><code># Anti-pattern\nversion: 1\n\nduckdb:\n  database: \"/opt/analytics/analytics.duckdb\"  # \u274c Hardcoded absolute path\n\nviews:\n  - name: users\n    source: parquet\n    uri: \"/home/user/project/data/users.parquet\"  # \u274c User-specific path\n\n  - name: events\n    source: parquet\n    uri: \"C:\\\\Projects\\\\Analytics\\\\Data\\\\events.parquet\"  # \u274c Windows-specific\n</code></pre> <p>Problems: - \u274c Fails when run from different working directories - \u274c Not portable across environments - \u274c Breaks in CI/CD pipelines - \u274c Team coordination required</p>"},{"location":"best-practices-path-management/#project-organization-structure","title":"Project Organization Structure","text":""},{"location":"best-practices-path-management/#recommended-directory-layout","title":"Recommended Directory Layout","text":"<pre><code>project-root/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 catalog.yaml              # Main configuration file\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 .gitignore\n\u2502\n\u251c\u2500\u2500 data/                     # Local data files\n\u2502   \u251c\u2500\u2500 raw/                  # Raw data sources\n\u2502   \u2502   \u251c\u2500\u2500 users.parquet\n\u2502   \u2502   \u251c\u2500\u2500 events/\n\u2502   \u2502   \u2514\u2500\u2500 transactions.parquet\n\u2502   \u251c\u2500\u2500 processed/            # Processed/transformed data\n\u2502   \u2502   \u251c\u2500\u2500 daily/\n\u2502   \u2502   \u2514\u2500\u2500 weekly/\n\u2502   \u2514\u2500\u2500 reference/            # Reference/lookup data\n\u2502       \u251c\u2500\u2500 geographies.parquet\n\u2502       \u2514\u2500\u2500 product-catalog.parquet\n\u2502\n\u251c\u2500\u2500 reference/                # Reference databases\n\u2502   \u251c\u2500\u2500 lookup-dbs/\n\u2502   \u2514\u2500\u2500 external-references/\n\u2502\n\u251c\u2500\u2500 sql/                      # SQL files for complex views\n\u2502   \u251c\u2500\u2500 views/\n\u2502   \u2514\u2500\u2500 procedures/\n\u2502\n\u251c\u2500\u2500 notebooks/                # Jupyter notebooks (optional)\n\u251c\u2500\u2500 scripts/                  # Utility scripts\n\u2514\u2500\u2500 tests/                    # Test files\n</code></pre>"},{"location":"best-practices-path-management/#configuration-mapping","title":"Configuration Mapping","text":"<pre><code>version: 1\n\nduckdb:\n  database: \"analytics.duckdb\"\n\nattachments:\n  # Local reference databases\n  duckdb:\n    - alias: lookups\n      path: \"reference/lookup-dbs/main-reference.duckdb\"\n      read_only: true\n\nviews:\n  # Raw data sources\n  - name: raw_users\n    source: parquet\n    uri: \"data/raw/users.parquet\"\n\n  - name: raw_events\n    source: parquet\n    uri: \"data/raw/events/*.parquet\"\n\n  # Processed data\n  - name: daily_metrics\n    source: parquet\n    uri: \"data/processed/daily/*.parquet\"\n\n  # Reference data\n  - name: geographies\n    source: parquet\n    uri: \"data/reference/geographies.parquet\"\n\n  # Complex views using reference data\n  - name: enriched_users\n    sql: |\n      SELECT u.*, g.country_name, g.region\n      FROM raw_users u\n      LEFT JOIN lookups.geographic_reference g ON u.geo_id = g.id\n</code></pre>"},{"location":"best-practices-path-management/#cross-platform-compatibility","title":"Cross-Platform Compatibility","text":""},{"location":"best-practices-path-management/#use-forward-slash-separators","title":"Use Forward Slash Separators","text":"<pre><code># \u2705 Cross-platform compatible\nviews:\n  - name: data_files\n    source: parquet\n    uri: \"data/subdirectory/files.parquet\"\n\n# \u274c Windows-only (not recommended)\nviews:\n  - name: windows_files\n    source: parquet\n    uri: \"data\\\\subdirectory\\\\files.parquet\"\n</code></pre>"},{"location":"best-practices-path-management/#avoid-platform-specific-patterns","title":"Avoid Platform-Specific Patterns","text":"<pre><code># \u2705 Platform-agnostic\nattachments:\n  duckdb:\n    - alias: reference\n      path: \"reference/data.duckdb\"\n\n# \u274c Platform-specific\nattachments:\n  duckdb:\n    - alias: reference\n      path: \"C:\\\\Data\\\\reference\\\\data.duckdb\"  # Windows only\n      path: \"/var/data/reference/data.duckdb\"   # Unix only\n</code></pre>"},{"location":"best-practices-path-management/#environment-specific-paths","title":"Environment-Specific Paths","text":"<pre><code># \u2705 Flexible with environment variables\nversion: 1\n\nduckdb:\n  database: \"${env:DB_PATH:analytics.duckdb}\"\n\nattachments:\n  duckdb:\n    - alias: reference\n      path: \"${env:REF_PATH:./reference.duckdb}\"\n      read_only: true\n\nviews:\n  - name: data\n    source: parquet\n    uri: \"${env:DATA_DIR:data}/*.parquet\"\n</code></pre>"},{"location":"best-practices-path-management/#security-best-practices","title":"Security Best Practices","text":""},{"location":"best-practices-path-management/#security-first-path-design","title":"Security-First Path Design","text":"<pre><code># \u2705 Secure path patterns\nviews:\n  - name: safe_local_data\n    source: parquet\n    uri: \"data/safe-data.parquet\"  # Within project bounds\n\n  - name: safe_shared_data\n    source: parquet\n    uri: \"../shared/safe-data.parquet\"  # Reasonable parent traversal\n\n  - name: remote_secure\n    source: parquet\n    uri: \"s3://secure-company-data/analytics/*.parquet\"  # Authenticated remote\n</code></pre>"},{"location":"best-practices-path-management/#dangerous-patterns-to-avoid","title":"Dangerous Patterns to Avoid","text":"<pre><code># \u274c Dangerous anti-patterns\nviews:\n  - name: dangerous_traversal\n    source: parquet\n    uri: \"../../../../etc/passwd\"  # \u274c Excessive traversal - BLOCKED\n\n  - name: system_paths\n    source: parquet\n    uri: \"/etc/shadow\"  # \u274c System file access - BLOCKED\n\n  - name: wildcard_danger\n    source: parquet\n    uri: \"../../../**/*\"  # \u274c Too broad - RISKY\n</code></pre>"},{"location":"best-practices-path-management/#security-validation","title":"Security Validation","text":"<pre><code># Regular security checks\nduckalog validate catalog.yaml                    # Basic validation\nduckalog validate-paths catalog.yaml --verbose   # Full path security check\n</code></pre>"},{"location":"best-practices-path-management/#team-collaboration-guidelines","title":"Team Collaboration Guidelines","text":""},{"location":"best-practices-path-management/#standardize-path-conventions","title":"Standardize Path Conventions","text":"<pre><code># Team-wide standard conventions\nversion: 1\n\nduckdb:\n  database: \"{team}-{project}.duckdb\"  # Template naming\n\nviews:\n  # Naming convention: {data_type}_{source}_{timeframe}\n  - name: raw_events_prod_2024\n    source: parquet\n    uri: \"data/raw/events/production/2024/*.parquet\"\n\n  - name: processed_metrics_daily\n    source: parquet\n    uri: \"data/processed/metrics/daily/*.parquet\"\n</code></pre>"},{"location":"best-practices-path-management/#documentation-standards","title":"Documentation Standards","text":"<pre><code># Include path documentation for team clarity\nversion: 1\n\nviews:\n  - name: customer_data\n    source: parquet\n    uri: \"data/customers/current/*\"\n    description: |\n      Current customer data from CRM system.\n      Updated daily via ETL pipeline.\n      Location: data/customers/current/\n      Owner: data-engineering-team\n      SLA: Updated by 06:00 UTC daily\n\n  - name: external_reference\n    source: parquet\n    uri: \"../shared/reference/geographies.parquet\"\n    description: |\n      Shared geographic reference data maintained by data governance team.\n      Location: ../shared/reference/\n      Contact: data-governance@company.com\n      Update frequency: Weekly\n</code></pre>"},{"location":"best-practices-path-management/#performance-optimization","title":"Performance Optimization","text":""},{"location":"best-practices-path-management/#organize-for-query-performance","title":"Organize for Query Performance","text":"<pre><code># Optimized data layout for performance\nversion: 1\n\nviews:\n  # Partitioned data for efficient querying\n  - name: partitioned_events\n    source: parquet\n    uri: \"data/events/by-date/year=*/month=*/day=*/*.parquet\"\n    description: \"Events partitioned by date for time-based queries\"\n\n  # Pre-aggregated data for common queries\n  - name: daily_summary\n    source: parquet\n    uri: \"data/summaries/daily/*.parquet\"\n    description: \"Pre-computed daily summaries for faster dashboard access\"\n\n  # Reference data optimized with DuckDB features\n  - name: indexed_lookups\n    source: parquet\n    uri: \"data/reference/indexed-tables/*.parquet\"\n    description: \"Reference tables with DuckDB-friendly indexing\"\n</code></pre>"},{"location":"best-practices-path-management/#path-based-performance-considerations","title":"Path-Based Performance Considerations","text":"<pre><code># \u2705 Efficient path patterns\nviews:\n  - name: efficient_globbing\n    source: parquet\n    uri: \"data/events/2024-*.parquet\"  # \u2705 Specific, manageable glob\n\n  - name: targeted_partitions\n    source: parquet\n    uri: \"data/partitions/date=2024-01-*/*.parquet\"  # \u2705 Hive-style partitions\n\n# \u274c Inefficient or problematic patterns\nviews:\n  - name: too_broad\n    source: parquet\n    uri: \"data/**/*\"  # \u274c Too broad, performance issues\n\n  - name: deep_wildcard\n    source: parquet\n    uri: \"**/events/**/*.parquet\"  # \u274c Excessive recursion\n</code></pre>"},{"location":"best-practices-path-management/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"best-practices-path-management/#configuration-for-pipelines","title":"Configuration for Pipelines","text":"<pre><code># Pipeline-friendly configuration\nversion: 1\n\nduckdb:\n  database: \"${env:CATALOG_NAME:ci-test-catalog}.duckdb\"\n\nviews:\n  - name: test_data\n    source: parquet\n    uri: \"data/test/sample-data.parquet\"\n    description: \"Small dataset for pipeline testing\"\n\n  - name: production_data\n    source: parquet\n    uri: \"${env:PROD_DATA_PATH:data/production}/*.parquet\"\n    description: \"Production data (environment-specific)\"\n</code></pre>"},{"location":"best-practices-path-management/#pipeline-validation-steps","title":"Pipeline Validation Steps","text":"<pre><code># .github/workflows/validate-catalog.yml\nname: Validate Catalog Configuration\n\non: [push, pull_request]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Setup Python\n      uses: actions/setup-python@v3\n      with:\n        python-version: '3.9'\n\n    - name: Install Duckalog\n      run: pip install duckalog\n\n    - name: Validate Configuration\n      run: duckalog validate catalog.yaml\n\n    - name: Check Path Accessibility\n      run: duckalog validate-paths catalog.yaml\n\n    - name: Test from Different Directories\n      run: |\n        # Test path resolution from different working directories\n        cd /tmp\n        duckalog validate ${{ github.workspace }}/catalog.yaml\n        duckalog show-paths ${{ github.workspace }}/catalog.yaml\n</code></pre>"},{"location":"best-practices-path-management/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":""},{"location":"best-practices-path-management/#regular-validation-script","title":"Regular Validation Script","text":"<pre><code>#!/bin/bash\n# validate-all-paths.sh - Regular path validation\n\necho \"\ud83d\udd0d Validating all catalog configurations...\"\n\n# Find all catalog files\nfind . -name \"catalog*.yaml\" -o -name \"catalog*.yml\" | while read catalog; do\n    echo \"\"\n    echo \"\ud83d\udccb Validating: $catalog\"\n\n    if duckalog validate \"$catalog\"; then\n        echo \"\u2705 Configuration valid\"\n\n        # Check path accessibility\n        if duckalog validate-paths \"$catalog\"; then\n            echo \"\u2705 All paths accessible\"\n        else\n            echo \"\u274c Path issues detected\"\n        fi\n    else\n        echo \"\u274c Configuration invalid\"\n    fi\ndone\n\necho \"\"\necho \"\ud83c\udfc1 Validation complete\"\n</code></pre>"},{"location":"best-practices-path-management/#documentation-maintenance","title":"Documentation Maintenance","text":"<pre><code># Include version and last updated information\nversion: 1\n\n# Metadata for tracking and maintenance\nmetadata:\n  config_version: \"1.2\"\n  last_updated: \"2024-01-15\"\n  updated_by: \"data-engineering-team\"\n  environment: \"production\"\n\nviews:\n  - name: critical_data\n    source: parquet\n    uri: \"data/critical/current-data.parquet\"\n    metadata:\n      importance: \"critical\"\n      sla: \"99.9% uptime\"\n      owner: \"data-platform-team\"\n      last_verified: \"2024-01-15\"\n</code></pre>"},{"location":"best-practices-path-management/#migration-strategies","title":"Migration Strategies","text":""},{"location":"best-practices-path-management/#incremental-migration-path","title":"Incremental Migration Path","text":"<pre><code># Phase 1: Keep both old and new paths\nversion: 1\n\nviews:\n  - name: data_v1_legacy  # Legacy absolute path\n    source: parquet\n    uri: \"/opt/data/legacy/users.parquet\"\n    description: \"Legacy path for backward compatibility\"\n\n  - name: data_v2_new     # New relative path\n    source: parquet\n    uri: \"data/users.parquet\"\n    description: \"New relative path for future use\"\n\n# Phase 2: Migration verification\nviews:\n  - name: data_comparison  # Verify both sources match\n    sql: |\n      SELECT\n        COUNT(*) as legacy_count,\n        (SELECT COUNT(*) FROM data_v2_new) as new_count\n      FROM data_v1_legacy\n    description: \"Verify migration between legacy and new data sources\"\n\n# Phase 3: Transition to relative paths only\nviews:\n  - name: users_final     # Final relative path only\n    source: parquet\n    uri: \"data/users.parquet\"\n    description: \"Final configuration using relative paths only\"\n</code></pre>"},{"location":"best-practices-path-management/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"best-practices-path-management/#path-not-found-debugging","title":"Path Not Found Debugging","text":"<pre><code># Step 1: Show resolved paths\nduckalog show-paths catalog.yaml --check\n\n# Step 2: Check file existence manually\nls -la resolved/path/to/file.parquet\n\n# Step 3: Verify working directory context\npwd\ncat catalog.yaml | grep uri\n</code></pre>"},{"location":"best-practices-path-management/#cross-platform-issues","title":"Cross-Platform Issues","text":"<pre><code># Problem: Windows path separators\nuri: \"data\\\\subdir\\\\file.parquet\"  # \u274c May fail on macOS/Linux\n\n# Solution: Use forward slashes\nuri: \"data/subdir/file.parquet\"    # \u2705 Works everywhere\n</code></pre>"},{"location":"best-practices-path-management/#environment-variable-issues","title":"Environment Variable Issues","text":"<pre><code># Debug environment variables\necho $DATA_DIR\necho $DB_PATH\n\n# Test with defaults\nduckalog validate catalog.yaml  # Uses defaults if variables not set\n</code></pre>"},{"location":"best-practices-path-management/#tooling-and-automation","title":"Tooling and Automation","text":""},{"location":"best-practices-path-management/#path-validation-automation","title":"Path Validation Automation","text":"<pre><code># scripts/validate_paths.py\nimport sys\nfrom pathlib import Path\nfrom duckalog.config import load_config\nfrom duckalog.path_resolution import validate_file_accessibility\n\ndef validate_catalog_paths(catalog_path):\n    \"\"\"Validate all file paths in a catalog configuration.\"\"\"\n    try:\n        config = load_config(str(catalog_path))\n        issues = []\n\n        for view in config.views:\n            if view.uri and view.source in ('parquet', 'delta'):\n                is_accessible, error = validate_file_accessibility(view.uri)\n                if not is_accessible:\n                    issues.append((view.name, view.uri, error))\n\n        return issues\n    except Exception as e:\n        return [(\"config_error\", str(catalog_path), str(e))]\n\nif __name__ == \"__main__\":\n    catalog_path = Path(sys.argv[1])\n    issues = validate_catalog_paths(catalog_path)\n\n    if issues:\n        print(\"\u274c Path validation failed:\")\n        for name, path, error in issues:\n            print(f\"  {name}: {error}\")\n        sys.exit(1)\n    else:\n        print(\"\u2705 All paths valid\")\n</code></pre>"},{"location":"best-practices-path-management/#configuration-health-check","title":"Configuration Health Check","text":"<pre><code># scripts/health-check.sh\n#!/bin/bash\n\necho \"\ud83c\udfe5 Catalog Health Check\"\necho \"=======================\"\n\n# List all catalogs\ncatalogs=$(find . -name \"catalog*.yaml\" -o -name \"catalog*.yml\")\n\nfor catalog in $catalogs; do\n    echo \"\"\n    echo \"\ud83d\udccb Checking: $catalog\"\n\n    # Basic validation\n    if duckalog validate \"$catalog\" 2&gt;/dev/null; then\n        echo \"  \u2705 Valid configuration\"\n    else\n        echo \"  \u274c Invalid configuration\"\n        continue\n    fi\n\n    # Path validation\n    if duckalog validate-paths \"$catalog\" 2&gt;/dev/null; then\n        echo \"  \u2705 All paths accessible\"\n    else\n        echo \"  \u274c Path accessibility issues\"\n    fi\n\n    # Show resolved paths (for inspection)\n    echo \"  \ud83d\udcc1 Path resolution:\"\n    duckalog show-paths \"$catalog\" | grep \"Resolved:\" | head -3 | while read line; do\n        echo \"      $line\"\n    done\ndone\n</code></pre> <p>These best practices ensure your Duckalog configurations are secure, maintainable, portable, and performant across all environments and use cases.</p>"},{"location":"configuration-path-examples/","title":"Configuration Examples: Path Resolution","text":"<p>This document provides practical configuration examples demonstrating how to use path resolution effectively in various scenarios.</p>"},{"location":"configuration-path-examples/#simple-project-structure","title":"Simple Project Structure","text":""},{"location":"configuration-path-examples/#basic-relative-paths","title":"Basic Relative Paths","text":"<pre><code># catalog.yaml - in project root\nversion: 1\n\nduckdb:\n  database: analytics.duckdb\n  install_extensions:\n    - parquet\n\nviews:\n  - name: users\n    source: parquet\n    uri: \"data/users.parquet\"\n    description: \"User profiles from parquet files\"\n\n  - name: events\n    source: parquet\n    uri: \"data/events/*.parquet\"\n    description: \"Event data with wildcard support\"\n</code></pre> <p>Project Structure: <pre><code>my-project/\n\u251c\u2500\u2500 catalog.yaml\n\u2514\u2500\u2500 data/\n    \u251c\u2500\u2500 users.parquet\n    \u2514\u2500\u2500 events/\n        \u251c\u2500\u2500 events_2024.parquet\n        \u2514\u2500\u2500 events_2023.parquet\n</code></pre></p>"},{"location":"configuration-path-examples/#configuration-with-directories","title":"Configuration with Directories","text":"<pre><code>version: 1\n\nduckdb:\n  database: multi_source.duckdb\n  install_extensions:\n    - parquet\n    - httpfs\n\nviews:\n  # Different data sources with different relative paths\n  - name: raw_data\n    source: parquet\n    uri: \"raw/consumer-events/*.parquet\"\n    description: \"Raw consumer event data\"\n\n  - name: processed_data\n    source: parquet\n    uri: \"processed/daily-aggregates.parquet\"\n    description: \"Processed daily aggregates\"\n\n  - name: reference_data\n    source: parquet\n    uri: \"reference/lookup-tables/*.parquet\"\n    description: \"Reference lookup tables\"\n\n  # Remote data (not affected by path resolution)\n  - name: cloud_backup\n    source: parquet\n    uri: \"s3://backup-bucket/analytics/*.parquet\"\n    description: \"Cloud backup data\"\n</code></pre> <p>Project Structure: <pre><code>data-platform/\n\u251c\u2500\u2500 catalog.yaml\n\u251c\u2500\u2500 raw/\n\u2502   \u2514\u2500\u2500 consumer-events/\n\u251c\u2500\u2500 processed/\n\u2502   \u2514\u2500\u2500 daily-aggregates.parquet\n\u2514\u2500\u2500 reference/\n    \u2514\u2500\u2500 lookup-tables/\n</code></pre></p>"},{"location":"configuration-path-examples/#multi-project-collaboration","title":"Multi-Project Collaboration","text":""},{"location":"configuration-path-examples/#shared-resources-structure","title":"Shared Resources Structure","text":"<pre><code># team-a/analytics.yaml\nversion: 1\n\nduckdb:\n  database: team_a_analytics.duckdb\n\nviews:\n  - name: our_events\n    source: parquet\n    uri: \"data/events/*.parquet\"\n    description: \"Team A event data\"\n\n  - name: shared_reference\n    source: parquet\n    uri: \"../shared-data/reference/geo-boundaries.parquet\"\n    description: \"Shared geographic reference data\"\n\n  - name: company_lookups\n    source: parquet\n    uri: \"../shared-data/lookups/product-categories.parquet\"\n    description: \"Company-wide product categories\"\n</code></pre> <p>Multi-Project Structure: <pre><code>company-analytics/\n\u251c\u2500\u2500 shared-data/\n\u2502   \u251c\u2500\u2500 reference/\n\u2502   \u2502   \u2514\u2500\u2500 geo-boundaries.parquet\n\u2502   \u2514\u2500\u2500 lookups/\n\u2502       \u2514\u2500\u2500 product-categories.parquet\n\u251c\u2500\u2500 team-a/\n\u2502   \u251c\u2500\u2500 analytics.yaml\n\u2502   \u2514\u2500\u2500 data/\n\u2514\u2500\u2500 team-b/\n    \u251c\u2500\u2500 analytics.yaml\n    \u2514\u2500\u2500 data/\n</code></pre></p>"},{"location":"configuration-path-examples/#team-b-configuration","title":"Team-B Configuration","text":"<pre><code># team-b/analytics.yaml\nversion: 1\n\nduckdb:\n  database: team_b_analytics.duckdb\n\nviews:\n  - name: our_metrics\n    source: parquet\n    uri: \"data/metrics/*.parquet\"\n    description: \"Team B metrics data\"\n\n  - name: same_geo_reference\n    source: parquet\n    uri: \"../shared-data/reference/geo-boundaries.parquet\"\n    description: \"Same reference data used by Team A\"\n\n  - name: shared_products\n    source: parquet\n    uri: \"../shared-data/lookups/product-categories.parquet\"\n    description: \"Shared product categories\"\n</code></pre>"},{"location":"configuration-path-examples/#advanced-path-patterns","title":"Advanced Path Patterns","text":""},{"location":"configuration-path-examples/#environment-variable-integration","title":"Environment Variable Integration","text":"<pre><code>version: 1\n\nduckdb:\n  database: \"${env:CATALOG_NAME:analytics}.duckdb\"\n\nviews:\n  - name: configurable_data\n    source: parquet\n    uri: \"${env:DATA_DIR:data}/*.parquet\"\n    description: \"Configurable data directory\"\n\n  - name: reference_tables\n    source: parquet\n    uri: \"${env:REF_DIR:reference}/{table}.parquet\"\n    description: \"Reference tables with variable substitution\"\n\nattachments:\n  duckdb:\n    - alias: historical_db\n      path: \"${env:HIST_DB_PATH:./historical.duckdb}\"\n      read_only: true\n</code></pre>"},{"location":"configuration-path-examples/#mixed-path-types","title":"Mixed Path Types","text":"<pre><code>version: 1\n\nduckdb:\n  database: mixed_paths.duckdb\n\n# Local attachments with relative paths\nattachments:\n  duckdb:\n    - alias: local_reference\n      path: \"reference/data.duckdb\"\n      read_only: true\n\n  # External database with absolute path\n  duckdb:\n    - alias: corporate_reference\n      path: \"/shared/data/corporate-reference.duckdb\"\n      read_only: true\n\nviews:\n  # Local data using relative paths\n  - name: local_parquet\n    source: parquet\n    uri: \"data/local-parquet/*.parquet\"\n    description: \"Local parquet data\"\n\n  # Analysis views using local data\n  - name: local_analysis\n    sql: |\n      SELECT *\n      FROM local_parquet lp\n      JOIN local_reference.reference_data rd ON lp.id = rd.id\n    description: \"Local data analysis\"\n\n  # Analysis using corporate reference\n  - name: corporate_analysis\n    sql: |\n      SELECT *\n      FROM local_parquet lp\n      JOIN corporate_reference.company_dimensions cd ON lp.company_id = cd.id\n    description: \"Analysis with corporate reference data\"\n</code></pre>"},{"location":"configuration-path-examples/#cross-platform-considerations","title":"Cross-Platform Considerations","text":""},{"location":"configuration-path-examples/#windows-compatible-configuration","title":"Windows-Compatible Configuration","text":"<pre><code>version: 1\n\nduckdb:\n  database: cross-platform.duckdb\n\nviews:\n  # Use forward slashes for cross-platform compatibility\n  - name: universal_data\n    source: parquet\n    uri: \"data/universal/*.parquet\"\n    description: \"Works on Windows, macOS, Linux\"\n\n  - name: parent_directory\n    source: parquet\n    uri: \"../shared/parent-data.parquet\"\n    description: \"Parent directory traversal works on all platforms\"\n\n  - name: complex_relative\n    source: parquet\n    uri: \"data/subdir/more-levels/deep-data.parquet\"\n    description: \"Complex relative path structure\"\n\nattachments:\n  duckdb:\n    - alias: local_db\n      path: \"databases/reference.duckdb\"\n      read_only: true\n</code></pre>"},{"location":"configuration-path-examples/#performance-optimization","title":"Performance Optimization","text":""},{"location":"configuration-path-examples/#optimized-path-configuration","title":"Optimized Path Configuration","text":"<pre><code>version: 1\n\nduckdb:\n  database: performance-optimized.duckdb\n  pragmas:\n    - \"SET memory_limit='8GB'\"\n  install_extensions:\n    - parquet\n\nviews:\n  # Partitioned data with relative paths\n  - name: partitioned_events\n    source: parquet\n    uri: \"data/partitioned/events/year=*/month=*/*.parquet\"\n    description: \"Partitioned event data for efficient querying\"\n\n  - name: aggregated_daily\n    source: parquet\n    uri: \"data/aggregated/daily/*.parquet\"\n    description: \"Pre-aggregated daily data for faster queries\"\n\n  - name: reference_indexes\n    source: parquet\n    uri: \"data/reference/with-indexes/*.parquet\"\n    description: \"Reference data with DuckDB indexes\"\n\n  # Views for different query patterns\n  - name: quick_lookups\n    sql: |\n      SELECT u.*, r.category_name\n      FROM reference_indexes.users u\n      JOIN reference_indexes.reference r ON u.category_id = r.category_id\n    description: \"Optimized for quick lookups\"\n\n  - name: time_series_analysis\n    sql: |\n      SELECT \n        DATE(event_timestamp) as event_date,\n        COUNT(*) as event_count,\n        COUNT(DISTINCT user_id) as unique_users\n      FROM partitioned_events\n      WHERE event_timestamp &gt;= CURRENT_DATE - INTERVAL '90 days'\n      GROUP BY DATE(event_timestamp)\n      ORDER BY event_date DESC\n    description: \"Time series analysis on partitioned data\"\n</code></pre>"},{"location":"configuration-path-examples/#security-focused-configuration","title":"Security-Focused Configuration","text":""},{"location":"configuration-path-examples/#secure-path-management","title":"Secure Path Management","text":"<pre><code>version: 1\n\nduckdb:\n  database: secure-analytics.duckdb\n\nviews:\n  # Safe relative paths within project bounds\n  - name: secure_user_data\n    source: parquet\n    uri: \"data/anonymized-users.parquet\"\n    description: \"Anonymized user data (safe within project)\"\n\n  - name: secure_events\n    source: parquet\n    uri: \"data/processed-events.parquet\"\n    description: \"Processed events (sanitized data)\"\n\n  # Reference data from same project\n  - name: secure_reference\n    source: parquet\n    uri: \"reference/dimension-tables.parquet\"\n    description: \"Reference data within project security scope\"\n\n  # Remote secure access (no local path issues)\n  - name: secure_remote\n    source: parquet\n    uri: \"s3://secure-company-analytics/data/processed/*.parquet\"\n    description: \"Secure remote data via S3\"\n</code></pre>"},{"location":"configuration-path-examples/#development-vs-production","title":"Development vs Production","text":""},{"location":"configuration-path-examples/#development-configuration","title":"Development Configuration","text":"<pre><code># config-dev.yaml\nversion: 1\n\nduckdb:\n  database: \"dev_analytics.duckdb\"\n\nviews:\n  - name: dev_users\n    source: parquet\n    uri: \"data/users-sample.parquet\"\n    description: \"Sample user data for development\"\n\n  - name: dev_events\n    source: parquet\n    uri: \"../dev-datasets/events-2024-sample.parquet\"\n    description: \"Sample events from dev datasets\"\n\n  - name: mock_reference\n    source: parquet\n    uri: \"reference/mock-lookup-tables.parquet\"\n    description: \"Mock reference data for testing\"\n</code></pre>"},{"location":"configuration-path-examples/#production-configuration","title":"Production Configuration","text":"<pre><code># config-prod.yaml\nversion: 1\n\nduckdb:\n  database: \"${env:PROD_DB_PATH:/var/data/analytics-prod.duckdb}\"\n\nviews:\n  - name: prod_users\n    source: parquet\n    uri: \"data/users.parquet\"  # Same relative structure\n    description: \"Production user data\"\n\n  - name: prod_events\n    source: parquet\n    uri: \"../production-datasets/events/full-events.parquet\"  # Different relative target\n    description: \"Full production events dataset\"\n\n  - name: prod_reference\n    source: parquet\n    uri: \"reference/production-lookup-tables.parquet\"  # Different reference data\n    description: \"Production reference tables\"\n</code></pre>"},{"location":"configuration-path-examples/#error-handling-examples","title":"Error Handling Examples","text":""},{"location":"configuration-path-examples/#configuration-with-built-in-redundancy","title":"Configuration with Built-in Redundancy","text":"<pre><code>version: 1\n\nduckdb:\n  database: resilient-analytics.duckdb\n\nviews:\n  - name: primary_data\n    source: parquet\n    uri: \"data/primary-dataset.parquet\"\n    description: \"Primary data source\"\n\n  - name: backup_data\n    source: parquet\n    uri: \"../backup/primary-dataset.parquet\"\n    description: \"Backup data source (same schema)\"\n\n  - name: fallback_metric\n    sql: |\n      SELECT \n        COALESCE(p.user_count, b.user_count) as user_count,\n        COALESCE(p.event_count, b.event_count) as event_count,\n        CASE\n          WHEN p.user_count IS NOT NULL THEN 'primary'\n          WHEN b.user_count IS NOT NULL THEN 'backup'\n          ELSE 'no_data'\n        END as data_source\n      FROM (\n        SELECT COUNT(DISTINCT user_id) as user_count, COUNT(*) as event_count\n        FROM primary_data\n      ) p\n      FULL OUTER JOIN (\n        SELECT COUNT(DISTINCT user_id) as user_count, COUNT(*) as event_count\n        FROM backup_data\n      ) b ON 1=1\n    description: \"Resilient metrics with fallback to backup\"\n</code></pre>"},{"location":"configuration-path-examples/#migration-examples","title":"Migration Examples","text":""},{"location":"configuration-path-examples/#legacy-absolute-path-migration","title":"Legacy Absolute Path Migration","text":"<p>Legacy Configuration: <pre><code># Before migration\nversion: 1\n\nduckdb:\n  database: \"/opt/analytics/analytics.duckdb\"\n\nviews:\n  - name: users\n    source: parquet\n    uri: \"/opt/analytics/data/users.parquet\"\n\n  - name: events\n    source: parquet\n    uri: \"/opt/analytics/data/events/*.parquet\"\n</code></pre></p> <p>Migrated Configuration: <pre><code># After migration to relative paths\nversion: 1\n\nduckdb:\n  database: \"analytics.duckdb\"  # Relative to config location\n\nviews:\n  - name: users\n    source: parquet\n    uri: \"data/users.parquet\"  # Relative to config directory\n\n  - name: events\n    source: parquet\n    uri: \"data/events/*.parquet\"  # Relative pattern\n</code></pre></p>"},{"location":"configuration-path-examples/#validation-and-testing-examples","title":"Validation and Testing Examples","text":""},{"location":"configuration-path-examples/#configuration-with-test-and-production-sections","title":"Configuration with Test and Production Sections","text":"<pre><code>version: 1\n\nduckdb:\n  database: \"testable-analytics.duckdb\"\n\n# Development/testing views\nviews:\n  - name: test_users_sample\n    source: parquet\n    uri: \"data/test/users-small.parquet\"\n    description: \"Small sample for testing\"\n\n  - name: test_events_sample\n    source: parquet\n    uri: \"data/test/events-sample.parquet\"\n    description: \"Sample events for testing\"\n\n  # Production views (same structure, different data)\n  - name: prod_users\n    source: parquet\n    uri: \"data/production/users.parquet\"\n    description: \"Production user data\"\n\n  - name: prod_events\n    source: parquet\n    uri: \"data/production/events/*.parquet\"\n    description: \"Production events\"\n\n  # Validation view to test data integrity\n  - name: validate_data_quality\n    sql: |\n      SELECT \n        'test_users_sample' as data_source,\n        COUNT(*) as total_count,\n        COUNT(DISTINCT user_id) as unique_users,\n        MIN(CASE WHEN user_id IS NULL THEN 1 END) as null_user_ids\n      FROM test_users_sample\n\n      UNION ALL\n\n      SELECT \n        'prod_users' as data_source,\n        COUNT(*) as total_count,\n        COUNT(DISTINCT user_id) as unique_users,\n        MIN(CASE WHEN user_id IS NULL THEN 1 END) as null_user_ids\n      FROM prod_users\n    description: \"Data quality validation across environments\"\n</code></pre> <p>These configuration examples demonstrate how to use path resolution effectively in various scenarios, from simple projects to complex multi-team environments.</p>"},{"location":"migration-path-resolution/","title":"Migration Guide: Path Resolution","text":"<p>This guide helps existing Duckalog users migrate from absolute paths to relative paths to take advantage of the new automatic path resolution feature.</p>"},{"location":"migration-path-resolution/#whats-changed","title":"What's Changed","text":""},{"location":"migration-path-resolution/#before-legacy-behavior","title":"Before (Legacy Behavior)","text":"<ul> <li>Relative paths like <code>\"data/file.parquet\"</code> were passed directly to DuckDB</li> <li>Users had to ensure they ran commands from the correct working directory</li> <li>Paths would fail if the working directory changed</li> <li>Absolute paths were required for reliable operation</li> </ul>"},{"location":"migration-path-resolution/#after-new-behavior","title":"After (New Behavior)","text":"<ul> <li>Relative paths are automatically resolved relative to the configuration file's directory</li> <li>Commands work consistently regardless of working directory</li> <li>Paths are resolved securely with validation</li> <li>Both relative and absolute paths continue to work</li> </ul>"},{"location":"migration-path-resolution/#migration-benefits","title":"Migration Benefits","text":""},{"location":"migration-path-resolution/#portability","title":"\u2705 Portability","text":"<pre><code># Old way: Required specific working directory\nviews:\n  - name: users\n    source: parquet\n    uri: \"/home/project/data/users.parquet\"  # Only works from specific location\n\n# New way: Works from anywhere\nviews:\n  - name: users\n    source: parquet\n    uri: \"data/users.parquet\"  # Works from any working directory\n</code></pre>"},{"location":"migration-path-resolution/#simplified-project-structure","title":"\u2705 Simplified Project Structure","text":"<pre><code># Recommended structure:\n# my-project/\n# \u251c\u2500\u2500 catalog.yaml       # Configuration\n# \u2514\u2500\u2500 data/              # Data files\n#     \u251c\u2500\u2500 users.parquet\n#     \u2514\u2500\u2500 events.parquet\n\n# In catalog.yaml:\nviews:\n  - name: users\n    source: parquet\n    uri: \"data/users.parquet\"  # Simple and clear\n</code></pre>"},{"location":"migration-path-resolution/#better-collaboration","title":"\u2705 Better Collaboration","text":"<ul> <li>Team members can work from different locations</li> <li>No need to coordinate working directories</li> <li>Version control friendly paths</li> </ul>"},{"location":"migration-path-resolution/#migration-process","title":"Migration Process","text":""},{"location":"migration-path-resolution/#step-1-assess-current-configuration","title":"Step 1: Assess Current Configuration","text":"<p>First, identify all absolute paths in your configuration:</p> <pre><code># Search for absolute paths (Unix/Linux/macOS)\ngrep -r \"\\\"uri.*:/\" your-config.yaml\n\n# Search for absolute paths (Windows)\ngrep -r \"\\\"uri.*[A-Z]:\" your-config.yaml\n</code></pre>"},{"location":"migration-path-resolution/#step-2-plan-path-changes","title":"Step 2: Plan Path Changes","text":"<p>Create a mapping of old to new paths:</p> Old Path New Path Notes <code>/home/user/data/users.parquet</code> <code>data/users.parquet</code> Move data relative to config <code>C:\\project\\data\\users.parquet</code> <code>data/users.parquet</code> Same structure, different separator <code>/absolute/reference.db</code> <code>../shared/reference.db</code> If shared with other projects"},{"location":"migration-path-resolution/#step-3-reorganize-data-files","title":"Step 3: Reorganize Data Files","text":"<p>Move your data files to be relative to your configuration:</p> <pre><code># Create data directory structure\nmkdir -p data processed reference\n\n# Move files to appropriate locations\nmv /path/to/users.parquet data/\nmv /path/to/events.parquet processed/\nmv /path/to/reference.db reference/\n</code></pre>"},{"location":"migration-path-resolution/#step-4-update-configuration","title":"Step 4: Update Configuration","text":"<p>Update your configuration file with relative paths:</p> <p>Before: <pre><code>version: 1\nviews:\n  - name: users\n    source: parquet\n    uri: \"/home/user/project/data/users.parquet\"\n\n  - name: events\n    source: parquet\n    uri: \"/home/user/project/data/events.parquet\"\n\nattachments:\n  duckdb:\n    - alias: reference\n      path: \"/home/user/project/reference/reference.db\"\n</code></pre></p> <p>After: <pre><code>version: 1\nviews:\n  - name: users\n    source: parquet\n    uri: \"data/users.parquet\"\n\n  - name: events\n    source: parquet\n    uri: \"processed/events.parquet\"\n\nattachments:\n  duckdb:\n    - alias: reference\n      path: \"reference/reference.db\"\n</code></pre></p>"},{"location":"migration-path-resolution/#step-5-test-migration","title":"Step 5: Test Migration","text":"<p>Validate that your updated configuration works:</p> <pre><code># Test from the configuration directory\ncd /path/to/config\nduckalog validate catalog.yaml\n\n# Test from a different working directory\ncd /tmp\nduckalog validate /path/to/config/catalog.yaml\n</code></pre> <p>Both should work identically.</p>"},{"location":"migration-path-resolution/#advanced-migration-scenarios","title":"Advanced Migration Scenarios","text":""},{"location":"migration-path-resolution/#scenario-1-shared-data-between-projects","title":"Scenario 1: Shared Data Between Projects","text":"<p>Challenge: You have data shared between multiple projects</p> <p>Solution: Use relative paths with reasonable parent directory traversal</p> <pre><code># Project A\nviews:\n  - name: shared_lookup\n    source: parquet\n    uri: \"../shared-data/lookup.parquet\"  # Relative to project A\n\n# Project B\nviews:\n  - name: shared_lookup\n    source: parquet\n    uri: \"../shared-data/lookup.parquet\"  # Same path, works from both\n</code></pre>"},{"location":"migration-path-resolution/#scenario-2-environment-specific-paths","title":"Scenario 2: Environment-Specific Paths","text":"<p>Challenge: Different paths for dev/staging/production</p> <p>Solution: Use environment variables with relative path defaults</p> <pre><code># For development (default to relative paths)\nattachments:\n  duckdb:\n    - alias: reference\n      path: \"${env:REFERENCE_DB_PATH:./reference.duckdb}\"\n      read_only: true\n\n# For production (can override with absolute path if needed)\n# export REFERENCE_DB_PATH=/var/data/reference.duckdb\n</code></pre>"},{"location":"migration-path-resolution/#scenario-3-windows-to-cross-platform","title":"Scenario 3: Windows to Cross-Platform","text":"<p>Challenge: Moving from Windows absolute paths to cross-platform relative paths</p> <p>Before (Windows): <pre><code>views:\n  - name: users\n    source: parquet\n    uri: \"C:\\\\Users\\\\User\\\\project\\\\data\\\\users.parquet\"\n</code></pre></p> <p>After (Cross-platform): <pre><code>views:\n  - name: users\n    source: parquet\n    uri: \"data/users.parquet\"  # Works on Windows, macOS, Linux\n</code></pre></p>"},{"location":"migration-path-resolution/#rollback-plan","title":"Rollback Plan","text":"<p>If you encounter issues during migration:</p> <ol> <li>Keep backups: Save your original configuration before making changes</li> <li>Test incrementally: Migrate one path at a time and test each change</li> <li>Verify functionality: Run your full workflow after each change</li> </ol> <p>To rollback: <pre><code># Restore original configuration\ncp catalog.yaml.backup catalog.yaml\n\n# Restore original data organization\nmv data/* /original/data/location/\n</code></pre></p>"},{"location":"migration-path-resolution/#verification-checklist","title":"Verification Checklist","text":"<p>After migration, verify:</p> <ul> <li>[ ] <code>duckalog validate</code> works from any working directory</li> <li>[ ] <code>duckalog build</code> creates the expected views</li> <li>[ ] <code>duckalog generate-sql</code> produces correct SQL with absolute paths</li> <li>[ ] Web UI (if used) loads and functions correctly</li> <li>[ ] All data sources are accessible</li> <li>[ ] No security violations are reported</li> </ul>"},{"location":"migration-path-resolution/#troubleshooting","title":"Troubleshooting","text":""},{"location":"migration-path-resolution/#issue-file-does-not-exist","title":"Issue: \"File does not exist\"","text":"<p>Cause: Data files not moved to correct relative location</p> <p>Solution:  <pre><code># Check where paths resolve to\npython3 -c \"\nfrom duckalog.config import load_config\nconfig = load_config('catalog.yaml')\nfor view in config.views:\n    if view.uri:\n        print(f'{view.name}: {view.uri}')\n\"\n</code></pre></p>"},{"location":"migration-path-resolution/#issue-path-resolution-violates-security-rules","title":"Issue: \"Path resolution violates security rules\"","text":"<p>Cause: Path tries to escape reasonable boundaries</p> <p>Solution: Avoid excessive parent directory traversal: <pre><code># Bad\nuri: \"../../../etc/passwd\"  # Blocked for security\n\n# Good\nuri: \"../shared/data.parquet\"  # Reasonable parent traversal\n</code></pre></p>"},{"location":"migration-path-resolution/#issue-cross-platform-compatibility","title":"Issue: Cross-platform compatibility","text":"<p>Cause: Using platform-specific path separators</p> <p>Solution: Use forward slashes for cross-platform compatibility: <pre><code># Good (cross-platform)\nuri: \"data/users.parquet\"\n\n# Platform-specific\nuri: \"data\\\\users.parquet\"  # Windows only\n</code></pre></p>"},{"location":"migration-path-resolution/#getting-help","title":"Getting Help","text":"<p>If you encounter issues during migration:</p> <ol> <li>Check logs: Look for path resolution debug messages</li> <li>Validate step by step: Test each change individually</li> <li>Use absolute paths temporarily: You can always fall back to absolute paths</li> <li>Consult examples: See <code>examples/</code> directory for reference patterns</li> </ol>"},{"location":"migration-path-resolution/#examples-for-reference","title":"Examples for Reference","text":"<p>See these working examples of relative path usage:</p> <ul> <li>Simple Parquet: <code>examples/simple_parquet/catalog.yml</code></li> <li>Multi-Source Analytics: <code>examples/data-integration/multi-source-analytics/catalog.yaml</code></li> <li>DuckDB Performance: <code>examples/production-operations/duckdb-performance-settings/catalog-*.yaml</code></li> </ul> <p>Each example demonstrates different patterns for organizing and referencing data files with relative paths.</p>"},{"location":"path-resolution-examples/","title":"Path Resolution Examples","text":"<p>This document provides practical examples of how path resolution works in Duckalog configurations, showing both the problem that path resolution solves and the solutions it enables.</p>"},{"location":"path-resolution-examples/#basic-path-resolution-examples","title":"Basic Path Resolution Examples","text":""},{"location":"path-resolution-examples/#example-1-simple-relative-paths","title":"Example 1: Simple Relative Paths","text":"<p>Project Structure: <pre><code>my-analytics/\n\u251c\u2500\u2500 catalog.yaml\n\u2514\u2500\u2500 data/\n    \u251c\u2500\u2500 users.parquet\n    \u2514\u2500\u2500 events.parquet\n</code></pre></p> <p>Configuration (<code>catalog.yaml</code>): <pre><code>version: 1\n\nduckdb:\n  database: analytics.duckdb\n\nviews:\n  - name: users\n    source: parquet\n    uri: \"data/users.parquet\"  # Resolves to: /path/to/my-analytics/data/users.parquet\n    description: \"User data\"\n\n  - name: events\n    source: parquet\n    uri: \"./data/events.parquet\"  # Resolves to: /path/to/my-analytics/data/events.parquet\n    description: \"Event data\"\n</code></pre></p> <p>Result: - Can run <code>duckalog build</code> from any directory - Paths consistently resolve to files relative to <code>catalog.yaml</code> - Works on Windows, macOS, and Linux</p>"},{"location":"path-resolution-examples/#example-2-parent-directory-traversal","title":"Example 2: Parent Directory Traversal","text":"<p>Project Structure: <pre><code>projects/\n\u251c\u2500\u2500 shared-data/\n\u2502   \u251c\u2500\u2500 reference/\n\u2502   \u2502   \u251c\u2500\u2500 lookup-tables.parquet\n\u2502   \u2502   \u2514\u2500\u2500 geographies.parquet\n\u2502   \u2514\u2500\u2500 external/\n\u2502       \u2514\u2500\u2500 third-party-data.parquet\n\u2502\n\u251c\u2500\u2500 customer-analytics/\n\u2502   \u251c\u2500\u2500 catalog.yaml\n\u2502   \u2514\u2500\u2500 queries/\n\u2502       \u2514\u2500\u2500 customer-metrics.sql\n\u2502\n\u2514\u2500\u2500 product-analytics/\n    \u251c\u2500\u2500 catalog.yaml\n    \u2514\u2500\u2500 reports/\n        \u2514\u2500\u2500 product-performance.sql\n</code></pre></p> <p>Customer Analytics Configuration (<code>customer-analytics/catalog.yaml</code>): <pre><code>version: 1\n\nduckdb:\n  database: customer_analytics.duckdb\n\nviews:\n  - name: shared_lookup\n    source: parquet\n    uri: \"../shared-data/reference/lookup-tables.parquet\"\n    description: \"Shared lookup tables\"\n\n  - name: geographies\n    source: parquet\n    uri: \"../shared-data/reference/geographies.parquet\"\n    description: \"Geographic reference data\"\n\n  - name: external_data\n    source: parquet\n    uri: \"../shared-data/external/third-party-data.parquet\"\n    description: \"External third-party data\"\n</code></pre></p> <p>Result: - Both projects can access shared data using relative paths - Teams can work independently while sharing reference data - No absolute paths hardcoded in configurations</p>"},{"location":"path-resolution-examples/#example-3-mixed-absolute-and-relative-paths","title":"Example 3: Mixed Absolute and Relative Paths","text":"<p>Configuration: <pre><code>version: 1\n\nduckdb:\n  database: analytics.duckdb\n\n# Attachments with mixed path types\nattachments:\n  duckdb:\n    - alias: local_reference\n      path: \"reference/local.duckdb\"  # Relative path\n      read_only: true\n\n    - alias: corporate_reference\n      path: \"/data/reference/corporate.duckdb\"  # Absolute path (for shared resources)\n      read_only: true\n\nviews:\n  # Local data using relative paths\n  - name: local_events\n    source: parquet\n    uri: \"data/events.parquet\"\n    description: \"Local event data\"\n\n  # Corporate data using absolute paths\n  - name: corporate_users\n    source: parquet\n    uri: \"/data/corporate/users/calendar-2024/*.parquet\"\n    description: \"Corporate user data\"\n\n  # Remote data (not modified by path resolution)\n  - name: cloud_data\n    source: parquet\n    uri: \"s3://company-bucket/analytics/*.parquet\"\n    description: \"Cloud data via S3\"\n</code></pre></p>"},{"location":"path-resolution-examples/#cross-platform-examples","title":"Cross-Platform Examples","text":""},{"location":"path-resolution-examples/#example-4-cross-platform-development","title":"Example 4: Cross-Platform Development","text":"<p>Configuration (works on all platforms): <pre><code>version: 1\n\nduckdb:\n  database: cross-platform.duckdb\n\nviews:\n  # Use forward slashes for cross-platform compatibility\n  - name: users_mac_windows_linux\n    source: parquet\n    uri: \"data/users.parquet\"  # \u2705 Works on macOS, Windows, Linux\n    description: \"Cross-platform user data\"\n\n  # Avoid platform-specific patterns\n  - name: not_recommended\n    source: parquet\n    uri: \"data\\\\users.parquet\"  # \u274c Windows-specific\n    description: \"Windows-only path pattern\"\n</code></pre></p> <p>Result: - Same configuration works across development environments - Teams with mixed OS setups can share configurations - Forward slashes provide maximum compatibility</p>"},{"location":"path-resolution-examples/#environment-variable-integration","title":"Environment Variable Integration","text":""},{"location":"path-resolution-examples/#example-5-environment-variables-with-path-resolution","title":"Example 5: Environment Variables with Path Resolution","text":"<p>Configuration: <pre><code>version: 1\n\nduckdb:\n  database: \"${env:PROJECT_NAME:analytics}.duckdb\"\n\nattachments:\n  duckdb:\n    - alias: reference\n      path: \"${env:REFERENCE_PATH:./reference.duckdb}\"\n      read_only: true\n\nviews:\n  - name: data\n    source: parquet\n    uri: \"${env:DATA_DIR:data}/*.parquet\"\n    description: \"Data from configurable directory\"\n</code></pre></p> <p>Usage Scenarios:</p> <pre><code># Development (default values work)\nduckalog build catalog.yaml\n# resolves: uri=\"data/*.parquet\", path=\"./reference.duckdb\"\n\n# Production (override paths)\nexport DATA_DIR=/var/data/production\nexport REFERENCE_PATH=/var/reference/production\nexport PROJECT_NAME=prod_analytics\n\nduckalog build catalog.yaml\n# resolves: uri=\"/var/data/production/*.parquet\", path=\"/var/reference/production/production.duckdb\"\n</code></pre>"},{"location":"path-resolution-examples/#security-examples","title":"Security Examples","text":""},{"location":"path-resolution-examples/#example-6-security-validation-in-action","title":"Example 6: Security Validation in Action","text":"<p>Configuration with Security Issues: <pre><code>version: 1\n\nduckdb:\n  database: security-test.duckdb\n\nviews:\n  # \u2705 Safe relative paths\n  - name: safe_relative\n    source: parquet\n    uri: \"data/users.parquet\"\n    description: \"Safe relative path\"\n\n  - name: safe_parent_traversal\n    source: parquet\n    uri: \"../shared/data.parquet\"\n    description: \"Reasonable parent traversal\"\n\n  # \u274c Blocked for security\n  - name: excessive_traversal\n    source: parquet\n    uri: \"../../../../etc/passwd\"\n    description: \"Blocked: excessive parent traversal\"\n\n  # \u274c Blocked for security\n  - name: dangerous_system_path\n    source: parquet\n    uri: \"/etc/passwd\"\n    description: \"Blocked: dangerous system path\"\n</code></pre></p> <p>Validation Result: <pre><code>$ duckalog validate catalog.yaml\n\u274c Config error: Path resolution violates security rules: '../..//../../etc/passwd' resolves to '/etc/passwd' which is outside reasonable bounds\n\n# Safe paths:\n# \u2705 \"data/users.parquet\" \u2192 /project/data/users.parquet\n# \u2705 \"../shared/data.parquet\" \u2192 /shared/data.parquet\n# \u274c \"../../../../etc/passwd\" \u2192 BLOCKED\n</code></pre></p>"},{"location":"path-resolution-examples/#real-world-scenarios","title":"Real-World Scenarios","text":""},{"location":"path-resolution-examples/#example-7-data-lake-integration","title":"Example 7: Data Lake Integration","text":"<p>Organization Structure: <pre><code>company/\n\u251c\u2500\u2500 data-lake/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u2502   \u251c\u2500\u2500 events/\n\u2502   \u2502   \u2514\u2500\u2500 users/\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2502   \u251c\u2500\u2500 daily/\n\u2502   \u2502   \u2514\u2500\u2500 weekly/\n\u2502   \u2514\u2500\u2500 reference/\n\u2502       \u251c\u2500\u2500 geographies/\n\u2502       \u2514\u2500\u2500 products/\n\u2502\n\u251c\u2500\u2500 analytics/\n\u2502   \u251c\u2500\u2500 marketing/\n\u2502   \u2502   \u251c\u2500\u2500 catalog.yaml\n\u2502   \u2502   \u2514\u2500\u2500 metrics/\n\u2502   \u2514\u2500\u2500 finance/\n\u2502       \u251c\u2500\u2500 catalog.yaml\n\u2502       \u2514\u2500\u2500 reports/\n\u2502\n\u2514\u2500\u2500 dashboards/\n    \u251c\u2500\u2500 kpi/\n    \u2514\u2500\u2500 segment/\n</code></pre></p> <p>Marketing Analytics Configuration (<code>analytics/marketing/catalog.yaml</code>): <pre><code>version: 1\n\nduckdb:\n  database: marketing_analytics.duckdb\n  install_extensions:\n    - parquet\n\nviews:\n  # Raw event data\n  - name: raw_events\n    source: parquet\n    uri: \"../../data-lake/raw/events/*.parquet\"\n    description: \"Raw marketing events\"\n\n  # Processed data\n  - name: daily_metrics\n    source: parquet\n    uri: \"../../data-lake/processed/daily/metrics/*.parquet\"\n    description: \"Daily processed metrics\"\n\n  # Reference data\n  - name: product_reference\n    source: parquet\n    uri: \"../../data-lake/reference/products/*.parquet\"\n    description: \"Product reference data\"\n\n  # Derived analytics views\n  - name: campaign_performance\n    sql: |\n      SELECT\n        campaign_id,\n        event_date,\n        COUNT(*) as impressions,\n        COUNT(DISTINCT user_id) as unique_users,\n        SUM(CASE WHEN event_type = 'conversion' THEN 1 ELSE 0 END) as conversions\n      FROM raw_events\n      WHERE event_date &gt;= CURRENT_DATE - INTERVAL '90 days'\n      GROUP BY campaign_id, event_date\n    description: \"Marketing campaign performance\"\n</code></pre></p>"},{"location":"path-resolution-examples/#example-8-cicd-pipeline-integration","title":"Example 8: CI/CD Pipeline Integration","text":"<p>GitHub Actions Workflow: <pre><code>name: Validate and Build Catalogs\n\non: [push, pull_request]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v3\n      with:\n        python-version: '3.9'\n\n    - name: Install duckalog\n      run: pip install duckalog\n\n    - name: Validate configurations\n      run: |\n        # Validate from project directory\n        duckalog validate analytics/marketing/catalog.yaml\n        duckalog validate analytics/finance/catalog.yaml\n\n        # Validate from different working directory (tests path resolution)\n        cd /tmp\n        duckalog validate ${{ github.workspace }}/analytics/marketing/catalog.yaml\n\n    - name: Show resolved paths\n      run: |\n        # Show how paths are resolved (useful for debugging)\n        duckalog show-paths analytics/marketing/catalog.yaml\n\n    - name: Build catalogs\n      run: |\n        # Build to test actual path resolution\n        duckalog build analytics/marketing/catalog.yaml --dry-run &gt; marketing.sql\n        duckalog build analytics/finance/catalog.yaml --dry-run &gt; finance.sql\n</code></pre></p>"},{"location":"path-resolution-examples/#migration-examples","title":"Migration Examples","text":""},{"location":"path-resolution-examples/#example-9-from-absolute-to-relative","title":"Example 9: From Absolute to Relative","text":"<p>Before (Absolute Paths): <pre><code># Project structure:\n# /home/user/projects/analytics/\n# \u251c\u2500\u2500 catalog.yaml\n# \u2514\u2500\u2500 data/\n#     \u251c\u2500\u2500 users.parquet\n#     \u2514\u2500\u2500 events.parquet\n\nversion: 1\n\nduckdb:\n  database: analytics.duckdb\n\nviews:\n  - name: users\n    source: parquet\n    uri: \"/home/user/projects/analytics/data/users.parquet\"  # Hardcoded absolute path\n    description: \"User data\"\n\n  - name: events\n    source: parquet\n    uri: \"/home/user/projects/analytics/data/events.parquet\"  # Hardcoded absolute path\n    description: \"Event data\"\n</code></pre></p> <p>After (Relative Paths - Recommended): <pre><code>version: 1\n\nduckdb:\n  database: analytics.duckdb\n\nviews:\n  - name: users\n    source: parquet\n    uri: \"data/users.parquet\"  # Relative to config file\n    description: \"User data\"\n\n  - name: events\n    source: parquet\n    uri: \"data/events.parquet\"  # Relative to config file\n    description: \"Event data\"\n</code></pre></p>"},{"location":"path-resolution-examples/#example-10-migration-benefits","title":"Example 10: Migration Benefits","text":"<p>Old Workflow (Absolute Paths): <pre><code># Only works from specific directory\ncd /home/user/projects/analytics\nduckalog build catalog.yaml\n\n# Fails from other directories\ncd /tmp\nduckalog build /home/user/projects/analytics/catalog.yaml\n# \u274c Error: /home/user/projects/analytics/data/users.parquet not accessible from /tmp\n</code></pre></p> <p>New Workflow (Relative Paths): <pre><code># Works from any directory\nduckalog build /home/user/projects/analytics/catalog.yaml\n\ncd /tmp\nduckalog build /home/user/projects/analytics/catalog.yaml\n# \u2705 Success: Paths resolve correctly regardless of working directory\n\n# CI/CD friendly\ndocker run --rm -v /home/user/projects:/data analytics-runner duckalog build /data/analytics/catalog.yaml\n</code></pre></p>"},{"location":"path-resolution-examples/#troubleshooting-examples","title":"Troubleshooting Examples","text":""},{"location":"path-resolution-examples/#example-11-debugging-path-issues","title":"Example 11: Debugging Path Issues","text":"<p>Configuration with Issues: <pre><code>version: 1\n\nduckdb:\n  database: debug-example.duckdb\n\nviews:\n  - name: missing_file\n    source: parquet\n    uri: \"data/missing.parquet\"\n    description: \"File doesn't exist\"\n\n  - name: wrong_case\n    source: parquet\n    uri: \"data/Users.parquet\"  # Wrong case on macOS/Linux\n    description: \"Case sensitivity issue\"\n</code></pre></p> <p>Debug Commands: <pre><code># Show resolved paths to debug issues\n$ duckalog show-paths catalog.yaml\nConfiguration: catalog.yaml\nConfig directory: /Users/volker/coding/libs/duckalog/debug-example\n\nView Paths:\n--------------------------------------------------------------------------------\nmissing_file:\n  Original: data/missing.parquet\n  Resolved: /Users/volker/coding/libs/duckalog/debug-example/data/missing.parquet\n\nwrong_case:\n  Original: data/Users.parquet\n  Resolved: /Users/volker/coding/libs/duckalog/debug-example/data/Users.parquet\n\n# Check file accessibility\n$ duckalog validate-paths catalog.yaml --check\n\u2705 Configuration is valid.\n\nChecking file accessibility...\n--------------------------------------------------\n\u274c missing_file: File does not exist: /Users/volker/coding/libs/duckalog/debug-example/data/missing.parquet\n\u274c wrong_case: File does not exist: /Users/volker/coding/libs/duckalog/debug-example/data/Users.parquet\n\n\u274c Found 2 inaccessible files:\n  - missing_file: File does not exist: /Users/volker/coding/libs/duckalog/debug-example/data/missing.parquet\n  - wrong_case: File does not exist: /Users/volker/coding/libs/duckalog/debug-example/data/Users.parquet\n</code></pre></p> <p>These examples demonstrate the practical benefits of path resolution and provide patterns for different use cases and scenarios.</p>"},{"location":"path-resolution/","title":"Path Resolution Guide","text":"<p>This guide explains how Duckalog handles relative and absolute paths in configuration files, ensuring consistent behavior across different working environments.</p>"},{"location":"path-resolution/#overview","title":"Overview","text":"<p>Duckalog automatically resolves relative paths to absolute paths during configuration processing. This ensures that catalogs work consistently regardless of where the <code>duckalog</code> command is executed from.</p>"},{"location":"path-resolution/#how-path-resolution-works","title":"How Path Resolution Works","text":""},{"location":"path-resolution/#resolution-algorithm","title":"Resolution Algorithm","text":"<ol> <li>Path Detection: Determine if a path is relative, absolute, or a remote URI</li> <li>Relative Path Resolution: Resolve relative paths against the configuration file's directory</li> <li>Security Validation: Ensure resolved paths don't escape safe boundaries</li> <li>Path Normalization: Normalize paths for SQL generation</li> </ol>"},{"location":"path-resolution/#path-types","title":"Path Types","text":""},{"location":"path-resolution/#relative-paths-automatically-resolved","title":"Relative Paths (Automatically Resolved)","text":"<pre><code>views:\n  - name: users\n    source: parquet\n    uri: \"data/users.parquet\"  # \u2192 /path/to/config/data/users.parquet\n    description: \"Relative to config directory\"\n\n  - name: events\n    source: parquet\n    uri: \"./events.parquet\"     # \u2192 /path/to/config/events.parquet\n    description: \"Current directory relative to config\"\n\n  - name: shared\n    source: parquet\n    uri: \"../shared/data.parquet\"  # \u2192 /path/to/shared/data.parquet\n    description: \"Parent directory relative to config\"\n</code></pre>"},{"location":"path-resolution/#absolute-paths-preserved-unchanged","title":"Absolute Paths (Preserved Unchanged)","text":"<pre><code>views:\n  - name: fixed_data\n    source: parquet\n    uri: \"/absolute/path/data.parquet\"  # Used as-is\n    description: \"Unix absolute path\"\n\n  - name: windows_data\n    source: parquet\n    uri: \"C:\\\\data\\\\file.parquet\"  # Used as-is\n    description: \"Windows absolute path\"\n</code></pre>"},{"location":"path-resolution/#remote-uris-not-modified","title":"Remote URIs (Not Modified)","text":"<pre><code>views:\n  - name: s3_data\n    source: parquet\n    uri: \"s3://my-bucket/data/file.parquet\"  # Used as-is\n    description: \"S3 URI\"\n\n  - name: http_data\n    source: parquet\n    uri: \"https://example.com/data/file.parquet\"  # Used as-is\n    description: \"HTTP URL\"\n\n  - name: gcs_data\n    source: parquet\n    uri: \"gs://bucket/data/file.parquet\"  # Used as-is\n    description: \"Google Cloud Storage URI\"\n</code></pre>"},{"location":"path-resolution/#security-features","title":"Security Features","text":""},{"location":"path-resolution/#directory-traversal-protection","title":"Directory Traversal Protection","text":"<p>Duckalog prevents malicious path patterns that could escape the configuration directory:</p> <pre><code># These patterns are blocked for security:\nviews:\n  - name: blocked1\n    source: parquet\n    uri: \"../../../etc/passwd\"  # \u274c Blocked: excessive parent traversal\n\n  - name: blocked2\n    source: parquet\n    uri: \"/etc/passwd\"  # \u274c Blocked: dangerous system path\n</code></pre>"},{"location":"path-resolution/#allowed-patterns","title":"Allowed Patterns","text":"<pre><code># These patterns are safe and allowed:\nviews:\n  - name: normal_relative\n    source: parquet\n    uri: \"data/file.parquet\"  # \u2705 Safe: within config directory\n\n  - name: reasonable_parent\n    source: parquet\n    uri: \"../shared/file.parquet\"  # \u2705 Safe: reasonable parent traversal\n\n  - name: subdirectory\n    source: parquet\n    uri: \"subdir/deep/nested/file.parquet\"  # \u2705 Safe: within bounds\n</code></pre>"},{"location":"path-resolution/#cross-platform-compatibility","title":"Cross-Platform Compatibility","text":""},{"location":"path-resolution/#windows-paths","title":"Windows Paths","text":"<ul> <li>Drive Letters: <code>C:\\</code>, <code>D:\\</code> paths are treated as absolute</li> <li>UNC Paths: <code>\\\\server\\share</code> paths are preserved</li> <li>Path Separators: Both <code>/</code> and <code>\\</code> are supported</li> </ul>"},{"location":"path-resolution/#unixlinuxmacos-paths","title":"Unix/Linux/macOS Paths","text":"<ul> <li>Absolute Paths: Paths starting with <code>/</code> are absolute</li> <li>Relative Paths: Everything else follows relative resolution rules</li> </ul>"},{"location":"path-resolution/#examples","title":"Examples","text":"<pre><code># Windows\nviews:\n  - name: windows_absolute\n    source: parquet\n    uri: \"C:\\\\Users\\\\data\\\\file.parquet\"  # Absolute, used as-is\n\n  - name: windows_relative\n    source: parquet\n    uri: \"data\\\\file.parquet\"  # Resolved to config dir + data\\file.parquet\n\n# Unix/macOS\nviews:\n  - name: unix_absolute\n    source: parquet\n    uri: \"/home/user/data/file.parquet\"  # Absolute, used as-is\n\n  - name: unix_relative\n    source: parquet\n    uri: \"data/file.parquet\"  # Resolved to config dir + data/file.parquet\n</code></pre>"},{"location":"path-resolution/#best-practices","title":"Best Practices","text":""},{"location":"path-resolution/#1-use-relative-paths-for-local-data","title":"1. Use Relative Paths for Local Data","text":"<pre><code># Recommended: Relative paths for project data\nviews:\n  - name: users\n    source: parquet\n    uri: \"data/users.parquet\"  # \u2705 Portable and predictable\n</code></pre>"},{"location":"path-resolution/#2-organize-data-by-purpose","title":"2. Organize Data by Purpose","text":"<pre><code># Recommended: Structured data organization\nviews:\n  - name: raw_events\n    source: parquet\n    uri: \"raw-data/events/*.parquet\"\n\n  - name: processed_metrics\n    source: parquet\n    uri: \"processed/metrics/daily/*.parquet\"\n\n  - name: reference_lookup\n    source: parquet\n    uri: \"reference/lookup-tables/*.parquet\"\n</code></pre>"},{"location":"path-resolution/#3-use-environment-variables-for-external-paths","title":"3. Use Environment Variables for External Paths","text":"<pre><code># For external dependencies, use environment variables\nattachments:\n  duckdb:\n    - alias: reference\n      path: \"${env:REFERENCE_DB_PATH:./reference.duckdb}\"\n      read_only: true\n\nviews:\n  - name: external_data\n    source: parquet\n    uri: \"${env:DATA_ROOT}/external/file.parquet\"\n</code></pre>"},{"location":"path-resolution/#4-keep-configuration-and-data-together","title":"4. Keep Configuration and Data Together","text":"<pre><code># Project structure:\n# my-project/\n# \u251c\u2500\u2500 catalog.yaml          # Configuration file\n# \u251c\u2500\u2500 data/\n# \u2502   \u251c\u2500\u2500 users.parquet\n# \u2502   \u2514\u2500\u2500 events.parquet\n# \u2514\u2500\u2500 processed/\n#     \u2514\u2500\u2500 metrics.parquet\n\n# In catalog.yaml:\nviews:\n  - name: users\n    source: parquet\n    uri: \"data/users.parquet\"  # Resolves to my-project/data/users.parquet\n\n  - name: metrics\n    source: parquet\n    uri: \"processed/metrics.parquet\"  # Resolves to my-project/processed/metrics.parquet\n</code></pre>"},{"location":"path-resolution/#migration-from-absolute-paths","title":"Migration from Absolute Paths","text":"<p>If you have existing configurations with absolute paths, you can migrate to relative paths:</p>"},{"location":"path-resolution/#before-absolute-paths","title":"Before (Absolute Paths)","text":"<pre><code>version: 1\nviews:\n  - name: users\n    source: parquet\n    uri: \"/home/project/data/users.parquet\"  # Absolute path\n</code></pre>"},{"location":"path-resolution/#after-relative-paths","title":"After (Relative Paths)","text":"<pre><code>version: 1\nviews:\n  - name: users\n    source: parquet\n    uri: \"data/users.parquet\"  # Relative to config location\n</code></pre>"},{"location":"path-resolution/#migration-steps","title":"Migration Steps","text":"<ol> <li>Move data files relative to your config file location</li> <li>Update paths in your configuration from absolute to relative</li> <li>Test the configuration to ensure paths resolve correctly</li> </ol>"},{"location":"path-resolution/#troubleshooting","title":"Troubleshooting","text":""},{"location":"path-resolution/#common-issues","title":"Common Issues","text":""},{"location":"path-resolution/#path-not-found","title":"Path Not Found","text":"<pre><code># Error: File does not exist\n# Solution: Check that data files exist in the expected location\n</code></pre>"},{"location":"path-resolution/#security-violation","title":"Security Violation","text":"<pre><code># Error: Path resolution violates security rules\n# Solution: Avoid excessive parent directory traversal (../../../)\n</code></pre>"},{"location":"path-resolution/#platform-differences","title":"Platform Differences","text":"<pre><code># Windows-specific issue:\nuri: \"data\\\\file.parquet\"  # Use forward slashes for cross-platform compatibility\nuri: \"data/file.parquet\"   # \u2705 Better: works on all platforms\n</code></pre>"},{"location":"path-resolution/#debugging-path-resolution","title":"Debugging Path Resolution","text":"<p>To troubleshoot path issues:</p> <ol> <li>Verify file existence: Ensure data files exist in the expected locations</li> <li>Check path patterns: Use forward slashes for cross-platform compatibility</li> <li>Validate security: Avoid excessive parent directory traversal</li> <li>Test resolution: Run from different working directories to verify consistency</li> </ol>"},{"location":"path-resolution/#api-reference","title":"API Reference","text":""},{"location":"path-resolution/#path-resolution-functions","title":"Path Resolution Functions","text":""},{"location":"path-resolution/#is_relative_pathpath-str-bool","title":"<code>is_relative_path(path: str) -&gt; bool</code>","text":"<p>Detects if a path is relative based on platform-specific rules.</p>"},{"location":"path-resolution/#resolve_relative_pathpath-str-config_dir-path-str","title":"<code>resolve_relative_path(path: str, config_dir: Path) -&gt; str</code>","text":"<p>Resolves a relative path to an absolute path relative to the configuration directory.</p>"},{"location":"path-resolution/#validate_path_securitypath-str-config_dir-path-bool","title":"<code>validate_path_security(path: str, config_dir: Path) -&gt; bool</code>","text":"<p>Validates that resolved paths don't violate security boundaries.</p>"},{"location":"path-resolution/#normalize_path_for_sqlpath-str-str","title":"<code>normalize_path_for_sql(path: str) -&gt; str</code>","text":"<p>Normalizes a path for use in SQL statements, handling quoting and formatting.</p>"},{"location":"path-resolution/#integration-points","title":"Integration Points","text":"<p>Path resolution is automatically applied during: - Configuration Loading: All config files have paths resolved during validation - SQL Generation: Resolved paths are used in generated SQL statements - Attachment Processing: Attachment paths are resolved for local files</p>"},{"location":"path-resolution/#examples_1","title":"Examples","text":"<p>See the <code>examples/</code> directory for comprehensive examples of path resolution in action:</p> <ul> <li>Simple Parquet: Basic relative path usage</li> <li>Multi-Source Analytics: Complex path patterns across different sources</li> <li>Environment Variables Security: External path management</li> <li>DuckDB Performance Settings: Optimized path structures</li> </ul> <p>For hands-on learning, try these examples: <pre><code>cd examples/simple_parquet\npython gen_data.py\nduckalog build catalog.yml\n\ncd examples/data-integration/multi-source-analytics\npython generate_data.py\nduckalog build catalog.yaml\n</code></pre></p>"},{"location":"examples/","title":"Duckalog Examples","text":"<p>Welcome to the Duckalog examples collection! These practical examples demonstrate real-world usage patterns and help you get started with different Duckalog configurations.</p>"},{"location":"examples/#choosing-an-example","title":"Choosing an Example","text":"<p>Use this guide to find the right example for your use case:</p>"},{"location":"examples/#im-getting-started-with-duckalog","title":"I'm getting started with Duckalog","text":"<p>\u2192 Start with: Simple Parquet Example - Learn the basics of creating DuckDB views over Parquet files - Perfect for simple analytics without complex joins - Covers local files and S3 storage</p>"},{"location":"examples/#i-need-to-combine-multiple-local-databases","title":"I need to combine multiple local databases","text":"<p>\u2192 Use: Local Attachments Example - Attach DuckDB and SQLite databases - Join data across different local databases - Learn read-only attachment patterns</p>"},{"location":"examples/#i-have-data-in-multiple-sources-parquet-databases-cloud-storage","title":"I have data in multiple sources (Parquet + databases + cloud storage)","text":"<p>\u2192 Follow: Multi-Source Analytics Example - Comprehensive example with S3, PostgreSQL, Iceberg, and local databases - Real-world analytics workflow - Complex joins and business logic</p>"},{"location":"examples/#i-need-to-deploy-configs-across-different-environments","title":"I need to deploy configs across different environments","text":"<p>\u2192 Read: Environment Variables Example - Secure credential management - Development, staging, and production configurations - Docker and Kubernetes deployment patterns</p>"},{"location":"examples/#i-want-to-fine-tune-duckdb-performance-and-behavior","title":"I want to fine-tune DuckDB performance and behavior","text":"<p>\u2192 Explore: DuckDB Settings Example - Configure session-level settings beyond pragmas - Optimize threading, memory, and caching - Control DuckDB features and progress output</p>"},{"location":"examples/#i-need-to-manage-credentials-for-cloud-services-and-databases","title":"I need to manage credentials for cloud services and databases","text":"<p>\u2192 Use: DuckDB Secrets Example - Secure credential management for S3, Azure, GCS, and databases - Support for environment variable interpolation - Persistent and temporary secrets with scoping - Integration with attachments and Iceberg catalogs</p>"},{"location":"examples/#quick-comparison","title":"Quick Comparison","text":"Example Difficulty Data Sources Key Learning Simple Parquet \ud83d\udfe2 Beginner Parquet files Local Attachments \ud83d\udfe1 Intermediate DuckDB/SQLite Multi-Source Analytics \ud83d\udd34 Advanced Multiple sources Environment Variables \ud83d\udfe1 Intermediate Any DuckDB Settings \ud83d\udfe1 Intermediate Any DuckDB Secrets \ud83d\udfe1 Intermediate Any"},{"location":"examples/#prerequisites-for-all-examples","title":"Prerequisites for All Examples","text":""},{"location":"examples/#required-software","title":"Required Software","text":"<ul> <li>Python 3.12+ with Duckalog installed:   <pre><code>pip install duckalog\n</code></pre></li> </ul>"},{"location":"examples/#optional-but-recommended","title":"Optional but Recommended","text":"<ul> <li> <p>DuckDB CLI for interactive querying:   <pre><code># Install DuckDB CLI (optional)\n# Visit: https://duckdb.org/docs/installation/\n</code></pre></p> </li> <li> <p>AWS CLI (for S3 examples):   <pre><code>pip install awscli\naws configure\n</code></pre></p> </li> </ul>"},{"location":"examples/#example-categories","title":"Example Categories","text":""},{"location":"examples/#by-data-source","title":"By Data Source","text":"<p>Parquet Files Only - Simple Parquet - Perfect starting point - Multi-Source Analytics - Includes Parquet with other sources</p> <p>Local Databases - Local Attachments - DuckDB and SQLite focus - Multi-Source Analytics - Local databases with cloud sources</p> <p>Cloud Storage &amp; Data Lakes - Simple Parquet - S3 configuration - Multi-Source Analytics - S3 + Iceberg catalogs - Environment Variables - Cloud credential management</p> <p>Enterprise/Production - Multi-Source Analytics - Production-ready patterns - Environment Variables - Deployment and security</p>"},{"location":"examples/#by-use-case","title":"By Use Case","text":"<p>Data Analytics - Simple Parquet - Basic analytics - Local Attachments - Cross-database analytics - Multi-Source Analytics - Enterprise analytics</p> <p>Data Integration - Local Attachments - Local data unification - Multi-Source Analytics - Cloud + local integration</p> <p>Development &amp; Deployment - Environment Variables - Environment-specific configs - Multi-Source Analytics - Production deployment patterns</p>"},{"location":"examples/#common-patterns-across-examples","title":"Common Patterns Across Examples","text":"<p>All examples demonstrate these important Duckalog concepts:</p> <ol> <li>Configuration Structure - Consistent YAML patterns</li> <li>View Composition - Building complex analytics from simple views</li> <li>Performance Optimization - Memory limits, threading, pragmas</li> <li>Error Handling - Validation and troubleshooting</li> <li>Best Practices - Security, maintainability, scalability</li> </ol>"},{"location":"examples/#getting-started","title":"Getting Started","text":"<ol> <li>Choose your example based on your use case above</li> <li>Read the prerequisites in your chosen example</li> <li>Follow the step-by-step guide provided in each example</li> <li>Experiment with the configurations to match your needs</li> <li>Combine patterns from multiple examples as needed</li> </ol>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<p>After working through examples:</p> <ul> <li>Read the User Guide in <code>../guides/index.md</code> for comprehensive documentation</li> <li>Explore the API Reference in <code>../reference/index.md</code> for detailed function documentation</li> <li>Review the Architecture in <code>../architecture.md</code> for high-level design details</li> <li>Join the community for questions and discussions</li> </ul>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>Have a great Duckalog pattern to share? Consider contributing:</p> <ol> <li>Create a new example following the patterns shown here</li> <li>Include clear explanations and real-world scenarios</li> <li>Add troubleshooting sections for common issues</li> <li>Ensure examples work with minimal setup</li> <li>Link from this index page</li> </ol>"},{"location":"examples/#need-help","title":"Need Help?","text":"<ul> <li>Configuration Issues: Check troubleshooting sections in examples</li> <li>API Questions: See API Reference</li> <li>General Usage: Review User Guide</li> <li>Technical Details: Read Architecture</li> </ul> <p>Choose an example above to get started, or explore them in order to build your Duckalog expertise progressively!</p>"},{"location":"examples/duckdb-secrets/","title":"DuckDB Secrets Example","text":"<p>This example demonstrates how to use DuckDB secrets in Duckalog to manage credentials for external services like S3, Azure, GCS, and databases. Secrets provide a secure way to handle authentication without hardcoding credentials in SQL.</p>"},{"location":"examples/duckdb-secrets/#when-to-use-secrets","title":"When to Use Secrets","text":"<p>Choose secrets when you need to: - Access cloud storage services (S3, Azure, GCS) with credentials - Connect to databases (PostgreSQL, MySQL) using authentication - Manage credentials across different environments securely - Support automatic credential fetching (credential chains) - Create persistent secrets that survive database restarts</p>"},{"location":"examples/duckdb-secrets/#basic-secret-configuration","title":"Basic Secret Configuration","text":""},{"location":"examples/duckdb-secrets/#s3-secret-with-static-credentials","title":"S3 Secret with Static Credentials","text":"<p>Create a file called <code>secrets-example.yaml</code>:</p> <pre><code>version: 1\n\nduckdb:\n  database: secrets_catalog.duckdb\n\n  # Extensions required for S3 access\n  install_extensions:\n    - httpfs\n\n  # Secrets for external services\n  secrets:\n    - type: s3\n      name: production_s3\n      key_id: AKIAIOSFODNN7EXAMPLE\n      secret: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n      region: us-west-2\n      endpoint: s3.amazonaws.com  # Optional: custom endpoint\n\nviews:\n  - name: sales_data\n    source: parquet\n    uri: \"s3://my-production-bucket/sales/*.parquet\"\n    description: \"Sales data from S3 with secret authentication\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#azure-storage-secret","title":"Azure Storage Secret","text":"<pre><code>version: 1\n\nduckdb:\n  database: azure_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  secrets:\n    - type: azure\n      name: azure_prod\n      provider: config\n      persistent: true\n      scope: 'prod/'\n      connection_string: DefaultEndpointsProtocol=https;AccountName=myaccount;AccountKey=mykey;EndpointSuffix=core.windows.net\n      # Alternative: Use tenant_id + account_name + secret\n      # tenant_id: my-tenant-id\n      # account_name: mystorageaccount\n      # secret: my-azure-secret\n\nviews:\n  - name: azure_logs\n    source: parquet\n    uri: \"azure://mycontainer/logs/*.parquet\"\n    description: \"Application logs from Azure storage\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#gcs-secret","title":"GCS Secret","text":"<pre><code>version: 1\n\nduckdb:\n  database: gcs_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  secrets:\n    - type: gcs\n      name: gcs_service_account\n      key_id: my-service-account@project.iam.gserviceaccount.com\n      secret: '-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKw...\\n-----END PRIVATE KEY-----'\n      endpoint: storage.googleapis.com\n\nviews:\n  - name: gcs_data\n    source: parquet\n    uri: \"gs://my-bucket/data/*.parquet\"\n    description: \"Data from Google Cloud Storage\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#advanced-secret-configurations","title":"Advanced Secret Configurations","text":""},{"location":"examples/duckdb-secrets/#credential-chain-provider","title":"Credential Chain Provider","text":"<p>For automatic credential detection (useful in AWS environments):</p> <pre><code>version: 1\n\nduckdb:\n  database: auto_cred_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  secrets:\n    - type: s3\n      name: s3_auto\n      provider: credential_chain\n      region: us-east-1\n      # No key_id/secret needed - DuckDB will auto-detect\n\nviews:\n  - name: auto_s3_data\n    source: parquet\n    uri: \"s3://auto-bucket/data/*.parquet\"\n    description: \"S3 data with automatic credentials\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#database-secrets","title":"Database Secrets","text":""},{"location":"examples/duckdb-secrets/#postgresql-secret","title":"PostgreSQL Secret","text":"<pre><code>version: 1\n\nduckdb:\n  database: pg_catalog.duckdb\n\n  secrets:\n    - type: postgres\n      name: analytics_db\n      provider: config\n      persistent: true\n      connection_string: postgresql://user:password@localhost:5432/analytics\n      # Alternative: Individual parameters\n      # host: localhost\n      # port: 5432\n      # database: analytics\n      # key_id: user\n      # secret: password\n\nviews:\n  - name: postgres_users\n    source: postgres\n    database: analytics_db\n    table: users\n    description: \"Users from PostgreSQL with secret authentication\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#mysql-secret","title":"MySQL Secret","text":"<pre><code>version: 1\n\nduckdb:\n  database: mysql_catalog.duckdb\n\n  secrets:\n    - type: mysql\n      name: webapp_db\n      connection_string: mysql://user:password@db.example.com:3306/webapp\n\nviews:\n  - name: mysql_products\n    source: mysql\n    database: webapp_db\n    table: products\n    description: \"Products from MySQL with secret authentication\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#http-basic-auth-secret","title":"HTTP Basic Auth Secret","text":"<pre><code>version: 1\n\nduckdb:\n  database: api_catalog.duckdb\n\n  secrets:\n    - type: http\n      name: api_auth\n      key_id: my-api-username\n      secret: my-api-password\n      # Optional: Add custom headers\n      options:\n        custom_header: \"Bearer-Token\"\n        timeout: 30\n\nviews:\n  - name: api_data\n    sql: |\n      SELECT * FROM read_csv_auto('https://api.example.com/data.csv')\n    description: \"Data from HTTP API with basic authentication\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#s3-secret-with-options","title":"S3 Secret with Options","text":"<p>For advanced S3 configurations, use the <code>options</code> field to specify DuckDB-specific parameters:</p> <pre><code>version: 1\n\nduckdb:\n  database: s3_advanced_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  secrets:\n    - type: s3\n      name: advanced_s3\n      key_id: ${env:LODL_ACCESS_KEY_ID}\n      secret: ${env:LODL_SECRET_ACCESS_KEY}\n      endpoint: ${env:LODL_ENDPOINT_URL}\n      options:\n        use_ssl: true                # Enable/disable SSL (default: true)\n        url_style: path              # URL style: 'path' or 'virtual'\n        session_token: ${env:AWS_SESSION_TOKEN}  # For temporary credentials\n        region: us-east-1           # Override region in options\n\nviews:\n  - name: advanced_data\n    source: parquet\n    uri: \"s3://my-advanced-bucket/data/*.parquet\"\n    description: \"Data from S3 with advanced options configuration\"\n</code></pre> <p>Common S3 Options: - <code>use_ssl</code>: Enable/disable SSL encryption (use <code>false</code> for local testing) - <code>url_style</code>: URL style for S3-compatible storage (<code>path</code> for MinIO/other, <code>virtual</code> for standard AWS S3) - <code>session_token</code>: AWS temporary session token - <code>region</code>: AWS region override</p>"},{"location":"examples/duckdb-secrets/#environment-variable-integration","title":"Environment Variable Integration","text":""},{"location":"examples/duckdb-secrets/#using-environment-variables-for-security","title":"Using Environment Variables for Security","text":"<pre><code>version: 1\n\nduckdb:\n  database: env_secrets_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  secrets:\n    - type: s3\n      name: secure_s3\n      key_id: ${env:AWS_ACCESS_KEY_ID}\n      secret: ${env:AWS_SECRET_ACCESS_KEY}\n      region: ${env:AWS_DEFAULT_REGION}\n\n    - type: postgres\n      name: db_auth\n      connection_string: ${env:DATABASE_URL}\n\nviews:\n  - name: secure_data\n    source: parquet\n    uri: \"s3://secure-bucket/data/*.parquet\"\n    description: \"Secure data using environment variables\"\n</code></pre> <p>Set environment variables:</p> <pre><code># AWS Credentials\nexport AWS_ACCESS_KEY_ID=\"AKIAIOSFODNN7EXAMPLE\"\nexport AWS_SECRET_ACCESS_KEY=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\nexport AWS_DEFAULT_REGION=\"us-west-2\"\n\n# Database URL\nexport DATABASE_URL=\"postgresql://user:password@localhost:5432/analytics\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#understanding-the-options-field","title":"Understanding the Options Field","text":"<p>The <code>options</code> field is available for all secret types (S3, Azure, GCS, Database, HTTP) and allows you to specify additional key-value parameters that are specific to each secret type. This field is particularly useful for:</p> <ul> <li>DuckDB-specific parameters: Settings like <code>use_ssl</code>, <code>url_style</code> for S3</li> <li>Service-specific options: Custom parameters for different providers  </li> <li>Advanced configurations: Session tokens, custom headers, connection settings</li> </ul>"},{"location":"examples/duckdb-secrets/#how-the-options-field-works","title":"How the Options Field Works","text":"<p>The <code>options</code> field accepts a dictionary of key-value pairs:</p> <pre><code>secrets:\n  - type: s3\n    name: my_secret\n    key_id: ${env:MY_KEY}\n    secret: ${env:MY_SECRET}\n    options:\n      # S3-specific parameters\n      use_ssl: true\n      url_style: path\n      session_token: ${env:AWS_SESSION_TOKEN}\n</code></pre> <p>Key Points: - <code>options</code> works the same way for all secret types - The exact parameters available depend on the secret type - Environment variables can be used within options values - This field helps prevent validation errors when you need additional parameters</p>"},{"location":"examples/duckdb-secrets/#best-practices-for-using-options","title":"Best Practices for Using Options","text":"<ol> <li> <p>Use Environment Variables: Keep secrets secure by using environment variables within options:    <pre><code>options:\n  session_token: ${env:AWS_SESSION_TOKEN}\n</code></pre></p> </li> <li> <p>Documentation First: Check if a parameter is supported by the underlying service before adding it to options</p> </li> <li> <p>Consistent Configuration: Use similar option patterns across different secret types when available</p> </li> <li> <p>Testing: Test options configuration in non-production environments first</p> </li> <li> <p>Version Control: Document which options are required for your specific setup</p> </li> </ol>"},{"location":"examples/duckdb-secrets/#s3-options-usage-examples","title":"S3 Options Usage Examples","text":"<p>Different configurations require different S3 options. Here are common scenarios:</p>"},{"location":"examples/duckdb-secrets/#minios3-compatible-storage","title":"MinIO/S3-Compatible Storage","text":"<pre><code>secrets:\n  - type: s3\n    name: minio_storage\n    key_id: ${env:MINIO_ACCESS_KEY}\n    secret: ${env:MINIO_SECRET_KEY}\n    endpoint: http://minio-server:9000\n    options:\n      use_ssl: false          # Often disabled for local MinIO\n      url_style: path          # Path style common for MinIO\n</code></pre>"},{"location":"examples/duckdb-secrets/#aws-s3-with-session-token","title":"AWS S3 with Session Token","text":"<pre><code>secrets:\n  - type: s3\n    name: aws_s3_temp\n    key_id: ${env:AWS_ACCESS_KEY_ID}\n    secret: ${env:AWS_SECRET_ACCESS_KEY}\n    region: us-east-1\n    options:\n      session_token: ${env:AWS_SESSION_TOKEN}\n      url_style: virtual        # Virtual style for standard AWS S3\n</code></pre>"},{"location":"examples/duckdb-secrets/#custom-s3-endpoint","title":"Custom S3 Endpoint","text":"<pre><code>secrets:\n  - type: s3\n    name: custom_endpoint\n    key_id: ${env:MY_ACCESS_KEY}\n    secret: ${env:MY_SECRET_KEY}\n    endpoint: https://s3.example.com\n    options:\n      use_ssl: true\n      region: custom-region-1\n</code></pre>"},{"location":"examples/duckdb-secrets/#secret-types-reference","title":"Secret Types Reference","text":""},{"location":"examples/duckdb-secrets/#s3-secret-fields","title":"S3 Secret Fields","text":"Field Required Description <code>type</code> Yes Must be <code>\"s3\"</code> <code>key_id</code> For <code>config</code> provider AWS access key ID <code>secret</code> For <code>config</code> provider AWS secret access key <code>region</code> Optional AWS region (e.g., <code>us-west-2</code>) <code>endpoint</code> Optional Custom S3 endpoint <code>scope</code> Optional URL prefix for secret scope <code>provider</code> Optional <code>\"config\"</code> (default) or <code>\"credential_chain\"</code> <code>persistent</code> Optional Whether secret persists across sessions <code>options</code> Optional Additional S3-specific parameters (e.g., <code>use_ssl</code>, <code>url_style</code>)"},{"location":"examples/duckdb-secrets/#azure-secret-fields","title":"Azure Secret Fields","text":"Field Required Description <code>type</code> Yes Must be <code>\"azure\"</code> <code>connection_string</code> Either/or Full connection string <code>tenant_id</code> Either/or Azure AD tenant ID <code>account_name</code> Either/or Storage account name <code>secret</code> For explicit auth Account key or password <code>scope</code> Optional URL prefix for secret scope <code>options</code> Optional Additional Azure-specific parameters"},{"location":"examples/duckdb-secrets/#gcs-secret-fields","title":"GCS Secret Fields","text":"Field Required Description <code>type</code> Yes Must be <code>\"gcs\"</code> <code>key_id</code> For <code>config</code> provider Service account email <code>secret</code> For <code>config</code> provider Private key content <code>endpoint</code> Optional Custom GCS endpoint <code>scope</code> Optional URL prefix for secret scope <code>options</code> Optional Additional GCS-specific parameters"},{"location":"examples/duckdb-secrets/#database-secret-fields","title":"Database Secret Fields","text":"Field Required Description <code>type</code> Yes <code>\"postgres\"</code> or <code>\"mysql\"</code> <code>connection_string</code> Either/or Full database connection string <code>host</code> Either/or Database host <code>port</code> Either/or Database port <code>database</code> Either/or Database name <code>key_id</code> Either/or Database username <code>secret</code> Either/or Database password <code>options</code> Optional Additional database-specific parameters"},{"location":"examples/duckdb-secrets/#http-secret-fields","title":"HTTP Secret Fields","text":"Field Required Description <code>type</code> Yes Must be <code>\"http\"</code> <code>key_id</code> Yes Username for basic auth <code>secret</code> Yes Password for basic auth <code>options</code> Optional Additional HTTP headers/options"},{"location":"examples/duckdb-secrets/#best-practices","title":"Best Practices","text":""},{"location":"examples/duckdb-secrets/#security","title":"Security","text":"<ol> <li> <p>Use Environment Variables: Never hardcode secrets in configuration files    <pre><code># Good: Use environment variables\nkey_id: ${env:AWS_ACCESS_KEY_ID}\nsecret: ${env:AWS_SECRET_ACCESS_KEY}\n\n# Bad: Hardcode secrets\nkey_id: AKIAIOSFODNN7EXAMPLE\nsecret: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n</code></pre></p> </li> <li> <p>Separate Environments: Use different secrets for dev/staging/prod</p> </li> <li>Use Persistent Secrets: For long-running applications</li> <li>Limit Secret Scope: Use scope to restrict secret to specific paths</li> <li>Rotate Credentials: Update secrets regularly without changing configuration</li> </ol>"},{"location":"examples/duckdb-secrets/#performance","title":"Performance","text":"<ol> <li>Use Credential Chains: In cloud environments for automatic credential rotation</li> <li>Scope Secrets: Limit secrets to specific buckets/prefixes</li> <li>Persistent vs Temporary: Use persistent secrets for frequently accessed resources</li> <li>Connection Pooling: For database secrets, consider connection pooling</li> </ol>"},{"location":"examples/duckdb-secrets/#organization","title":"Organization","text":"<ol> <li>Name Secrets Clearly: Use descriptive names like <code>prod_s3</code>, <code>dev_postgres</code></li> <li>Group by Environment: Keep production, development, staging secrets separate</li> <li>Document Dependencies: Note which views depend on which secrets</li> <li>Version Control: Keep secret configurations out of version control</li> </ol>"},{"location":"examples/duckdb-secrets/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/duckdb-secrets/#common-issues","title":"Common Issues","text":"<p>1. Secret Not Found Error: <pre><code>Catalog Error: Secret with name 'my_secret' does not exist\n</code></pre> Solution: Check that the secret was created successfully and the name matches exactly.</p> <p>2. Permission Denied: <pre><code>Catalog Error: Permission denied\n</code></pre> Solution: Verify credentials, region, and IAM permissions for cloud services.</p> <p>3. Invalid Secret Configuration: <pre><code>Config Error: S3 config provider requires key_id and secret\n</code></pre> Solution: Ensure all required fields for the secret type are provided.</p> <p>4. Environment Variable Not Set: <pre><code>Config Error: Environment variable 'AWS_ACCESS_KEY_ID' is not set\n</code></pre> Solution: Set the required environment variables before running duckalog.</p>"},{"location":"examples/duckdb-secrets/#debugging-secrets","title":"Debugging Secrets","text":"<pre><code>-- List all secrets\nSELECT name, type, provider, persistent FROM duckdb_secrets();\n\n-- Check specific secret\nSELECT * FROM duckdb_secrets() WHERE name = 'my_secret';\n\n-- Test secret access\nSELECT * FROM read_csv_auto('s3://my-bucket/test.csv') LIMIT 1;\n</code></pre>"},{"location":"examples/duckdb-secrets/#integration-with-other-duckalog-features","title":"Integration with Other Duckalog Features","text":""},{"location":"examples/duckdb-secrets/#secrets-with-attachments","title":"Secrets with Attachments","text":"<pre><code>version: 1\n\nduckdb:\n  database: integrated_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  secrets:\n    - type: postgres\n      name: analytics_db\n      connection_string: ${env:ANALYTICS_DB_URL}\n\n  attachments:\n    postgres:\n      - alias: analytics\n        # Use the secret for authentication\n        # DuckDB will automatically use the postgres secret\n\nviews:\n  - name: analytics_data\n    source: postgres\n    database: analytics_db\n    table: sales\n    description: \"Analytics data using secret authentication\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#secrets-with-iceberg-catalogs","title":"Secrets with Iceberg Catalogs","text":"<pre><code>version: 1\n\nduckdb:\n  database: lakehouse_catalog.duckdb\n  install_extensions:\n    - httpfs\n    - iceberg\n\n  secrets:\n    - type: s3\n      name: lakehouse_s3\n      key_id: ${env:LAKEHOUSE_AWS_KEY}\n      secret: ${env:LAKEHOUSE_AWS_SECRET}\n      region: us-east-1\n\n  iceberg_catalogs:\n    - name: production_iceberg\n      catalog_type: rest\n      uri: https://iceberg.example.com\n      # DuckDB will use S3 secret for S3 paths in this catalog\n\nviews:\n  - name: iceberg_sales\n    source: iceberg\n    catalog: production_iceberg\n    table: sales\n    description: \"Iceberg sales data using S3 secret\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#production-deployment","title":"Production Deployment","text":""},{"location":"examples/duckdb-secrets/#docker-example","title":"Docker Example","text":"<pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY catalog.yaml .\nCOPY .env .\n\n# Set environment variables\nENV AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\nENV AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\nENV AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}\n\nCMD [\"duckalog\", \"build\", \"catalog.yaml\"]\n</code></pre>"},{"location":"examples/duckdb-secrets/#kubernetes-example","title":"Kubernetes Example","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: duckdb-secrets\ntype: Opaque\nstringData:\n  AWS_ACCESS_KEY_ID: \"AKIAIOSFODNN7EXAMPLE\"\n  AWS_SECRET_ACCESS_KEY: \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: duckalog-config\ndata:\n  catalog.yaml: |\n    version: 1\n    duckdb:\n      database: /data/duckdb.duckdb\n      install_extensions:\n        - httpfs\n      secrets:\n        - type: s3\n          name: k8s_s3\n          key_id: ${AWS_ACCESS_KEY_ID}\n          secret: ${AWS_SECRET_ACCESS_KEY}\n          region: us-west-2\n      views:\n        - name: app_data\n          source: parquet\n          uri: \"s3://app-bucket/data/*.parquet\"\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: duckalog-builder\nspec:\n  containers:\n  - name: duckalog\n    image: my-registry/duckalog:latest\n    envFrom:\n      - secretRef:\n          name: duckdb-secrets\n    volumeMounts:\n      - name: config\n        mountPath: /app/config.yaml\n        subPath: catalog.yaml\n  volumes:\n    - name: config\n      configMap:\n        name: duckalog-config\n</code></pre> <p>This example shows how DuckDB secrets in Duckalog provide comprehensive credential management for external services, enabling secure and scalable data access patterns.</p>"},{"location":"examples/duckdb-settings/","title":"DuckDB Settings Example","text":"<p>This example demonstrates how to use DuckDB settings in Duckalog to configure session behavior that goes beyond pragmas. Settings are applied after pragmas and allow you to control DuckDB's runtime behavior.</p>"},{"location":"examples/duckdb-settings/#when-to-use-settings","title":"When to Use Settings","text":"<p>Choose settings when you need to: - Configure DuckDB session parameters that aren't pragmas - Control memory allocation and threading behavior - Enable/disable specific DuckDB features - Set custom configuration parameters for performance tuning</p>"},{"location":"examples/duckdb-settings/#basic-settings-configuration","title":"Basic Settings Configuration","text":""},{"location":"examples/duckdb-settings/#single-setting-example","title":"Single Setting Example","text":"<p>Create a file called <code>settings-example.yaml</code>:</p> <pre><code>version: 1\n\nduckdb:\n  database: settings_catalog.duckdb\n\n  # Extensions (optional)\n  install_extensions:\n    - httpfs\n\n  # Pragmas (applied before settings)\n  pragmas:\n    - \"PRAGMA enable_optimizer=true\"\n    - \"PRAGMA enable_profiling=true\"\n\n  # Settings (applied after pragmas)\n  settings: \"SET enable_progress_bar = false\"\n\nviews:\n  - name: sample_data\n    sql: |\n      SELECT \n        'Settings Demo' as title,\n        CURRENT_TIMESTAMP as demo_time\n    description: \"Simple demo view to test settings\"\n</code></pre>"},{"location":"examples/duckdb-settings/#multiple-settings-example","title":"Multiple Settings Example","text":"<p>For more comprehensive configuration:</p> <pre><code>version: 1\n\nduckdb:\n  database: advanced_settings.duckdb\n  install_extensions:\n    - httpfs\n    - fts\n\n  pragmas:\n    - \"PRAGMA enable_optimizer=true\"\n    - \"PRAGMA enable_profiling=true\"\n\n  # Multiple settings as a list\n  settings:\n    - \"SET enable_progress_bar = false\"\n    - \"SET threads = 4\"\n    - \"SET memory_limit = '2GB'\"\n    - \"SET enable_http_metadata_cache = true\"\n\nviews:\n  - name: analytics_data\n    source: parquet\n    uri: \"s3://your-bucket/analytics/*.parquet\"\n    description: \"Analytics data with optimized settings\"\n\n  - name: performance_metrics\n    sql: |\n      SELECT \n        COUNT(*) as total_rows,\n        COUNT(DISTINCT user_id) as unique_users,\n        AVG(session_duration) as avg_session\n      FROM analytics_data\n      WHERE event_date &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n    description: \"Performance metrics with optimized settings\"\n</code></pre>"},{"location":"examples/duckdb-settings/#settings-with-environment-variables","title":"Settings with Environment Variables","text":"<p>Just like pragmas, settings support environment variable interpolation:</p> <pre><code>version: 1\n\nduckdb:\n  database: env_settings.duckdb\n\n  settings:\n    - \"SET threads = ${env:THREAD_COUNT}\"\n    - \"SET memory_limit = '${env:MEMORY_LIMIT}'\"\n    - \"SET enable_progress_bar = ${env:ENABLE_PROGRESS:false}\"\n\nviews:\n  - name: config_demo\n    sql: |\n      SELECT \n        current_setting('threads') as threads,\n        current_setting('memory_limit') as memory_limit,\n        current_setting('enable_progress_bar') as progress_bar_enabled\n    description: \"Demo view showing current settings\"\n</code></pre> <p>Set the environment variables:</p> <pre><code>export THREAD_COUNT=\"8\"\nexport MEMORY_LIMIT=\"4GB\"\nexport ENABLE_PROGRESS=\"false\"\n</code></pre>"},{"location":"examples/duckdb-settings/#common-duckdb-settings","title":"Common DuckDB Settings","text":""},{"location":"examples/duckdb-settings/#performance-settings","title":"Performance Settings","text":"<pre><code>duckdb:\n  settings:\n    # Threading and parallelism\n    - \"SET threads = 8\"                    # Number of CPU threads\n    - \"SET enable_progress_bar = false\"    # Disable progress output\n\n    # Memory management\n    - \"SET memory_limit = '4GB'\"           # Maximum memory usage\n\n    # Network and caching\n    - \"SET enable_http_metadata_cache = true\"   # Cache HTTP metadata\n    - \"SET enable_object_cache = true\"          # Cache object files\n</code></pre>"},{"location":"examples/duckdb-settings/#query-optimization-settings","title":"Query Optimization Settings","text":"<pre><code>duckdb:\n  settings:\n    # Query execution\n    - \"SET enable_progress_bar = false\"\n    - \"SET preserve_insertion_order = false\"    # Faster unordered results\n\n    # Join performance\n    - \"SET force_parallelism = true\"             # Force parallel execution\n</code></pre>"},{"location":"examples/duckdb-settings/#development-and-debugging-settings","title":"Development and Debugging Settings","text":"<pre><code>duckdb:\n  settings:\n    # Debugging\n    - \"SET enable_progress_bar = true\"           # Show progress for long queries\n    - \"SET enable_profiling = true\"              # Enable query profiling\n\n    # Output formatting\n    - \"SET max_width = 120\"                     # Output width\n    - \"SET null_display = 'NULL'\"                # How NULL values are displayed\n</code></pre>"},{"location":"examples/duckdb-settings/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"examples/duckdb-settings/#1-create-configuration","title":"1. Create Configuration","text":"<p>Save one of the examples above as <code>settings-example.yaml</code>.</p>"},{"location":"examples/duckdb-settings/#2-set-environment-variables-if-using-interpolation","title":"2. Set Environment Variables (if using interpolation)","text":"<pre><code>export THREAD_COUNT=\"4\"\nexport MEMORY_LIMIT=\"2GB\"\n</code></pre>"},{"location":"examples/duckdb-settings/#3-validate-configuration","title":"3. Validate Configuration","text":"<pre><code>duckalog validate settings-example.yaml\n</code></pre>"},{"location":"examples/duckdb-settings/#4-build-catalog","title":"4. Build Catalog","text":"<pre><code>duckalog build settings-example.yaml\n</code></pre>"},{"location":"examples/duckdb-settings/#5-verify-settings-applied","title":"5. Verify Settings Applied","text":"<pre><code># Connect to the created database\nduckdb settings_catalog.duckdb\n\n# Check current settings\nSELECT name, value FROM duckdb_settings() WHERE name LIKE '%thread%' OR name LIKE '%memory%';\n\n# Or use the current_setting function\nSELECT current_setting('threads') as thread_count;\nSELECT current_setting('memory_limit') as memory_limit;\nSELECT current_setting('enable_progress_bar') as progress_enabled;\n</code></pre>"},{"location":"examples/duckdb-settings/#settings-vs-pragmas","title":"Settings vs Pragmas","text":""},{"location":"examples/duckdb-settings/#when-to-use-pragmas","title":"When to Use Pragmas","text":"<ul> <li>Database-level configuration that affects the entire database file</li> <li>Persistent settings that should be saved with the database</li> <li>Low-level optimizations like <code>PRAGMA enable_optimizer</code></li> </ul>"},{"location":"examples/duckdb-settings/#when-to-use-settings_1","title":"When to Use Settings","text":"<ul> <li>Session-level configuration for the current connection</li> <li>Runtime behavior like progress bars and threading</li> <li>Temporary configuration that shouldn't persist</li> <li>Feature toggles like <code>SET enable_http_metadata_cache</code></li> </ul>"},{"location":"examples/duckdb-settings/#execution-order","title":"Execution Order","text":"<ol> <li>Extensions are installed and loaded</li> <li>Pragmas are executed (database-level)</li> <li>Settings are executed (session-level)</li> <li>Views are created</li> </ol>"},{"location":"examples/duckdb-settings/#advanced-examples","title":"Advanced Examples","text":""},{"location":"examples/duckdb-settings/#conditional-settings-based-on-environment","title":"Conditional Settings Based on Environment","text":"<pre><code>version: 1\n\nduckdb:\n  database: conditional_settings.duckdb\n\n  # Use different settings based on environment\n  settings: \"${env:DUCKDB_SETTINGS:SET enable_progress_bar = false}\"\n\nviews:\n  - name: environment_info\n    sql: |\n      SELECT \n        'Environment: ${env:ENV_NAME:development}' as environment,\n        current_setting('enable_progress_bar') as progress_bar\n    description: \"Show environment and settings\"\n</code></pre>"},{"location":"examples/duckdb-settings/#settings-for-different-workloads","title":"Settings for Different Workloads","text":"<pre><code># Production configuration - optimized for performance\nversion: 1\n\nduckdb:\n  database: production_catalog.duckdb\n  settings:\n    - \"SET enable_progress_bar = false\"\n    - \"SET threads = 16\"\n    - \"SET memory_limit = '8GB'\"\n    - \"SET enable_object_cache = true\"\n\n# Development configuration - more verbose\nversion: 1\n\nduckdb:\n  database: dev_catalog.duckdb\n  settings:\n    - \"SET enable_progress_bar = true\"\n    - \"SET threads = 2\"\n    - \"SET memory_limit = '1GB'\"\n    - \"SET enable_profiling = true\"\n</code></pre>"},{"location":"examples/duckdb-settings/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/duckdb-settings/#common-settings-issues","title":"Common Settings Issues","text":"<p>1. Invalid Setting Name: <pre><code>Error: Catalog Error: unrecognized configuration parameter\n</code></pre> Solution: Check DuckDB documentation for valid setting names, or use <code>PRAGMA table_info(duckdb_settings())</code> to see available settings.</p> <p>2. Setting Value Type Mismatch: <pre><code>Error: Parser Error: Expected type\n</code></pre> Solution: Ensure the value type matches what the setting expects (string, integer, boolean).</p> <p>3. Settings Not Applied: - Check that settings are in the correct format (must start with \"SET \") - Verify settings are applied after pragmas in the execution order - Use <code>current_setting('setting_name')</code> to verify the value</p>"},{"location":"examples/duckdb-settings/#debugging-settings","title":"Debugging Settings","text":"<pre><code>-- List all current settings\nSELECT name, value FROM duckdb_settings();\n\n-- Check specific setting\nSELECT current_setting('threads') as thread_count;\n\n-- Show settings that were changed from defaults\nSELECT name, value, default_value \nFROM duckdb_settings() \nWHERE value != default_value;\n</code></pre>"},{"location":"examples/duckdb-settings/#best-practices","title":"Best Practices","text":"<ol> <li>Use Environment Variables: For sensitive values or deployment-specific settings</li> <li>Document Settings: Add comments explaining why each setting is needed</li> <li>Test Settings: Verify settings work as expected in your environment</li> <li>Separate Configs: Use different configs for development vs production</li> <li>Monitor Performance: Check that settings actually improve performance</li> </ol> <p>This example shows how DuckDB settings in Duckalog provide fine-grained control over DuckDB's behavior, complementing the existing pragmas system for comprehensive configuration management.</p>"},{"location":"examples/environment-vars/","title":"Environment Variables Example","text":"<p>This example demonstrates how to use environment variables effectively in Duckalog configurations. You'll learn security best practices, environment-specific configurations, and credential management patterns that keep your configs portable and secure.</p>"},{"location":"examples/environment-vars/#when-to-use-this-example","title":"When to Use This Example","text":"<p>Choose this example if: - You need to keep credentials out of configuration files - You want different configs for development, staging, and production - You're deploying Duckalog across multiple environments - You need to comply with security policies (no hardcoded secrets) - You want to make configs reusable across different setups</p>"},{"location":"examples/environment-vars/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Duckalog installed: <pre><code>pip install duckalog\n</code></pre></p> </li> <li> <p>Basic understanding of environment variables in your shell: <pre><code># Check if a variable exists\necho $AWS_ACCESS_KEY_ID\n\n# Set a variable\nexport DATABASE_PASSWORD=\"my-secret-password\"\n\n# Unset a variable\nunset DATABASE_PASSWORD\n</code></pre></p> </li> </ol>"},{"location":"examples/environment-vars/#environment-variable-syntax","title":"Environment Variable Syntax","text":"<p>Duckalog supports environment variable interpolation using the <code>${env:VAR_NAME}</code> syntax:</p> <pre><code># Basic syntax\nsome_field: \"${env:VARIABLE_NAME}\"\n\n# With default values (Duckalog specific feature)\nsome_field: \"${env:VARIABLE_NAME:default_value}\"\n\n# Nested references\nconfig:\n  host: \"${env:DB_HOST}\"\n  connection_string: \"postgresql://${env:DB_USER}:${env:DB_PASSWORD}@${env:DB_HOST}:${env:DB_PORT}/${env:DB_NAME}\"\n</code></pre> <p>Important Notes: - Variable names are case-sensitive - Undefined variables will cause validation errors (unless default is provided) - Variables are resolved during configuration loading - No quotes needed around the <code>${env:...}</code> syntax</p>"},{"location":"examples/environment-vars/#security-best-practices","title":"Security Best Practices","text":""},{"location":"examples/environment-vars/#1-never-commit-secrets","title":"1. Never Commit Secrets","text":"<p>\u274c Wrong - Don't do this: <pre><code># This config file should NEVER be committed to version control\nduckdb:\n  pragmas:\n    - \"SET s3_access_key_id='AKIA...'\"  # Hardcoded credentials\n    - \"SET s3_secret_access_key='real-secret-key'\"\npostgres:\n  password: \"super-secret-password\"      # Hardcoded password\n</code></pre></p> <p>\u2705 Correct - Use environment variables: <pre><code># This config file is safe to commit\nduckdb:\n  pragmas:\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\npostgres:\n  password: \"${env:DATABASE_PASSWORD}\"\n</code></pre></p>"},{"location":"examples/environment-vars/#2-use-gitignore","title":"2. Use <code>.gitignore</code>","text":"<p>Create a <code>.gitignore</code> file to prevent accidentally committing sensitive files:</p> <pre><code># Environment files\n.env\n.env.local\n.env.production\n\n# Generated catalogs\n*.duckdb\n*.db\n\n# Logs\n*.log\n\n# Temporary files\ntmp/\ntemp/\n</code></pre>"},{"location":"examples/environment-vars/#3-environment-specific-files","title":"3. Environment-Specific Files","text":"<p>Use <code>.env</code> files for local development (add to <code>.gitignore</code>):</p> <pre><code># .env.local (for local development)\nAWS_ACCESS_KEY_ID=AKIA...\nAWS_SECRET_ACCESS_KEY=...\nDATABASE_PASSWORD=dev-password\n\n# .env.production (for production deployment)\nAWS_ACCESS_KEY_ID=AKIA...\nAWS_SECRET_ACCESS_KEY=...\nDATABASE_PASSWORD=production-password\n</code></pre> <p>Load with: <pre><code># Load environment file\nsource .env.local\n\n# Or use a tool like direnv\n# (add to .gitignore)\necho \"source .env.local\" &gt; .envrc\ndirenv allow\n</code></pre></p>"},{"location":"examples/environment-vars/#basic-environment-configuration","title":"Basic Environment Configuration","text":""},{"location":"examples/environment-vars/#development-configuration","title":"Development Configuration","text":"<p>Create <code>config-development.yaml</code>:</p> <pre><code>version: 1\n\nduckdb:\n  database: dev_catalog.duckdb\n  pragmas:\n    # Development settings - less restrictive\n    - \"SET memory_limit='512MB'\"\n    - \"SET threads=2\"\n    - \"SET search_path='public'\"\n\n# Development database (local or dev environment)\nattachments:\n  postgres:\n    - alias: dev_db\n      host: \"${env:DEV_DB_HOST:localhost}\"\n      port: 5432\n      database: \"${env:DEV_DB_NAME:analytics_dev}\"\n      user: \"${env:DEV_DB_USER:dev_user}\"\n      password: \"${env:DEV_DB_PASSWORD:dev_password}\"\n\n# S3 development bucket\nviews:\n  - name: dev_data\n    source: parquet\n    uri: \"s3://${env:DEV_S3_BUCKET:my-dev-bucket}/data/*.parquet\"\n</code></pre>"},{"location":"examples/environment-vars/#production-configuration","title":"Production Configuration","text":"<p>Create <code>config-production.yaml</code>:</p> <pre><code>version: 1\n\nduckdb:\n  database: prod_catalog.duckdb\n  pragmas:\n    # Production settings - more restrictive and performant\n    - \"SET memory_limit='8GB'\"\n    - \"SET threads=8\"\n    - \"SET temp_directory='/tmp/duckdb_temp'\"\n\n    # Cloud storage for production\n    - \"SET s3_region='${env:AWS_REGION:us-east-1}'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n    - \"SET s3_session_token='${env:AWS_SESSION_TOKEN}'\"\n\n# Production databases\nattachments:\n  postgres:\n    - alias: prod_db\n      host: \"${env:PROD_DB_HOST}\"\n      port: 5432\n      database: \"${env:PROD_DB_NAME}\"\n      user: \"${env:PROD_DB_USER}\"\n      password: \"${env:PROD_DB_PASSWORD}\"\n      sslmode: require\n\n# Production data sources\nviews:\n  - name: production_data\n    source: parquet\n    uri: \"s3://${env:PROD_S3_BUCKET}/data/*.parquet\"\n\n  - name: prod_metrics\n    sql: |\n      SELECT \n        DATE(created_at) as metric_date,\n        COUNT(*) as total_records,\n        AVG(value) as avg_value\n      FROM production_data\n      GROUP BY DATE(created_at)\n      ORDER BY metric_date DESC\n</code></pre>"},{"location":"examples/environment-vars/#common-environment-variable-patterns","title":"Common Environment Variable Patterns","text":""},{"location":"examples/environment-vars/#1-aws-and-cloud-configuration","title":"1. AWS and Cloud Configuration","text":"<pre><code># AWS credentials and region\nduckdb:\n  pragmas:\n    - \"SET s3_region='${env:AWS_REGION:us-east-1}'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n    - \"SET s3_session_token='${env:AWS_SESSION_TOKEN}'\"\n\n# S3 buckets by environment\nviews:\n  - name: events\n    source: parquet\n    uri: \"s3://${env:S3_BUCKET_PREFIX}-events/${env:ENVIRONMENT:dev}/data/*.parquet\"\n</code></pre> <p>Required environment variables: <pre><code>export AWS_REGION=\"us-west-2\"\nexport AWS_ACCESS_KEY_ID=\"AKIA...\"\nexport AWS_SECRET_ACCESS_KEY=\"...\"\nexport AWS_SESSION_TOKEN=\"...\"  # For temporary credentials\nexport S3_BUCKET_PREFIX=\"company\"\nexport ENVIRONMENT=\"prod\"\n</code></pre></p>"},{"location":"examples/environment-vars/#2-database-connections","title":"2. Database Connections","text":"<pre><code>attachments:\n  postgres:\n    - alias: primary\n      host: \"${env:DB_HOST}\"\n      port: \"${env:DB_PORT:5432}\"\n      database: \"${env:DB_NAME}\"\n      user: \"${env:DB_USER}\"\n      password: \"${env:DB_PASSWORD}\"\n      sslmode: \"${env:DB_SSL_MODE:require}\"\n\n  - alias: analytics\n      host: \"${env:ANALYTICS_DB_HOST}\"\n      port: 5432\n      database: \"${env:ANALYTICS_DB_NAME}\"\n      user: \"${env:ANALYTICS_DB_USER}\"\n      password: \"${env:ANALYTICS_DB_PASSWORD}\"\n\n  duckdb:\n    - alias: reference\n      path: \"${env:REFERENCE_DB_PATH:./reference.duckdb}\"\n      read_only: true\n</code></pre> <p>Database environment variables: <pre><code>export DB_HOST=\"prod-db.example.com\"\nexport DB_PORT=\"5432\"\nexport DB_NAME=\"analytics\"\nexport DB_USER=\"analytics_user\"\nexport DB_PASSWORD=\"secure_password\"\nexport DB_SSL_MODE=\"require\"\n\nexport ANALYTICS_DB_HOST=\"analytics-db.example.com\"\nexport ANALYTICS_DB_NAME=\"analytics_warehouse\"\nexport ANALYTICS_DB_USER=\"warehouse_user\"\nexport ANALYTICS_DB_PASSWORD=\"warehouse_password\"\n\nexport REFERENCE_DB_PATH=\"/data/reference.duckdb\"\n</code></pre></p>"},{"location":"examples/environment-vars/#3-iceberg-and-data-lake-configuration","title":"3. Iceberg and Data Lake Configuration","text":"<pre><code>iceberg_catalogs:\n  - name: production_catalog\n    catalog_type: rest\n    uri: \"${env:ICEBERG_URI}\"\n    warehouse: \"s3://${env:ICEBERG_WAREHOUSE_BUCKET}/production/\"\n    options:\n      token: \"${env:ICEBERG_TOKEN}\"\n      region: \"${env:AWS_REGION:us-east-1}\"\n\n  - name: staging_catalog\n    catalog_type: rest\n    uri: \"${env:ICEBERG_STAGING_URI}\"\n    warehouse: \"s3://${env:ICEBERG_WAREHOUSE_BUCKET}/staging/\"\n    options:\n      token: \"${env:ICEBERG_STAGING_TOKEN}\"\n      region: \"${env:AWS_REGION:us-east-1}\"\n</code></pre> <p>Iceberg environment variables: <pre><code>export ICEBERG_URI=\"https://catalog.company.com\"\nexport ICEBERG_WAREHOUSE_BUCKET=\"company-data-warehouse\"\nexport ICEBERG_TOKEN=\"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\"\n\nexport ICEBERG_STAGING_URI=\"https://staging-catalog.company.com\"\nexport ICEBERG_STAGING_TOKEN=\"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\"\n</code></pre></p>"},{"location":"examples/environment-vars/#4-application-configuration","title":"4. Application Configuration","text":"<pre><code># Application-level settings\nduckdb:\n  database: \"${env:CATALOG_DATABASE_NAME:catalog}.duckdb\"\n  pragmas:\n    - \"SET memory_limit='${env:MEMORY_LIMIT:1GB}'\"\n    - \"SET threads='${env:THREAD_COUNT:2}'\"\n    - \"SET timezone='${env:TIMEZONE:UTC}'\"\n\n# File paths\nattachments:\n  duckdb:\n    - alias: reference\n      path: \"${env:REFERENCE_DATA_PATH:./reference_data.duckdb}\"\n\n# S3 configuration\nviews:\n  - name: data_source\n    source: \"${env:DATA_SOURCE_TYPE:parquet}\"\n    uri: \"${env:DATA_URI:s3://default-bucket/data/*.parquet}\"\n</code></pre> <p>Application environment variables: <pre><code>export CATALOG_DATABASE_NAME=\"my_analytics\"\nexport MEMORY_LIMIT=\"4GB\"\nexport THREAD_COUNT=\"8\"\nexport TIMEZONE=\"America/New_York\"\n\nexport REFERENCE_DATA_PATH=\"/data/reference.duckdb\"\n\nexport DATA_SOURCE_TYPE=\"parquet\"\nexport DATA_URI=\"s3://my-bucket/production-data/*.parquet\"\n</code></pre></p>"},{"location":"examples/environment-vars/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"examples/environment-vars/#development-vs-production-strategy","title":"Development vs Production Strategy","text":"<p>Create a base configuration with environment overlays:</p> <p><code>base-config.yaml</code>: <pre><code>version: 1\n\nduckdb:\n  database: \"${env:CATALOG_NAME:analytics}.duckdb\"\n\nviews:\n  - name: user_data\n    source: parquet\n    uri: \"${env:DATA_BUCKET}/users/*.parquet\"\n\n  - name: event_data\n    source: parquet\n    uri: \"${env:DATA_BUCKET}/events/*.parquet\"\n\n  - name: analytics_summary\n    sql: |\n      SELECT \n        DATE(e.timestamp) as event_date,\n        COUNT(*) as total_events,\n        COUNT(DISTINCT e.user_id) as unique_users\n      FROM event_data e\n      JOIN user_data u ON e.user_id = u.id\n      GROUP BY DATE(e.timestamp)\n</code></pre></p> <p>Development environment (<code>.env.dev</code>): <pre><code>export CATALOG_NAME=\"dev_analytics\"\nexport DATA_BUCKET=\"s3://company-dev-data\"\nexport AWS_ACCESS_KEY_ID=\"dev-key\"\nexport AWS_SECRET_ACCESS_KEY=\"dev-secret\"\n</code></pre></p> <p>Production environment (<code>.env.prod</code>): <pre><code>export CATALOG_NAME=\"prod_analytics\"\nexport DATA_BUCKET=\"s3://company-prod-data\"\nexport AWS_ACCESS_KEY_ID=\"prod-key\"\nexport AWS_SECRET_ACCESS_KEY=\"prod-secret\"\n</code></pre></p>"},{"location":"examples/environment-vars/#docker-and-container-deployment","title":"Docker and Container Deployment","text":"<p><code>Dockerfile</code>: <pre><code>FROM python:3.9-slim\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Copy application\nCOPY . /app\nWORKDIR /app\n\n# Use environment variables for configuration\nENV CONFIG_FILE=\"/app/config-production.yaml\"\nENV LOG_LEVEL=\"INFO\"\n\nCMD [\"duckalog\", \"build\", \"$CONFIG_FILE\"]\n</code></pre></p> <p>Docker Compose (development): <pre><code># docker-compose.yml\nversion: '3.8'\nservices:\n  duckalog:\n    build: .\n    environment:\n      - CONFIG_FILE=config-development.yaml\n      - CATALOG_NAME=dev_catalog\n      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n      - DB_PASSWORD=${DB_PASSWORD}\n    volumes:\n      - ./data:/app/data\n      - ./configs:/app/configs\n</code></pre></p> <p>Kubernetes ConfigMap and Secret: <pre><code># configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: duckalog-config\ndata:\n  config-production.yaml: |\n    version: 1\n    duckdb:\n      database: \"/data/catalog.duckdb\"\n      pragmas:\n        - \"SET memory_limit='8GB'\"\n        - \"SET threads=8\"\n    views:\n      - name: production_data\n        source: parquet\n        uri: \"s3://${env:PRODUCTION_BUCKET}/data/*.parquet\"\n\n---\n# secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: duckalog-secrets\ntype: Opaque\nstringData:\n  AWS_ACCESS_KEY_ID: \"AKIA...\"\n  AWS_SECRET_ACCESS_KEY: \"...\"\n  PRODUCTION_BUCKET: \"company-prod-data\"\n</code></pre></p>"},{"location":"examples/environment-vars/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"examples/environment-vars/#1-set-up-environment-variables","title":"1. Set Up Environment Variables","text":"<p>Option A: Direct environment variables <pre><code># Set variables for current session\nexport AWS_ACCESS_KEY_ID=\"AKIA...\"\nexport AWS_SECRET_ACCESS_KEY=\"...\"\nexport DATABASE_PASSWORD=\"secret\"\nexport ENVIRONMENT=\"development\"\n\n# Verify they're set\necho $AWS_ACCESS_KEY_ID\n</code></pre></p> <p>Option B: Using .env file <pre><code># Create .env file\ncat &gt; .env &lt;&lt; EOF\nAWS_ACCESS_KEY_ID=AKIA...\nAWS_SECRET_ACCESS_KEY=...\nDATABASE_PASSWORD=secret\nENVIRONMENT=development\nEOF\n\n# Load the file\nsource .env\n\n# Add to .gitignore\necho \".env\" &gt;&gt; .gitignore\n</code></pre></p> <p>Option C: Using direnv (recommended) <pre><code># Install direnv\n# Then in your project directory:\necho \"source .env\" &gt; .envrc\ndirenv allow\n# direnv automatically loads .env when you cd into the directory\n</code></pre></p>"},{"location":"examples/environment-vars/#2-create-configuration-with-environment-variables","title":"2. Create Configuration with Environment Variables","text":"<pre><code># config.yaml\nversion: 1\n\nduckdb:\n  database: \"${env:CATALOG_NAME:analytics}.duckdb\"\n  pragmas:\n    - \"SET memory_limit='${env:MEMORY_LIMIT:1GB}'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n\nattachments:\n  postgres:\n    - alias: main_db\n      host: \"${env:DB_HOST}\"\n      database: \"${env:DB_NAME}\"\n      user: \"${env:DB_USER}\"\n      password: \"${env:DB_PASSWORD}\"\n\nviews:\n  - name: sample_data\n    source: parquet\n    uri: \"s3://${env:DATA_BUCKET}/sample/*.parquet\"\n</code></pre>"},{"location":"examples/environment-vars/#3-validate-configuration","title":"3. Validate Configuration","text":"<pre><code># Check for missing environment variables\nduckalog validate config.yaml\n\n# If variables are missing, you'll see errors like:\n# ConfigError: Environment variable 'AWS_ACCESS_KEY_ID' not found\n</code></pre>"},{"location":"examples/environment-vars/#4-generate-sql-with-environment-variables","title":"4. Generate SQL with Environment Variables","text":"<pre><code># The SQL will be generated with environment variables resolved\nduckalog generate-sql config.yaml --output generated.sql\n\n# Check the output to see resolved values (be careful with secrets!)\ncat generated.sql\n</code></pre>"},{"location":"examples/environment-vars/#5-build-catalog","title":"5. Build Catalog","text":"<pre><code># Build with environment variables\nduckalog build config.yaml\n\n# This will create catalog.duckdb (or whatever name you specified)\n</code></pre>"},{"location":"examples/environment-vars/#6-use-in-scripts","title":"6. Use in Scripts","text":"<pre><code># build_catalog.py\nimport os\nfrom duckalog import build_catalog, validate_config\n\n# Set environment based on command line argument\nenv = os.environ.get('ENVIRONMENT', 'development')\nconfig_file = f'config-{env}.yaml'\n\n# Validate first\ntry:\n    validate_config(config_file)\n    print(f\"\u2705 Configuration validated for {env} environment\")\nexcept Exception as e:\n    print(f\"\u274c Configuration error: {e}\")\n    exit(1)\n\n# Build catalog\nbuild_catalog(config_file)\nprint(f\"\u2705 Catalog built for {env} environment\")\n</code></pre> <pre><code># Run for different environments\nENVIRONMENT=development python build_catalog.py\nENVIRONMENT=production python build_catalog.py\n</code></pre>"},{"location":"examples/environment-vars/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/environment-vars/#common-issues","title":"Common Issues","text":"<p>1. Missing Environment Variables <pre><code># Check what's set\nenv | grep -E \"(AWS|DB|DATA)\"\n\n# Check specific variable\necho \"AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID\"\n\n# List all variables (be careful with secrets!)\nenv\n</code></pre></p> <p>2. Variable Not Expanding <pre><code># Make sure you're using the correct syntax\ngood: \"${env:VARIABLE_NAME}\"\nbad: \"$VARIABLE_NAME\"                    # Wrong\nbad: \"${VARIABLE_NAME}\"                  # Missing env: prefix\nbad: \"${env:}\"                           # Empty variable name\n</code></pre></p> <p>3. Default Values Not Working <pre><code># Correct syntax for defaults\nwith_default: \"${env:MISSING_VAR:default_value}\"\nwithout_default: \"${env:ANOTHER_VAR}\"    # Will error if not set\n\n# Note: Default values must be strings\ncorrect: \"${env:PORT:5432}\"              # String default\nincorrect: \"${env:PORT:5432}\"            # Still string, but looks like number\n</code></pre></p> <p>4. Special Characters in Values <pre><code># For passwords with special characters, quotes might be needed\npassword: \"${env:DB_PASSWORD}\"           # Usually works\npassword: \"'${env:DB_PASSWORD}'\"         # Force quotes if needed\n\n# If your password contains quotes, escape them\npassword: \"${env:COMPLEX_PASSWORD:my\\\"special'pass}\"\n</code></pre></p>"},{"location":"examples/environment-vars/#debug-commands","title":"Debug Commands","text":"<p>Check environment variable resolution: <pre><code># Create a simple config to test variables\ncat &gt; test_vars.yaml &lt;&lt; EOF\nversion: 1\ntest_field: \"${env:TEST_VAR:default_value}\"\nEOF\n\n# Validate and see what happens\nduckalog validate test_vars.yaml\n\n# Set the variable and test again\nexport TEST_VAR=\"resolved_value\"\nduckalog validate test_vars.yaml\n</code></pre></p> <p>List all environment variables used in config: <pre><code># Use grep to find env variable usage\ngrep -o '\\${env:[^}]*}' config.yaml | sort | uniq\n</code></pre></p>"},{"location":"examples/environment-vars/#security-testing","title":"Security Testing","text":"<p>Check for accidentally committed secrets: <pre><code># Search for potential secrets in config files\ngrep -r \"password\\|secret\\|key\\|token\" *.yaml | grep -v env:\n\n# Check git history for secrets\ngit log -p --grep=\"password\\|secret\\|key\" --all\n\n# Use git-secrets or similar tools\ngit-secrets --scan\n</code></pre></p>"},{"location":"examples/environment-vars/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"examples/environment-vars/#1-environment-variable-validation","title":"1. Environment Variable Validation","text":"<pre><code># validate_env.py\nimport os\nimport sys\n\nrequired_vars = [\n    'AWS_ACCESS_KEY_ID',\n    'AWS_SECRET_ACCESS_KEY',\n    'DATABASE_PASSWORD'\n]\n\nmissing_vars = []\nfor var in required_vars:\n    if not os.environ.get(var):\n        missing_vars.append(var)\n\nif missing_vars:\n    print(f\"\u274c Missing required environment variables:\")\n    for var in missing_vars:\n        print(f\"   - {var}\")\n    print(\"\\nSet them with:\")\n    print(f\"export {' '.join(f'{var}=...' for var in missing_vars)}\")\n    sys.exit(1)\n\nprint(\"\u2705 All required environment variables are set\")\n</code></pre>"},{"location":"examples/environment-vars/#2-dynamic-configuration-selection","title":"2. Dynamic Configuration Selection","text":"<pre><code># dynamic_config.py\nimport os\nfrom pathlib import Path\n\ndef get_config_for_environment():\n    env = os.environ.get('ENVIRONMENT', 'development')\n\n    # Map environment to config file\n    config_map = {\n        'development': 'configs/config-dev.yaml',\n        'staging': 'configs/config-staging.yaml',\n        'production': 'configs/config-prod.yaml'\n    }\n\n    config_file = config_map.get(env)\n    if not config_file or not Path(config_file).exists():\n        raise FileNotFoundError(f\"No config found for environment: {env}\")\n\n    return config_file\n\n# Usage\nconfig = get_config_for_environment()\nprint(f\"Using configuration: {config}\")\n</code></pre>"},{"location":"examples/environment-vars/#3-secret-rotation-support","title":"3. Secret Rotation Support","text":"<pre><code># config-with-rotation.yaml\nversion: 1\n\nduckdb:\n  pragmas:\n    # Support both old and new AWS credentials during rotation\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID_NEW:${env:AWS_ACCESS_KEY_ID}}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY_NEW:${env:AWS_SECRET_ACCESS_KEY}}'\"\n\nattachments:\n  postgres:\n    - alias: main\n      password: \"${env:DB_PASSWORD_NEW:${env:DB_PASSWORD}}\"\n</code></pre> <p>This environment variable pattern enables secure, portable, and maintainable Duckalog configurations across all your deployment environments.</p>"},{"location":"examples/local-attachments/","title":"Local Attachments Example","text":"<p>This example demonstrates how to attach and work with local DuckDB and SQLite databases in Duckalog. It's ideal for combining data from multiple local databases or integrating existing data stores into your analytics pipeline.</p>"},{"location":"examples/local-attachments/#when-to-use-this-example","title":"When to Use This Example","text":"<p>Choose this example if:</p> <ul> <li>You have multiple local DuckDB databases to combine</li> <li>You need to integrate existing SQLite databases</li> <li>You want to join data across different local databases</li> <li>You're working with read-only reference data</li> <li>You need to build a unified view of local data sources</li> <li>You want to avoid duplicating data by referencing instead of copying</li> </ul>"},{"location":"examples/local-attachments/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Duckalog installed: <pre><code>pip install duckalog\n</code></pre></p> </li> <li> <p>Sample local databases - Create test data for demonstration:    <pre><code># Create sample DuckDB databases\nimport duckdb\n\n# Reference data database\ncon_ref = duckdb.connect(\"reference_data.duckdb\")\ncon_ref.execute(\"\"\"\n  CREATE TABLE users AS\n  SELECT \n    user_id,\n    name,\n    email,\n    signup_date,\n    country,\n    segment\n  FROM range(1, 101) t(user_id)\n  CROSS JOIN (VALUES \n    ('Alice', 'alice@example.com', '2023-01-15', 'US', 'enterprise'),\n    ('Bob', 'bob@example.com', '2023-02-20', 'UK', 'smb'),\n    ('Carol', 'carol@example.com', '2023-03-10', 'DE', 'enterprise'),\n    ('David', 'david@example.com', '2023-04-05', 'US', 'startup'),\n    ('Eve', 'eve@example.com', '2023-05-12', 'FR', 'smb')\n  ) u(name, email, signup_date, country, segment)\n  WHERE user_id &lt;= 5\n\"\"\")\n\n# Analytics database with sales data\ncon_analytics = duckdb.connect(\"analytics_data.duckdb\")\ncon_analytics.execute(\"\"\"\n  CREATE TABLE sales AS\n  SELECT \n    sale_id,\n    user_id,\n    product_name,\n    amount,\n    sale_date,\n    region\n  FROM range(1, 501) t(sale_id)\n  CROSS JOIN (VALUES \n    ('Laptop', 1200.00, '2023-01-01', 'North'),\n    ('Mouse', 25.00, '2023-01-02', 'South'),\n    ('Keyboard', 75.00, '2023-01-03', 'East'),\n    ('Monitor', 300.00, '2023-01-04', 'West'),\n    ('Desk', 450.00, '2023-01-05', 'North')\n  ) p(product_name, amount, sale_date, region)\n  CROSS JOIN (SELECT user_id FROM range(1, 6)) u(user_id)\n  WHERE sale_id &lt;= 100\n\"\"\")\n\ncon_ref.close()\ncon_analytics.close()\n\n# Create sample SQLite database\nimport sqlite3\n\ncon_sqlite = sqlite3.connect(\"legacy_system.db\")\ncon_sqlite.execute(\"\"\"\n  CREATE TABLE customer_preferences (\n    user_id INTEGER PRIMARY KEY,\n    preferred_categories TEXT,\n    communication_style TEXT,\n    last_contact_date DATE\n  )\n\"\"\")\n\npreferences_data = [\n  (1, 'electronics,books', 'email', '2023-06-15'),\n  (2, 'home,garden', 'phone', '2023-06-10'),\n  (3, 'technology,software', 'email', '2023-06-20'),\n  (4, 'gaming,electronics', 'slack', '2023-06-18'),\n  (5, 'office,productivity', 'email', '2023-06-12')\n]\n\ncon_sqlite.executemany(\n  \"INSERT INTO customer_preferences VALUES (?, ?, ?, ?)\",\n  preferences_data\n)\n\ncon_sqlite.commit()\ncon_sqlite.close()\n\nprint(\"Created sample databases: reference_data.duckdb, analytics_data.duckdb, legacy_system.db\")\n</code></pre></p> </li> </ol>"},{"location":"examples/local-attachments/#basic-attachment-configuration","title":"Basic Attachment Configuration","text":""},{"location":"examples/local-attachments/#single-database-attachment","title":"Single Database Attachment","text":"<p>Create a file called <code>local-attachments.yaml</code>:</p> <pre><code>version: 1\n\n# DuckDB configuration\nduckdb:\n  database: unified_catalog.duckdb\n  pragmas:\n    # Performance settings for local work\n    - \"SET memory_limit='1GB'\"\n    - \"SET threads=2\"\n    - \"SET temp_directory='/tmp/duckdb_temp'\"  # For large operations\n\n# Attach local databases\nattachments:\n  duckdb:\n    - alias: reference       # How you'll reference this database\n      path: ./reference_data.duckdb   # Path to DuckDB file\n      read_only: true        # Prevent modifications (recommended)\n\n    - alias: analytics\n      path: ./analytics_data.duckdb\n      read_only: true\n\n  sqlite:\n    - alias: legacy         # SQLite database reference\n      path: ./legacy_system.db\n\n# Views that use attached databases\nviews:\n  - name: user_reference\n    source: duckdb          # Attached DuckDB database\n    database: reference     # Alias from attachments section\n    table: users            # Table name in attached database\n    description: \"User reference data from local DuckDB\"\n\n  - name: sales_data\n    source: duckdb\n    database: analytics\n    table: sales\n    description: \"Sales data from analytics database\"\n\n  - name: customer_prefs\n    source: sqlite          # SQLite attachment\n    database: legacy        # SQLite alias\n    table: customer_preferences\n    description: \"Customer preferences from legacy system\"\n</code></pre> <p>Key configuration elements: - <code>attachments</code> section defines external databases - <code>alias</code> provides reference name used in views - <code>read_only: true</code> prevents accidental data modification - Views reference attached databases by alias - Different source types: <code>duckdb</code>, <code>sqlite</code>, <code>postgres</code></p>"},{"location":"examples/local-attachments/#advanced-attachment-patterns","title":"Advanced Attachment Patterns","text":""},{"location":"examples/local-attachments/#multiple-tables-from-same-database","title":"Multiple Tables from Same Database","text":"<pre><code>views:\n  # Access multiple tables from the same attachment\n  - name: user_profiles\n    source: duckdb\n    database: reference\n    table: users\n    description: \"User profiles\"\n\n  - name: user_stats\n    source: duckdb\n    database: reference\n    table: user_statistics  # Different table, same database\n    description: \"User statistics\"\n\n  - name: sales_summary\n    source: duckdb\n    database: analytics\n    table: sales\n    description: \"Raw sales transactions\"\n</code></pre>"},{"location":"examples/local-attachments/#cross-database-joins","title":"Cross-Database Joins","text":"<pre><code>  # Join data across attached databases\n  - name: user_sales_enriched\n    sql: |\n      SELECT \n        u.user_id,\n        u.name,\n        u.email,\n        u.country,\n        u.segment,\n        p.preferred_categories,\n        p.communication_style,\n        COUNT(s.sale_id) as total_sales,\n        SUM(s.amount) as total_revenue,\n        AVG(s.amount) as avg_sale_amount\n      FROM user_profiles u\n      LEFT JOIN customer_prefs p ON u.user_id = p.user_id\n      LEFT JOIN sales_data s ON u.user_id = s.user_id\n      GROUP BY u.user_id, u.name, u.email, u.country, u.segment, \n               p.preferred_categories, p.communication_style\n      ORDER BY total_revenue DESC NULLS LAST\n    description: \"Unified user view combining all data sources\"\n\n  - name: sales_by_region\n    sql: |\n      SELECT \n        u.country,\n        u.segment,\n        COUNT(DISTINCT u.user_id) as user_count,\n        COUNT(s.sale_id) as total_sales,\n        SUM(s.amount) as total_revenue,\n        AVG(s.amount) as avg_sale_value\n      FROM user_profiles u\n      JOIN sales_data s ON u.user_id = s.user_id\n      GROUP BY u.country, u.segment\n      ORDER BY total_revenue DESC\n    description: \"Sales performance by region and segment\"\n</code></pre>"},{"location":"examples/local-attachments/#read-only-vs-read-write-attachments","title":"Read-Only vs Read-Write Attachments","text":""},{"location":"examples/local-attachments/#read-only-attachments-recommended","title":"Read-Only Attachments (Recommended)","text":"<pre><code>attachments:\n  duckdb:\n    - alias: reference\n      path: ./reference_data.duckdb\n      read_only: true        # Safe - prevents modifications\n</code></pre> <p>Benefits: - Prevents accidental data corruption - Allows safe concurrent access - Better for production use - Clear data provenance</p> <p>Use when: - Reference data that shouldn't change - Historical data - Data shared across multiple processes - Production environments</p>"},{"location":"examples/local-attachments/#read-write-attachments-use-carefully","title":"Read-Write Attachments (Use Carefully)","text":"<pre><code>attachments:\n  duckdb:\n    - alias: staging\n      path: ./staging_data.duckdb\n      read_only: false       # Allows modifications\n\n  sqlite:\n    - alias: legacy\n      path: ./legacy_system.db\n      read_only: false\n</code></pre> <p>Considerations: - Use only when you need to modify source data - Be aware of potential concurrency issues - Ensure proper backup strategies - Consider performance implications</p> <p>Use when: - Staging area for data processing - Temporary transformations - Controlled ETL processes</p>"},{"location":"examples/local-attachments/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"examples/local-attachments/#1-create-configuration-file","title":"1. Create Configuration File","text":"<p>Save the configuration above as <code>local-attachments.yaml</code>.</p>"},{"location":"examples/local-attachments/#2-validate-configuration","title":"2. Validate Configuration","text":"<pre><code>duckalog validate local-attachments.yaml\n</code></pre> <p>Expected output: <pre><code>\u2705 Configuration is valid\n\u2705 All database attachments found\n\u2705 All views defined correctly\n</code></pre></p>"},{"location":"examples/local-attachments/#3-generate-sql-optional","title":"3. Generate SQL (Optional)","text":"<p>Preview the SQL that will be executed:</p> <pre><code>duckalog generate-sql local-attachments.yaml --output attachments.sql\ncat attachments.sql\n</code></pre>"},{"location":"examples/local-attachments/#4-build-the-catalog","title":"4. Build the Catalog","text":"<pre><code>duckalog build local-attachments.yaml\n</code></pre> <p>This creates <code>unified_catalog.duckdb</code> with views over your attached databases.</p>"},{"location":"examples/local-attachments/#5-query-your-data","title":"5. Query Your Data","text":"<pre><code># Connect with DuckDB\nduckdb unified_catalog.duckdb\n\n# Example queries:\n# Simple view access\nSELECT * FROM user_reference LIMIT 10;\n\n# Cross-database join\nSELECT \n  u.name,\n  u.country,\n  COUNT(s.sale_id) as sales_count,\n  SUM(s.amount) as total_revenue\nFROM user_reference u\nLEFT JOIN sales_data s ON u.user_id = s.user_id\nGROUP BY u.user_id, u.name, u.country\nORDER BY total_revenue DESC;\n\n# Complex analytics across sources\nSELECT * FROM user_sales_enriched WHERE total_sales &gt; 0;\n</code></pre>"},{"location":"examples/local-attachments/#6-use-programmatically","title":"6. Use Programmatically","text":"<pre><code>from duckalog import load_config, build_catalog\nimport duckdb\n\n# Build catalog\nbuild_catalog(\"local-attachments.yaml\")\n\n# Connect to unified catalog\ncon = duckdb.connect(\"unified_catalog.duckdb\")\n\n# Simple queries\nusers_df = con.execute(\"SELECT * FROM user_reference WHERE country = 'US'\").df()\nprint(\"US Users:\")\nprint(users_df)\n\n# Cross-database analytics\nanalytics_df = con.execute(\"\"\"\n    SELECT \n      country,\n      segment,\n      COUNT(*) as user_count,\n      SUM(total_revenue) as segment_revenue\n    FROM user_sales_enriched\n    GROUP BY country, segment\n    ORDER BY segment_revenue DESC\n\"\"\").df()\nprint(\"\\nRevenue by Country/Segment:\")\nprint(analytics_df)\n\n# Find power users\npower_users = con.execute(\"\"\"\n    SELECT name, email, total_sales, total_revenue\n    FROM user_sales_enriched\n    WHERE total_sales &gt;= 5\n    ORDER BY total_revenue DESC\n\"\"\").df()\nprint(\"\\nPower Users:\")\nprint(power_users)\n</code></pre>"},{"location":"examples/local-attachments/#performance-optimization","title":"Performance Optimization","text":""},{"location":"examples/local-attachments/#memory-management","title":"Memory Management","text":"<pre><code>duckdb:\n  pragmas:\n    - \"SET memory_limit='2GB'\"      # Adjust based on available RAM\n    - \"SET threads=4\"               # Match CPU cores\n    - \"SET temp_directory='/tmp/duckdb_temp'\"  # For large operations\n    - \"SET wal_autocheckpoint='1GB'\" # For read-write attachments\n</code></pre>"},{"location":"examples/local-attachments/#large-dataset-considerations","title":"Large Dataset Considerations","text":"<pre><code># For large databases, consider selective views\nviews:\n  - name: recent_users\n    sql: |\n      SELECT * FROM user_reference\n      WHERE signup_date &gt;= CURRENT_DATE - INTERVAL 365 DAYS\n    description: \"Users from last year\"\n\n  - name: high_value_sales\n    sql: |\n      SELECT * FROM sales_data\n      WHERE amount &gt;= 100.0\n    description: \"Sales above $100\"\n</code></pre>"},{"location":"examples/local-attachments/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<p>This example teaches:</p> <ol> <li>Database Attachment: How to connect external DuckDB/SQLite databases</li> <li>Read-Only Safety: Best practices for safe data access</li> <li>Cross-Database Joins: Joining data across multiple local databases</li> <li>Alias Management: Organizing and referencing attached databases</li> <li>Performance Optimization: Memory and threading for local work</li> <li>Data Unification: Combining disparate data sources</li> <li>SQL Composition: Building complex analytics across sources</li> </ol>"},{"location":"examples/local-attachments/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/local-attachments/#common-issues","title":"Common Issues","text":"<p>1. Database File Not Found: <pre><code># Check file exists and path is correct\nls -la reference_data.duckdb\nls -la analytics_data.duckdb\nls -la legacy_system.db\n\n# Verify current directory\npwd\n</code></pre></p> <p>2. Permission Errors: <pre><code># Check read permissions\nls -l *.duckdb *.db\n\n# Fix permissions if needed\nchmod 644 *.duckdb *.db\n\n# For read-write access, ensure write permissions\nchmod 666 *.duckdb *.db\n</code></pre></p> <p>3. Schema/Table Not Found: <pre><code>-- Check available tables in attached database\nSHOW TABLES FROM reference;\nSHOW TABLES FROM analytics;\nSHOW TABLES FROM legacy;\n</code></pre></p> <p>4. Memory Issues: <pre><code># Reduce memory usage\nduckdb:\n  pragmas:\n    - \"SET memory_limit='512MB'\"\n    - \"SET threads=2\"\n</code></pre></p> <p>5. Slow Cross-Database Queries: <pre><code># Optimize for local performance\nduckdb:\n  pragmas:\n    - \"SET threads=8\"                    # Use more cores\n    - \"SET temp_directory='/fast/ssd/'\"  # Use fast storage\n</code></pre></p>"},{"location":"examples/local-attachments/#debug-commands","title":"Debug Commands","text":"<pre><code>-- List all attached databases\nDATABASE_LIST;\n\n-- Show schema for attached database\nDESCRIBE reference.users;\n\n-- Check query plan for optimization\nEXPLAIN SELECT * FROM user_sales_enriched;\n\n-- Monitor memory usage\nPRAGMA memory_limit;\nPRAGMA threads;\n</code></pre>"},{"location":"examples/local-attachments/#variations","title":"Variations","text":""},{"location":"examples/local-attachments/#backup-and-restore-pattern","title":"Backup and Restore Pattern","text":"<pre><code># Create backup before building catalog\n- name: backup_step\n  sql: |\n    -- Backup critical reference data\n    CREATE TABLE backup_users AS SELECT * FROM user_reference;\n</code></pre>"},{"location":"examples/local-attachments/#incremental-data-loading","title":"Incremental Data Loading","text":"<pre><code># Load only new/changed data\nviews:\n  - name: updated_sales\n    sql: |\n      SELECT * FROM sales_data\n      WHERE sale_date &gt;= (SELECT MAX(sale_date) FROM backup_sales)\n</code></pre>"},{"location":"examples/local-attachments/#data-quality-checks","title":"Data Quality Checks","text":"<pre><code># Add data quality views\n- name: data_quality_report\n  sql: |\n    SELECT \n      'user_reference' as table_name,\n      COUNT(*) as record_count,\n      COUNT(DISTINCT user_id) as unique_users,\n      COUNT(*) - COUNT(email) as missing_emails\n    FROM user_reference\n    UNION ALL\n    SELECT \n      'sales_data' as table_name,\n      COUNT(*) as record_count,\n      COUNT(DISTINCT user_id) as unique_users,\n      COUNT(*) - COUNT(amount) as missing_amounts\n    FROM sales_data\n</code></pre> <p>This example provides a solid foundation for working with local database attachments in Duckalog. The patterns shown here can be extended to handle more complex scenarios and larger datasets.</p>"},{"location":"examples/local-attachments/#security-considerations","title":"Security Considerations","text":""},{"location":"examples/local-attachments/#file-permissions","title":"File Permissions","text":"<pre><code># Set appropriate permissions for databases\nchmod 640 reference_data.duckdb        # Read/write for owner, read for group\nchmod 644 legacy_system.db             # Read-only for all\n\n# For sensitive data, consider encryption\nchmod 600 sensitive_data.duckdb        # Owner read/write only\n</code></pre>"},{"location":"examples/local-attachments/#data-access-control","title":"Data Access Control","text":"<pre><code># Use views to control data access\nviews:\n  - name: public_user_data\n    sql: |\n      SELECT user_id, name, country  -- Exclude sensitive fields\n      FROM user_reference\n</code></pre> <p>This attachment pattern enables powerful data integration while maintaining security and performance. Adapt these patterns to your specific local data landscape and requirements.</p>"},{"location":"examples/multi-source-analytics/","title":"Multi-Source Analytics Example","text":"<p>This example demonstrates how to use Duckalog to build a comprehensive analytics catalog that combines data from multiple sources into a unified view. You'll learn how to configure attachments, Iceberg catalogs, and cloud storage to create powerful analytical queries.</p>"},{"location":"examples/multi-source-analytics/#business-scenario","title":"Business Scenario","text":"<p>Imagine you're building an analytics platform for a growing company. Your data lives in several places: - Raw events stored as Parquet files in S3 (high-volume, cost-effective storage) - Reference data in local DuckDB databases (fast, local access) - Legacy customer data in SQLite (existing systems) - Product information in PostgreSQL (operational systems) - Processed event data in Iceberg tables (data warehouse)</p> <p>This example shows how to unify all these sources into a single DuckDB catalog that enables rich analytics across your entire data landscape.</p>"},{"location":"examples/multi-source-analytics/#prerequisites","title":"Prerequisites","text":"<p>Before running this example, ensure you have:</p> <ol> <li> <p>Python 3.9+ with Duckalog installed:    <pre><code>pip install duckalog\n</code></pre></p> </li> <li> <p>Required environment variables (see Environment Variables section below)</p> </li> <li> <p>Sample data sources (or use the provided examples with mock paths):</p> </li> <li>S3 bucket with Parquet files</li> <li>Iceberg catalog access (e.g., AWS Glue, Tabular)</li> <li>PostgreSQL database</li> <li>Local DuckDB/SQLite databases (can be created for testing)</li> </ol>"},{"location":"examples/multi-source-analytics/#complete-configuration","title":"Complete Configuration","text":"<p>Here's the full configuration with explanations for each section:</p>"},{"location":"examples/multi-source-analytics/#base-configuration","title":"Base Configuration","text":"<pre><code># Configuration file version\nversion: 1\n\n# DuckDB database settings\nduckdb:\n  # Output database file\n  database: multi_source.duckdb\n\n  # Extensions needed for cloud and data lake access\n  install_extensions:\n    - httpfs          # For HTTP/S3 file access\n    - iceberg         # For Iceberg table support\n\n  # Performance and behavior settings\n  pragmas:\n    - \"SET memory_limit='2GB'\"      # Limit memory usage\n    - \"SET threads=4\"               # Parallel processing\n    - \"SET timezone='UTC'\"          # Consistent time handling\n\n    # Cloud storage configuration\n    # These settings enable S3 access for Parquet files\n    - \"SET s3_region='us-west-2'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n    - \"SET s3_session_token='${env:AWS_SESSION_TOKEN}'\"  # Optional for temp credentials\n</code></pre> <p>Key Concepts Demonstrated: - Extension installation for extended functionality - Memory and threading optimization for performance - Environment variable injection for credentials - Timezone consistency for global data</p>"},{"location":"examples/multi-source-analytics/#database-attachments","title":"Database Attachments","text":"<pre><code># External database attachments\nattachments:\n  # Local DuckDB databases (read-only for safety)\n  duckdb:\n    - alias: reference         # How you'll reference this database in views\n      path: ./reference_data.duckdb     # Local file path\n      read_only: true         # Prevent accidental modifications\n\n    - alias: historical\n      path: ./historical_data.duckdb\n      read_only: true\n\n  # Legacy system in SQLite format\n  sqlite:\n    - alias: legacy           # Reference name for SQL queries\n      path: ./legacy_system.db  # Local SQLite database file\n\n  # Production and staging PostgreSQL databases\n  postgres:\n    - alias: prod_db         # Production database reference\n      host: \"${env:PROD_DB_HOST}\"    # Environment variable for security\n      port: 5432\n      database: analytics_prod\n      user: \"${env:PROD_DB_USER}\"\n      password: \"${env:PROD_DB_PASSWORD}\"\n      sslmode: require        # Secure connection\n\n    - alias: staging_db      # Staging environment\n      host: \"${env:STAGING_DB_HOST}\"\n      port: 5432\n      database: analytics_staging\n      user: \"${env:STAGING_DB_USER}\"\n      password: \"${env:STAGING_DB_PASSWORD}\"\n      sslmode: require\n</code></pre> <p>Key Concepts Demonstrated: - Multiple database attachment types (DuckDB, SQLite, PostgreSQL) - Read-only attachments for safety - Environment variable usage for credentials - SSL configuration for secure connections - Environment-specific configurations (prod vs staging)</p>"},{"location":"examples/multi-source-analytics/#iceberg-catalogs","title":"Iceberg Catalogs","text":"<pre><code># Iceberg data lake catalogs\niceberg_catalogs:\n  - name: production_iceberg    # Reference name used in views\n    catalog_type: rest          # REST catalog (vs Hadoop, Spark, etc.)\n    uri: \"https://iceberg-catalog.production.company.com\"  # Catalog endpoint\n    warehouse: \"s3://company-data-warehouse/production/\"   # Storage location\n    options:\n      token: \"${env:ICEBERG_PROD_TOKEN}\"  # Authentication\n\n  - name: staging_iceberg       # Staging environment catalog\n    catalog_type: rest\n    uri: \"https://iceberg-catalog.staging.company.com\"\n    warehouse: \"s3://company-data-warehouse/staging/\"\n    options:\n      token: \"${env:ICEBERG_STAGING_TOKEN}\"\n</code></pre> <p>Key Concepts Demonstrated: - REST catalog configuration (most common for cloud data lakes) - Environment-specific catalog separation - Token-based authentication - Warehouse path configuration</p>"},{"location":"examples/multi-source-analytics/#views-definition","title":"Views Definition","text":"<p>Views are the core of your analytics - they define how data is accessed and transformed:</p> <pre><code># Data access and transformation views\nviews:\n  # Raw data from different sources\n  - name: raw_events                    # View name for querying\n    source: parquet                     # Data source type\n    uri: \"s3://company-data-lake/events/raw/*.parquet\"  # File pattern\n    description: \"Raw events from data lake\"\n\n  - name: processed_events\n    source: iceberg                     # Iceberg table access\n    catalog: production_iceberg         # Reference to configured catalog\n    table: analytics.processed_events   # Schema.table format\n    description: \"Processed events from production Iceberg catalog\"\n\n  - name: user_profiles\n    source: duckdb                      # Attached DuckDB database\n    database: reference                 # Alias from attachments section\n    table: users                        # Table name in attached database\n    description: \"User reference data\"\n\n  - name: legacy_customers\n    source: sqlite                      # SQLite attachment\n    database: legacy                    # SQLite alias\n    table: customers\n    description: \"Legacy customer data from SQLite\"\n\n  - name: product_data\n    source: postgres                    # PostgreSQL attachment\n    database: prod_db                   # Production database alias\n    table: products\n    description: \"Product data from production Postgres\"\n</code></pre> <p>Key Concepts Demonstrated: - Multiple source types in unified catalog - Schema.table naming for structured data - Descriptive metadata for documentation</p>"},{"location":"examples/multi-source-analytics/#advanced-analytics-views","title":"Advanced Analytics Views","text":"<p>These views demonstrate complex analytics across multiple data sources:</p> <pre><code>  # Enriched analytics views\n  - name: enriched_events\n    sql: |\n      SELECT\n        e.event_id,\n        e.timestamp,\n        e.event_type,\n        e.user_id,\n        e.session_id,\n        e.properties,\n        u.name as user_name,           # Join user reference data\n        u.email,\n        u.segment,\n        p.name as product_name,        # Join product catalog\n        p.category as product_category\n      FROM raw_events e\n      LEFT JOIN user_profiles u ON e.user_id = u.id\n      LEFT JOIN product_data p ON e.properties-&gt;&gt;'product_id' = p.id\n    description: \"Events enriched with user and product data\"\n\n  - name: event_metrics\n    sql: |\n      SELECT\n        DATE(timestamp) as event_date,     # Daily aggregation\n        event_type,\n        COUNT(*) as event_count,\n        COUNT(DISTINCT user_id) as unique_users,\n        COUNT(DISTINCT session_id) as unique_sessions\n      FROM enriched_events\n      GROUP BY DATE(timestamp), event_type\n      ORDER BY event_date DESC, event_count DESC\n    description: \"Daily event metrics for reporting\"\n\n  - name: user_activity_summary\n    sql: |\n      SELECT\n        u.id as user_id,\n        u.name,\n        u.email,\n        u.segment,\n        COUNT(DISTINCT DATE(ee.timestamp)) as active_days,\n        COUNT(DISTINCT ee.session_id) as total_sessions,\n        COUNT(*) as total_events,\n        MAX(ee.timestamp) as last_activity\n      FROM user_profiles u\n      LEFT JOIN enriched_events ee ON u.id = ee.user_id\n      GROUP BY u.id, u.name, u.email, u.segment\n      ORDER BY total_events DESC\n    description: \"User engagement summary\"\n\n  - name: product_performance\n    sql: |\n      SELECT\n        p.id as product_id,\n        p.name as product_name,\n        p.category,\n        COUNT(*) as event_count,\n        COUNT(DISTINCT ee.user_id) as unique_users,\n        MAX(ee.timestamp) as last_mentioned\n      FROM product_data p\n      JOIN enriched_events ee ON p.id = ee.properties-&gt;&gt;'product_id'\n      GROUP BY p.id, p.name, p.category\n      ORDER BY event_count DESC\n    description: \"Product engagement analysis\"\n</code></pre> <p>Key Concepts Demonstrated: - SQL view composition (building on previous views) - Cross-source joins (Parquet + DuckDB + PostgreSQL) - JSON field access (<code>properties-&gt;&gt;'product_id'</code>) - Aggregation and window functions - Complex business logic implementation</p>"},{"location":"examples/multi-source-analytics/#executive-reporting","title":"Executive Reporting","text":"<pre><code>  - name: daily_kpi_report\n    sql: |\n      WITH daily_metrics AS (\n        SELECT\n          DATE(timestamp) as event_date,\n          COUNT(*) as total_events,\n          COUNT(DISTINCT user_id) as daily_active_users,\n          COUNT(DISTINCT session_id) as daily_sessions\n        FROM enriched_events\n        GROUP BY DATE(timestamp)\n      )\n      SELECT\n        event_date,\n        total_events,\n        daily_active_users,\n        daily_sessions,\n        ROUND(total_events * 1.0 / daily_sessions, 2) as events_per_session,\n        LAG(daily_active_users) OVER (ORDER BY event_date) as prev_day_users,\n        ROUND((daily_active_users - LAG(daily_active_users) OVER (ORDER BY event_date)) * 100.0 /\n              LAG(daily_active_users) OVER (ORDER BY event_date), 2) as user_growth_pct\n      FROM daily_metrics\n      ORDER BY event_date DESC\n    description: \"Executive KPI dashboard data\"\n</code></pre> <p>Key Concepts Demonstrated: - Common Table Expressions (CTEs) - Window functions for trend analysis - Percentage growth calculations - Executive-level metrics aggregation</p>"},{"location":"examples/multi-source-analytics/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"examples/multi-source-analytics/#1-prepare-environment-variables","title":"1. Prepare Environment Variables","text":"<p>Create a <code>.env</code> file or export variables in your shell:</p> <pre><code># AWS credentials for S3 access\nexport AWS_ACCESS_KEY_ID=\"your_aws_access_key\"\nexport AWS_SECRET_ACCESS_KEY=\"your_aws_secret_key\"\nexport AWS_SESSION_TOKEN=\"your_session_token\"  # Optional\n\n# PostgreSQL credentials\nexport PROD_DB_HOST=\"prod-db.company.com\"\nexport PROD_DB_USER=\"analytics_user\"\nexport PROD_DB_PASSWORD=\"secure_password\"\n\nexport STAGING_DB_HOST=\"staging-db.company.com\"\nexport STAGING_DB_USER=\"analytics_staging\"\nexport STAGING_DB_PASSWORD=\"staging_password\"\n\n# Iceberg catalog tokens\nexport ICEBERG_PROD_TOKEN=\"your_production_catalog_token\"\nexport ICEBERG_STAGING_TOKEN=\"your_staging_catalog_token\"\n</code></pre>"},{"location":"examples/multi-source-analytics/#2-validate-configuration","title":"2. Validate Configuration","text":"<p>Before building, validate your configuration:</p> <pre><code>duckalog validate docs/examples/multi-source-analytics-config.yaml\n</code></pre> <p>This checks: - YAML syntax - Environment variable resolution - View definitions - Attachment configurations</p>"},{"location":"examples/multi-source-analytics/#3-generate-sql-optional","title":"3. Generate SQL (Optional)","text":"<p>Preview the SQL that will be executed:</p> <pre><code>duckalog generate-sql docs/examples/multi-source-analytics-config.yaml --output preview.sql\ncat preview.sql\n</code></pre>"},{"location":"examples/multi-source-analytics/#4-build-the-catalog","title":"4. Build the Catalog","text":"<p>Create your unified analytics catalog:</p> <pre><code>duckalog build docs/examples/multi-source-analytics-config.yaml\n</code></pre> <p>This will: - Install required DuckDB extensions - Create <code>multi_source.duckdb</code> database - Set up all attachments - Create all views and joins</p>"},{"location":"examples/multi-source-analytics/#5-query-your-data","title":"5. Query Your Data","text":"<p>Connect to the unified catalog:</p> <pre><code># Using DuckDB CLI\nduckdb multi_source.duckdb\n\n# Example queries in DuckDB:\nSELECT * FROM daily_kpi_report ORDER BY event_date DESC LIMIT 10;\n\nSELECT \n  event_type,\n  COUNT(*) as events,\n  COUNT(DISTINCT user_id) as users\nFROM enriched_events \nWHERE DATE(timestamp) &gt;= CURRENT_DATE - INTERVAL 7 DAYS\nGROUP BY event_type\nORDER BY events DESC;\n</code></pre>"},{"location":"examples/multi-source-analytics/#6-use-programmatically","title":"6. Use Programmatically","text":"<pre><code>from duckalog import load_config\nimport duckdb\n\n# Load configuration\nconfig = load_config(\"docs/examples/multi-source-analytics-config.yaml\")\n\n# Connect to the created catalog\ncon = duckdb.connect(\"multi_source.duckdb\")\n\n# Run analytics queries\ndf = con.execute(\"\"\"\n    SELECT * FROM user_activity_summary \n    WHERE active_days &gt;= 7 \n    ORDER BY total_events DESC\n\"\"\").df()\n\nprint(df.head())\n</code></pre>"},{"location":"examples/multi-source-analytics/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<p>This example teaches several important Duckalog patterns:</p> <ol> <li>Multi-Source Unification: How to bring together Parquet, DuckDB, SQLite, PostgreSQL, and Iceberg</li> <li>Environment-Based Configuration: Using environment variables for different deployments</li> <li>Progressive Data Enrichment: Building from raw data to enriched analytics</li> <li>Performance Optimization: Memory limits, threading, and read-only attachments</li> <li>Security Best Practices: No hardcoded credentials, read-only when possible</li> <li>SQL Composition: Building complex analytics through view composition</li> <li>Business Logic: Implementing real analytics patterns (KPI reporting, user engagement)</li> </ol>"},{"location":"examples/multi-source-analytics/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/multi-source-analytics/#common-issues","title":"Common Issues","text":"<p>Missing Environment Variables: <pre><code># Check what variables are missing\nduckalog validate docs/examples/multi-source-analytics-config.yaml\n# Look for \"Environment variable 'VAR_NAME' not found\" errors\n</code></pre></p> <p>S3 Connection Errors: <pre><code># Verify AWS credentials\naws s3 ls s3://company-data-lake/events/raw/\n# Check S3 region and credentials in config\n</code></pre></p> <p>Database Connection Failures: <pre><code># Test PostgreSQL connection manually\npsql -h prod-db.company.com -U analytics_user -d analytics_prod\n# Verify SSL settings and firewall rules\n</code></pre></p> <p>Iceberg Catalog Issues: <pre><code># Check catalog accessibility\ncurl -H \"Authorization: Bearer $ICEBERG_PROD_TOKEN\" \\\n     \"https://iceberg-catalog.production.company.com/v1/config\"\n</code></pre></p>"},{"location":"examples/multi-source-analytics/#performance-tips","title":"Performance Tips","text":"<ol> <li>Memory Limits: Adjust <code>memory_limit</code> based on available RAM</li> <li>Threading: Set <code>threads</code> to number of CPU cores</li> <li>Read-Only Attachments: Use <code>read_only: true</code> when possible</li> <li>Query Optimization: Use <code>generate-sql</code> to optimize complex views</li> <li>Index Consideration: Ensure source tables have appropriate indexes</li> </ol>"},{"location":"examples/multi-source-analytics/#variations-and-customizations","title":"Variations and Customizations","text":""},{"location":"examples/multi-source-analytics/#environment-specific-configs","title":"Environment-Specific Configs","text":"<p>Create separate configs for different environments:</p> <p><code>config-production.yaml</code>: <pre><code>iceberg_catalogs:\n  - name: main\n    catalog_type: rest\n    uri: \"https://iceberg.company.com/prod\"\n    # Production settings\n</code></pre></p> <p><code>config-staging.yaml</code>: <pre><code>iceberg_catalogs:\n  - name: main\n    catalog_type: rest\n    uri: \"https://iceberg.company.com/staging\"\n    # Staging settings\n</code></pre></p>"},{"location":"examples/multi-source-analytics/#incremental-loading","title":"Incremental Loading","text":"<p>For large datasets, consider incremental views:</p> <pre><code>- name: recent_events\n  sql: |\n    SELECT * FROM enriched_events\n    WHERE DATE(timestamp) &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n</code></pre>"},{"location":"examples/multi-source-analytics/#custom-aggregations","title":"Custom Aggregations","text":"<p>Add domain-specific aggregations:</p> <pre><code>- name: cohort_analysis\n  sql: |\n    SELECT \n      DATE_TRUNC('week', u.signup_date) as cohort_week,\n      COUNT(DISTINCT u.id) as cohort_size,\n      COUNT(DISTINCT ee.user_id) as retained_users\n    FROM user_profiles u\n    LEFT JOIN enriched_events ee ON u.id = ee.user_id\n    GROUP BY DATE_TRUNC('week', u.signup_date)\n</code></pre> <p>This example demonstrates Duckalog's power to unify diverse data sources into a coherent analytics platform. Adapt the patterns shown here to your specific data landscape and business requirements.</p>"},{"location":"examples/path-resolution-examples/","title":"Path Resolution Examples","text":"<p>This section provides practical examples of using Duckalog's path resolution feature in various scenarios.</p>"},{"location":"examples/path-resolution-examples/#example-1-basic-project-structure","title":"Example 1: Basic Project Structure","text":""},{"location":"examples/path-resolution-examples/#project-layout","title":"Project Layout","text":"<pre><code>data-analytics/\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 catalog.yaml\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u2502   \u251c\u2500\u2500 events.parquet\n\u2502   \u2502   \u2514\u2500\u2500 users.parquet\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2502   \u251c\u2500\u2500 daily_metrics.parquet\n\u2502   \u2502   \u2514\u2500\u2500 user_segments.parquet\n\u2502   \u2514\u2500\u2500 external/\n\u2502       \u2514\u2500\u2500 reference_data.parquet\n\u2514\u2500\u2500 databases/\n    \u2514\u2500\u2500 reference.duckdb\n</code></pre>"},{"location":"examples/path-resolution-examples/#configuration-configcatalogyaml","title":"Configuration (<code>config/catalog.yaml</code>)","text":"<pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='2GB'\"\n\nattachments:\n  duckdb:\n    - alias: refdb\n      path: ../databases/reference.duckdb\n      read_only: true\n\nviews:\n  # Raw data files\n  - name: raw_events\n    source: parquet\n    uri: ../data/raw/events.parquet\n    description: \"Raw event data from production\"\n\n  - name: raw_users\n    source: parquet\n    uri: ../data/raw/users.parquet\n    description: \"Raw user data\"\n\n  # Processed data files\n  - name: daily_metrics\n    source: parquet\n    uri: ../data/processed/daily_metrics.parquet\n    description: \"Daily aggregated metrics\"\n\n  - name: user_segments\n    source: parquet\n    uri: ../data/processed/user_segments.parquet\n    description: \"User segmentation data\"\n\n  # External reference data\n  - name: external_reference\n    source: parquet\n    uri: ../data/external/reference_data.parquet\n    description: \"External reference data\"\n\n  # Data from attached database\n  - name: reference_customers\n    source: duckdb\n    database: refdb\n    table: customers\n    description: \"Customer reference data\"\n\n  # Combined analytics view\n  - name: analytics_dashboard\n    sql: |\n      SELECT \n        u.user_id,\n        u.name,\n        u.email,\n        us.segment_name,\n        dm.event_date,\n        dm.total_events,\n        dm.unique_users\n      FROM raw_users u\n      LEFT JOIN user_segments us ON u.user_id = us.user_id\n      LEFT JOIN daily_metrics dm ON u.segment_id = dm.segment_id\n      WHERE dm.event_date &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n</code></pre>"},{"location":"examples/path-resolution-examples/#loading-with-path-resolution","title":"Loading with Path Resolution","text":"<pre><code>from duckalog import load_config, generate_sql\n\n# Load configuration with automatic path resolution\nconfig = load_config(\"config/catalog.yaml\")\n\nprint(f\"Loaded config with {len(config.views)} views\")\n\n# Verify paths were resolved\nfor view in config.views:\n    if hasattr(view, 'uri') and view.uri:\n        print(f\"View: {view.name}\")\n        print(f\"  URI: {view.uri}\")\n        print(f\"  Is absolute: {view.uri.startswith('/')}\")\n</code></pre>"},{"location":"examples/path-resolution-examples/#example-2-multi-environment-configuration","title":"Example 2: Multi-Environment Configuration","text":""},{"location":"examples/path-resolution-examples/#development-vs-production","title":"Development vs Production","text":"<p>Development Structure: <pre><code>dev-project/\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 catalog.yaml\n\u2502   \u2514\u2500\u2500 catalog-dev.yaml\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 sample/\n\u2502   \u2502   \u2514\u2500\u2500 events.parquet\n\u2502   \u2514\u2500\u2500 testing/\n\u2502       \u2514\u2500\u2500 users.parquet\n\u2514\u2500\u2500 databases/\n    \u2514\u2500\u2500 dev_reference.duckdb\n</code></pre></p> <p>Production Structure: <pre><code>prod-project/\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 catalog.yaml\n\u2502   \u2514\u2500\u2500 catalog-prod.yaml\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 live/\n\u2502   \u2502   \u2514\u2500\u2500 events.parquet\n\u2502   \u2514\u2500\u2500 archive/\n\u2502       \u2514\u2500\u2500 users.parquet\n\u2514\u2500\u2500 databases/\n    \u2514\u2500\u2500 prod_reference.duckdb\n</code></pre></p>"},{"location":"examples/path-resolution-examples/#development-configuration-dev-projectconfigcatalogyaml","title":"Development Configuration (<code>dev-project/config/catalog.yaml</code>)","text":"<pre><code>version: 1\n\nduckdb:\n  database: dev_catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='1GB'\"\n\nattachments:\n  duckdb:\n    - alias: refdb\n      path: ../databases/dev_reference.duckdb\n      read_only: true\n\nviews:\n  - name: dev_events\n    source: parquet\n    uri: ../data/sample/events.parquet\n    description: \"Development sample events\"\n\n  - name: dev_users\n    source: parquet\n    uri: ../data/testing/users.parquet\n    description: \"Development test users\"\n</code></pre>"},{"location":"examples/path-resolution-examples/#production-configuration-prod-projectconfigcatalogyaml","title":"Production Configuration (<code>prod-project/config/catalog.yaml</code>)","text":"<pre><code>version: 1\n\nduckdb:\n  database: prod_catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='8GB'\"\n    - \"SET threads=8\"\n\nattachments:\n  duckdb:\n    - alias: refdb\n      path: ../databases/prod_reference.duckdb\n      read_only: true\n\nviews:\n  - name: prod_events\n    source: parquet\n    uri: ../data/live/events.parquet\n    description: \"Production live events\"\n\n  - name: prod_users\n    source: parquet\n    uri: ../data/archive/users.parquet\n    description: \"Production archived users\"\n</code></pre>"},{"location":"examples/path-resolution-examples/#environment-aware-loading","title":"Environment-Aware Loading","text":"<pre><code>import os\nfrom duckalog import load_config\n\ndef load_environment_config(base_path: str, env: str = \"dev\"):\n    \"\"\"Load configuration for specific environment\"\"\"\n    config_path = os.path.join(base_path, f\"{env}-project/config/catalog.yaml\")\n    return load_config(config_path)\n\n# Load development configuration\ndev_config = load_environment_config(\"/path/to/projects\", \"dev\")\nprint(f\"Development config: {dev_config.views[0].uri}\")\n\n# Load production configuration  \nprod_config = load_environment_config(\"/path/to/projects\", \"prod\")\nprint(f\"Production config: {prod_config.views[0].uri}\")\n</code></pre>"},{"location":"examples/path-resolution-examples/#example-3-shared-enterprise-resources","title":"Example 3: Shared Enterprise Resources","text":""},{"location":"examples/path-resolution-examples/#enterprise-structure","title":"Enterprise Structure","text":"<pre><code>company/\n\u251c\u2500\u2500 shared/\n\u2502   \u251c\u2500\u2500 reference_data/\n\u2502   \u2502   \u251c\u2500\u2500 geography.parquet\n\u2502   \u2502   \u251c\u2500\u2500 currency_rates.parquet\n\u2502   \u2502   \u2514\u2500\u2500 company_calendar.parquet\n\u2502   \u251c\u2500\u2500 databases/\n\u2502   \u2502   \u251c\u2500\u2500 enterprise_master.duckdb\n\u2502   \u2502   \u2514\u2500\u2500 analytics_cache.duckdb\n\u2502   \u2514\u2500\u2500 schemas/\n\u2502       \u2514\u2500\u2500 standard_views.sql\n\u251c\u2500\u2500 analytics/\n\u2502   \u251c\u2500\u2500 config/\n\u2502   \u2502   \u2514\u2500\u2500 catalog.yaml\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 department_a/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 metrics.parquet\n\u2502   \u2502   \u2514\u2500\u2500 department_b/\n\u2502   \u2502       \u2514\u2500\u2500 reports.parquet\n\u2502   \u2514\u2500\u2500 sql/\n\u2502       \u2514\u2500\u2500 custom_views.sql\n\u2514\u2500\u2500 finance/\n    \u251c\u2500\u2500 config/\n    \u2502   \u2514\u2500\u2500 catalog.yaml\n    \u251c\u2500\u2500 data/\n    \u2502   \u251c\u2500\u2500 transactions/\n    \u2502   \u2502   \u2514\u2500\u2500 daily.parquet\n    \u2502   \u2514\u2500\u2500 budgets/\n    \u2502       \u2514\u2500\u2500 quarterly.parquet\n    \u2514\u2500\u2500 sql/\n        \u2514\u2500\u2500 financial_views.sql\n</code></pre>"},{"location":"examples/path-resolution-examples/#analytics-configuration-companyanalyticsconfigcatalogyaml","title":"Analytics Configuration (<code>company/analytics/config/catalog.yaml</code>)","text":"<pre><code>version: 1\n\nduckdb:\n  database: analytics_catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='4GB'\"\n    - \"SET threads=4\"\n\n# Attach shared enterprise databases\nattachments:\n  duckdb:\n    - alias: enterprise_master\n      path: ../../shared/databases/enterprise_master.duckdb\n      read_only: true\n    - alias: analytics_cache\n      path: ../../shared/databases/analytics_cache.duckdb\n      read_only: true\n\nviews:\n  # Shared enterprise reference data\n  - name: geography\n    source: parquet\n    uri: ../../shared/reference_data/geography.parquet\n    description: \"Geographic reference data\"\n\n  - name: currency_rates\n    source: parquet\n    uri: ../../shared/reference_data/currency_rates.parquet\n    description: \"Currency exchange rates\"\n\n  - name: company_calendar\n    source: parquet\n    uri: ../../shared/reference_data/company_calendar.parquet\n    description: \"Company calendar with holidays\"\n\n  # Local department data\n  - name: dept_a_metrics\n    source: parquet\n    uri: ../data/department_a/metrics.parquet\n    description: \"Department A metrics\"\n\n  - name: dept_b_reports\n    source: parquet\n    uri: ../data/department_b/reports.parquet\n    description: \"Department B reports\"\n\n  # Views using enterprise database\n  - name: enterprise_customers\n    source: duckdb\n    database: enterprise_master\n    table: customers\n    description: \"Enterprise customer master\"\n\n  # Combined analytics views\n  - name: regional_performance\n    sql: |\n      SELECT \n        g.region_name,\n        g.country_code,\n        dm.metric_type,\n        dm.metric_value,\n        dm.reporting_date\n      FROM geography g\n      JOIN dept_a_metrics dm ON g.region_id = dm.region_id\n      WHERE dm.reporting_date &gt;= CURRENT_DATE - INTERVAL 90 DAYS\n\n  - name: budget_vs_actual\n    sql: |\n      SELECT \n        cc.fiscal_year,\n        cc.fiscal_quarter,\n        b.budget_amount,\n        SUM(dm.metric_value) as actual_amount,\n        (SUM(dm.metric_value) - b.budget_amount) as variance\n      FROM company_calendar cc\n      JOIN dept_b_reports dm ON cc.date_key = dm.reporting_date\n      LEFT JOIN (\n        SELECT * FROM enterprise_master.finance_budgets\n      ) b ON cc.fiscal_quarter = b.quarter AND cc.fiscal_year = b.year\n      GROUP BY cc.fiscal_year, cc.fiscal_quarter, b.budget_amount\n</code></pre>"},{"location":"examples/path-resolution-examples/#finance-configuration-companyfinanceconfigcatalogyaml","title":"Finance Configuration (<code>company/finance/config/catalog.yaml</code>)","text":"<pre><code>version: 1\n\nduckdb:\n  database: finance_catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='6GB'\"\n    - \"SET threads=6\"\n\n# Attach shared enterprise databases (same as analytics)\nattachments:\n  duckdb:\n    - alias: enterprise_master\n      path: ../../shared/databases/enterprise_master.duckdb\n      read_only: true\n\nviews:\n  # Shared enterprise reference data\n  - name: currency_rates\n    source: parquet\n    uri: ../../shared/reference_data/currency_rates.parquet\n    description: \"Currency exchange rates\"\n\n  # Finance-specific data\n  - name: daily_transactions\n    source: parquet\n    uri: ../data/transactions/daily.parquet\n    description: \"Daily transaction data\"\n\n  - name: quarterly_budgets\n    source: parquet\n    uri: ../data/budgets/quarterly.parquet\n    description: \"Quarterly budget data\"\n\n  # Financial analytics\n  - name: transaction_analysis\n    sql: |\n      SELECT \n        dt.transaction_date,\n        dt.customer_id,\n        dt.amount,\n        dt.currency,\n        cr.usd_rate,\n        (dt.amount * cr.usd_rate) as amount_usd,\n        em.customer_segment\n      FROM daily_transactions dt\n      JOIN currency_rates cr ON dt.currency = cr.currency_code \n        AND dt.transaction_date = cr.effective_date\n      LEFT JOIN enterprise_master.customers em ON dt.customer_id = em.customer_id\n      WHERE dt.transaction_date &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n</code></pre>"},{"location":"examples/path-resolution-examples/#example-4-cloud-and-local-hybrid-setup","title":"Example 4: Cloud and Local Hybrid Setup","text":""},{"location":"examples/path-resolution-examples/#hybrid-project-structure","title":"Hybrid Project Structure","text":"<pre><code>hybrid-analytics/\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 catalog.yaml\n\u251c\u2500\u2500 local-data/\n\u2502   \u251c\u2500\u2500 sensitive/\n\u2502   \u2502   \u2514\u2500\u2500 pii_data.parquet\n\u2502   \u2514\u2500\u2500 cache/\n\u2502       \u2514\u2500\u2500 local_cache.duckdb\n\u251c\u2500\u2500 databases/\n\u2502   \u2514\u2500\u2500 reference.duckdb\n\u2514\u2500\u2500 sql/\n    \u2514\u2500\u2500 views/\n</code></pre>"},{"location":"examples/path-resolution-examples/#hybrid-configuration-configcatalogyaml","title":"Hybrid Configuration (<code>config/catalog.yaml</code>)","text":"<pre><code>version: 1\n\nduckdb:\n  database: hybrid_catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='4GB'\"\n    - \"SET s3_region='us-west-2'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n\nattachments:\n  duckdb:\n    - alias: refdb\n      path: ../databases/reference.duckdb\n      read_only: true\n    - alias: cache\n      path: ../local-data/cache/local_cache.duckdb\n      read_only: false  # Allow writing to cache\n\nviews:\n  # Cloud data (remote URIs - not resolved)\n  - name: cloud_events\n    source: parquet\n    uri: s3://data-lake/raw/events/*.parquet\n    description: \"Cloud-based raw events\"\n\n  - name: cloud_users  \n    source: parquet\n    uri: s3://data-lake/processed/users.parquet\n    description: \"Cloud-based processed users\"\n\n  # Local sensitive data (resolved relative paths)\n  - name: sensitive_pii\n    source: parquet\n    uri: ../local-data/sensitive/pii_data.parquet\n    description: \"Local PII data (not stored in cloud)\"\n\n  # Local reference data\n  - name: local_reference\n    source: duckdb\n    database: refdb\n    table: reference_data\n    description: \"Local reference database\"\n\n  # Cache data (read-write attachment)\n  - name: cached_results\n    source: duckdb\n    database: cache\n    table: precomputed_metrics\n    description: \"Locally cached computation results\"\n\n  # Hybrid views combining cloud and local data\n  - name: enhanced_events\n    sql: |\n      SELECT \n        ce.event_id,\n        ce.timestamp,\n        ce.event_type,\n        ce.properties,\n        sp.customer_name,\n        sp.email,\n        sp.segment,\n        lr.internal_category\n      FROM cloud_events ce\n      LEFT JOIN sensitive_pii sp ON ce.user_id = sp.user_id\n      LEFT JOIN local_reference lr ON ce.event_type = lr.event_category\n      WHERE ce.timestamp &gt;= CURRENT_DATE - INTERVAL 7 DAYS\n\n  # View that updates local cache\n  - name: update_cache\n    sql: |\n      -- Update local cache with latest aggregations\n      INSERT OR REPLACE INTO cache.precomputed_metrics\n      SELECT \n        DATE(timestamp) as event_date,\n        event_type,\n        COUNT(*) as event_count,\n        COUNT(DISTINCT user_id) as unique_users,\n        CURRENT_TIMESTAMP as cache_updated\n      FROM cloud_events \n      WHERE timestamp &gt;= CURRENT_DATE - INTERVAL 1 DAY\n      GROUP BY DATE(timestamp), event_type\n</code></pre>"},{"location":"examples/path-resolution-examples/#example-5-security-demonstration","title":"Example 5: Security Demonstration","text":""},{"location":"examples/path-resolution-examples/#security-test-configuration","title":"Security Test Configuration","text":"<pre><code>version: 1\n\nduckdb:\n  database: security_test.duckdb\n\nviews:\n  # \u2705 ALLOWED - Safe relative paths\n  - name: safe_local_data\n    source: parquet\n    uri: ./data/local.parquet\n    description: \"Safe local data\"\n\n  - name: safe_parent_data\n    source: parquet\n    uri: ../shared/data.parquet\n    description: \"Safe parent directory data\"\n\n  - name: safe_grandparent_data\n    source: parquet\n    uri: ../../project/common.parquet\n    description: \"Safe grandparent directory data\"\n\n  # \u2705 ALLOWED - Remote URIs (bypass local file security)\n  - name: remote_s3_data\n    source: parquet\n    uri: s3://secure-bucket/data.parquet\n    description: \"Remote S3 data\"\n\n  - name: remote_http_data\n    source: parquet\n    uri: https://api.example.com/data.parquet\n    description: \"Remote HTTP data\"\n\n  # \u274c BLOCKED - Security violations (these would cause errors)\n  # Uncommenting these would cause ConfigError during loading\n\n  # - name: system_files\n  #   source: parquet\n  #   uri: ../../../etc/passwd\n  #   description: \"Blocked - system file access\"\n\n  # - name: system_config\n  #   source: parquet\n  #   uri: ../usr/local/config.parquet\n  #   description: \"Blocked - system directory access\"\n\n  # - name: excessive_traversal\n  #   source: parquet\n  #   uri: ../../../../../../../root/.ssh/id_rsa\n  #   description: \"Blocked - excessive directory traversal\"\n</code></pre>"},{"location":"examples/path-resolution-examples/#security-validation-code","title":"Security Validation Code","text":"<pre><code>from duckalog import load_config, ConfigError\nfrom duckalog.path_resolution import validate_path_security, is_relative_path\nfrom pathlib import Path\n\ndef test_security_examples():\n    \"\"\"Demonstrate security validation\"\"\"\n\n    config_dir = Path(\"/project/config\")\n\n    # Test safe paths\n    safe_paths = [\n        \"./data/local.parquet\",\n        \"../shared/data.parquet\", \n        \"../../project/common.parquet\"\n    ]\n\n    print(\"Testing safe paths:\")\n    for path in safe_paths:\n        is_rel = is_relative_path(path)\n        is_safe = validate_path_security(path, config_dir)\n        print(f\"  {path}: relative={is_rel}, safe={is_safe}\")\n\n    # Test dangerous paths\n    dangerous_paths = [\n        \"../../../etc/passwd\",\n        \"../usr/local/config.parquet\",\n        \"../../../../root/.ssh/id_rsa\"\n    ]\n\n    print(\"\\nTesting dangerous paths:\")\n    for path in dangerous_paths:\n        is_rel = is_relative_path(path)\n        is_safe = validate_path_security(path, config_dir)\n        print(f\"  {path}: relative={is_rel}, safe={is_safe}\")\n\n# Run security tests\ntest_security_examples()\n\n# Test configuration loading with security validation\ntry:\n    config = load_config(\"security_test_catalog.yaml\")\n    print(\"Configuration loaded successfully\")\nexcept ConfigError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"examples/path-resolution-examples/#example-6-programmatic-path-resolution","title":"Example 6: Programmatic Path Resolution","text":""},{"location":"examples/path-resolution-examples/#direct-path-resolution-usage","title":"Direct Path Resolution Usage","text":"<pre><code>from duckalog.path_resolution import (\n    resolve_relative_path,\n    validate_path_security,\n    detect_path_type,\n    normalize_path_for_sql\n)\nfrom pathlib import Path\n\ndef demonstrate_path_resolution():\n    \"\"\"Demonstrate path resolution functionality\"\"\"\n\n    config_dir = Path(\"/project/analytics/config\")\n\n    # Test different path types\n    test_paths = [\n        \"data/events.parquet\",\n        \"../shared/reference.parquet\",\n        \"/absolute/path/data.parquet\",\n        \"s3://bucket/data.parquet\",\n        \"../../../etc/passwd\",  # This should fail security validation\n    ]\n\n    print(\"Path Resolution Examples:\")\n    print(\"=\" * 50)\n\n    for path in test_paths:\n        print(f\"\\nOriginal path: {path}\")\n\n        # Detect path type\n        path_type = detect_path_type(path)\n        print(f\"  Path type: {path_type}\")\n\n        # Check if relative\n        is_rel = is_relative_path(path)\n        print(f\"  Is relative: {is_rel}\")\n\n        # Validate security\n        try:\n            is_safe = validate_path_security(path, config_dir)\n            print(f\"  Security validation: {'PASS' if is_safe else 'BLOCKED'}\")\n        except Exception as e:\n            print(f\"  Security validation: ERROR - {e}\")\n\n        # Resolve if relative and safe\n        if is_rel:\n            try:\n                resolved = resolve_relative_path(path, config_dir)\n                print(f\"  Resolved path: {resolved}\")\n\n                # Normalize for SQL\n                sql_path = normalize_path_for_sql(resolved)\n                print(f\"  SQL format: {sql_path}\")\n\n            except Exception as e:\n                print(f\"  Resolution failed: {e}\")\n        else:\n            print(f\"  Path unchanged: {path}\")\n\n# Run demonstration\ndemonstrate_path_resolution()\n\n# Example: Building dynamic configuration\ndef build_dynamic_config(config_file_path: str, data_patterns: dict):\n    \"\"\"Build configuration with dynamic path resolution\"\"\"\n\n    config_dir = Path(config_file_path).parent\n    resolved_views = []\n\n    for view_name, path_pattern in data_patterns.items():\n        try:\n            # Resolve the path pattern\n            resolved_path = resolve_relative_path(path_pattern, config_dir)\n\n            # Create view with resolved path\n            view = {\n                \"name\": view_name,\n                \"source\": \"parquet\",\n                \"uri\": resolved_path,\n                \"description\": f\"Auto-resolved view for {view_name}\"\n            }\n            resolved_views.append(view)\n\n        except Exception as e:\n            print(f\"Failed to resolve {view_name}: {e}\")\n\n    return {\n        \"version\": 1,\n        \"duckdb\": {\"database\": \"dynamic_catalog.duckdb\"},\n        \"views\": resolved_views\n    }\n\n# Usage example\ndata_sources = {\n    \"raw_events\": \"../data/events/raw/*.parquet\",\n    \"processed_users\": \"../data/users/processed.parquet\", \n    \"reference_data\": \"./reference/common.parquet\"\n}\n\ndynamic_config = build_dynamic_config(\"/project/config/catalog.yaml\", data_sources)\nprint(\"\\nDynamic configuration:\")\nprint(dynamic_config)\n</code></pre>"},{"location":"examples/path-resolution-examples/#example-7-migration-from-absolute-to-relative-paths","title":"Example 7: Migration from Absolute to Relative Paths","text":""},{"location":"examples/path-resolution-examples/#before-absolute-paths","title":"Before: Absolute Paths","text":"<pre><code># old-catalog.yaml\nversion: 1\n\nduckdb:\n  database: /Users/john/projects/analytics/catalog.duckdb\n\nviews:\n  - name: events\n    source: parquet\n    uri: /Users/john/projects/analytics/data/events.parquet\n\n  - name: users\n    source: parquet\n    uri: /Users/john/projects/analytics/data/users.parquet\n\nattachments:\n  duckdb:\n    - alias: refdb\n      path: /Users/john/shared/reference.duckdb\n      read_only: true\n</code></pre>"},{"location":"examples/path-resolution-examples/#after-relative-paths","title":"After: Relative Paths","text":"<pre><code># new-catalog.yaml\nversion: 1\n\nduckdb:\n  database: catalog.duckdb\n\nviews:\n  - name: events\n    source: parquet\n    uri: ./data/events.parquet\n\n  - name: users\n    source: parquet\n    uri: ./data/users.parquet\n\nattachments:\n  duckdb:\n    - alias: refdb\n      path: ../shared/reference.duckdb\n      read_only: true\n</code></pre>"},{"location":"examples/path-resolution-examples/#migration-script","title":"Migration Script","text":"<pre><code>import yaml\nimport re\nfrom pathlib import Path\nfrom duckalog.path_resolution import is_relative_path\n\ndef migrate_to_relative_paths(config_path: str, base_path: str = None):\n    \"\"\"Migrate absolute paths to relative paths in a configuration file\"\"\"\n\n    config_path = Path(config_path)\n    if base_path is None:\n        base_path = config_path.parent\n\n    # Load existing configuration\n    with open(config_path, 'r') as f:\n        config = yaml.safe_load(f)\n\n    # Helper function to convert absolute to relative path\n    def to_relative_path(abs_path: str) -&gt; str:\n        if not abs_path or not is_relative_path(abs_path):\n            try:\n                path_obj = Path(abs_path)\n                if path_obj.is_absolute():\n                    rel_path = path_obj.relative_to(base_path)\n                    return str(rel_path)\n            except (ValueError, OSError):\n                pass\n        return abs_path\n\n    # Convert database path\n    if 'duckdb' in config and 'database' in config['duckdb']:\n        config['duckdb']['database'] = to_relative_path(\n            config['duckdb']['database']\n        )\n\n    # Convert view URIs\n    if 'views' in config:\n        for view in config['views']:\n            if 'uri' in view:\n                view['uri'] = to_relative_path(view['uri'])\n\n    # Convert attachment paths\n    if 'attachments' in config:\n        for attach_type in ['duckdb', 'sqlite']:\n            if attach_type in config['attachments']:\n                for attachment in config['attachments'][attach_type]:\n                    if 'path' in attachment:\n                        attachment['path'] = to_relative_path(attachment['path'])\n\n    # Save migrated configuration\n    backup_path = config_path.with_suffix('.yaml.backup')\n    config_path.rename(backup_path)\n\n    with open(config_path, 'w') as f:\n        yaml.dump(config, f, default_flow_style=False)\n\n    print(f\"Configuration migrated to relative paths\")\n    print(f\"Original saved to: {backup_path}\")\n    print(f\"New config: {config_path}\")\n\n# Usage\nmigrate_to_relative_paths(\"old-catalog.yaml\")\n</code></pre> <p>These examples demonstrate the flexibility and security of Duckalog's path resolution feature across various real-world scenarios. The key benefits are:</p> <ol> <li>Portability: Configurations can be moved between environments</li> <li>Security: Automatic protection against dangerous file access</li> <li>Maintainability: Clear relative path structure</li> <li>Flexibility: Works with local files, shared resources, and remote URIs</li> <li>Cross-Platform: Consistent behavior across operating systems</li> </ol>"},{"location":"examples/simple-parquet/","title":"Simple Parquet Example","text":"<p>This example shows how to create DuckDB views over Parquet files stored in cloud storage. It's perfect for getting started with Duckalog when your data is already in Parquet format.</p>"},{"location":"examples/simple-parquet/#when-to-use-this-example","title":"When to Use This Example","text":"<p>Choose this example if: - Your data is in Parquet files (local or S3) - You want to query Parquet data with SQL - You're working with partitioned data - You need simple, fast analytics without complex joins - You want to combine multiple Parquet datasets</p>"},{"location":"examples/simple-parquet/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Duckalog installed: <pre><code>pip install duckalog\n</code></pre></p> </li> <li> <p>Sample Parquet files - You can use your own or create test data:     ```python     # Create sample Parquet files for testing     import polars as pl     import duckdb</p> </li> </ol> <p>events_df = pd.DataFrame({        'event_id': range(1, 1001),        'user_id': [i % 100 + 1 for i in range(1000)],        'event_type': ['page_view', 'click', 'purchase'] * 333 + ['page_view'],        'timestamp': pd.date_range('2023-01-01', periods=1000, freq='H'),        'value': [i * 0.5 for i in range(1000)]    })</p> <p># Save as Parquet    users_df.to_parquet('./users.parquet')    events_df.to_parquet('./events.parquet')</p> <p># Or upload to S3 (if you have AWS credentials configured)    import boto3    s3 = boto3.client('s3')    users_df.to_parquet('s3://your-bucket/data/users.parquet')    events_df.to_parquet('s3://your-bucket/data/events.parquet')    ```</p>"},{"location":"examples/simple-parquet/#create-sample-data","title":"Create sample data","text":"<p>users_df = pl.DataFrame({    'user_id': range(1, 101),    'name': [f'User {i}' for i in range(1, 101)],    'signup_date': pd.date_range('2023-01-01', periods=100),    'region': ['US', 'EU', 'APAC'] * 33 + ['US']    })</p>"},{"location":"examples/simple-parquet/#basic-configuration-pattern","title":"Basic Configuration Pattern","text":""},{"location":"examples/simple-parquet/#single-view-example","title":"Single View Example","text":"<p>Create a file called <code>simple-parquet.yaml</code>:</p> <pre><code>version: 1\n\n# DuckDB configuration\nduckdb:\n  database: simple_catalog.duckdb\n  install_extensions:\n    - httpfs  # Required for cloud storage access\n\n  pragmas:\n    # Performance settings\n    - \"SET memory_limit='1GB'\"\n    - \"SET threads=2\"\n\n    # S3 configuration (if using cloud storage)\n    - \"SET s3_region='us-east-1'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n\n# View definitions\nviews:\n  # Simple single-table view\n  - name: users\n    source: parquet\n    uri: \"s3://your-bucket/data/users.parquet\"  # Or local: \"./data/users.parquet\"\n    description: \"User reference data from Parquet\"\n</code></pre> <p>With Path Resolution:</p> <p>Duckalog automatically resolves relative paths to absolute paths relative to the configuration file location. This makes your configurations more portable and consistent.</p> <pre><code>version: 1\n\nduckdb:\n  database: simple_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  pragmas:\n    - \"SET memory_limit='1GB'\"\n    - \"SET s3_region='us-east-1'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n\nviews:\n  # Local relative paths - automatically resolved\n  - name: local_users\n    source: parquet\n    uri: ./data/users.parquet  # Resolved to absolute path relative to this config\n    description: \"Local user data with automatic path resolution\"\n\n  - name: local_events\n    source: parquet\n    uri: ../shared/events.parquet  # Parent directory access allowed\n    description: \"Shared events data from parent directory\"\n\n  # Cloud URIs - unchanged by path resolution\n  - name: cloud_users\n    source: parquet\n    uri: \"s3://your-bucket/data/users.parquet\"\n    description: \"Cloud user data (URI unchanged)\"\n</code></pre> <p>Key configuration elements: - <code>source: parquet</code> - Specifies Parquet file source - <code>uri</code> - Path to Parquet file (supports wildcards for partitioned data) - Extension installation for cloud access - S3 credentials via environment variables</p>"},{"location":"examples/simple-parquet/#multi-view-example-with-joins","title":"Multi-View Example with Joins","text":"<p>For more complex analytics, define multiple views:</p> <pre><code>version: 1\n\nduckdb:\n  database: analytics_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  pragmas:\n    - \"SET memory_limit='2GB'\"\n    - \"SET s3_region='us-east-1'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n\nviews:\n  # Raw data views\n  - name: raw_users\n    source: parquet\n    uri: \"s3://your-bucket/data/users/*.parquet\"  # Supports partitioned data\n    description: \"User data from partitioned Parquet files\"\n\n  - name: raw_events\n    source: parquet\n    uri: \"s3://your-bucket/data/events/*.parquet\"\n    description: \"Event data from partitioned Parquet files\"\n\n  # Derived views with analytics\n  - name: daily_active_users\n    sql: |\n      SELECT \n        DATE(timestamp) as event_date,\n        COUNT(DISTINCT user_id) as active_users,\n        COUNT(*) as total_events\n      FROM raw_events\n      GROUP BY DATE(timestamp)\n      ORDER BY event_date DESC\n    description: \"Daily active user metrics\"\n\n  - name: user_summary\n    sql: |\n      SELECT \n        u.user_id,\n        u.name,\n        u.region,\n        u.signup_date,\n        COUNT(DISTINCT DATE(e.timestamp)) as active_days,\n        COUNT(e.event_id) as total_events,\n        MAX(e.timestamp) as last_activity\n      FROM raw_users u\n      LEFT JOIN raw_events e ON u.user_id = e.user_id\n      GROUP BY u.user_id, u.name, u.region, u.signup_date\n      ORDER BY total_events DESC NULLS LAST\n    description: \"User engagement summary\"\n\n  - name: popular_content\n    sql: |\n      SELECT \n        event_type,\n        COUNT(*) as event_count,\n        COUNT(DISTINCT user_id) as unique_users,\n        AVG(value) as avg_value\n      FROM raw_events\n      WHERE event_type IN ('page_view', 'click', 'purchase')\n      GROUP BY event_type\n      ORDER BY event_count DESC\n    description: \"Most popular content types\"\n</code></pre>"},{"location":"examples/simple-parquet/#s3-configuration-deep-dive","title":"S3 Configuration Deep Dive","text":""},{"location":"examples/simple-parquet/#environment-variables","title":"Environment Variables","text":"<p>Set your AWS credentials as environment variables:</p> <pre><code># Option 1: Direct environment variables\nexport AWS_ACCESS_KEY_ID=\"AKIA...\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_SESSION_TOKEN=\"optional-session-token\"  # For temporary credentials\n\n# Option 2: Using AWS CLI\naws configure set aws_access_key_id AKIA...\naws configure set aws_secret_access_key your-secret-key\n\n# Option 3: Using IAM roles (for EC2/ECS)\n# IAM role attached to instance/container\n</code></pre>"},{"location":"examples/simple-parquet/#s3-uri-patterns","title":"S3 URI Patterns","text":"<p>Duckalog supports various S3 URI patterns:</p> <pre><code># Single file\nuri: \"s3://bucket/path/file.parquet\"\n\n# All files in directory\nuri: \"s3://bucket/path/*.parquet\"\n\n# Partitioned data with pattern\nuri: \"s3://bucket/year=2023/month=01/*.parquet\"\n\n# Specific date range\nuri: \"s3://bucket/data/2023-01-*.parquet\"\n\n# Multiple patterns (use SQL view composition)\nuri: \"s3://bucket/events/2023-*-*.parquet\"\n</code></pre>"},{"location":"examples/simple-parquet/#performance-considerations","title":"Performance Considerations","text":"<pre><code>duckdb:\n  pragmas:\n    # Optimize for Parquet reading\n    - \"SET threads=4\"                    # Match your CPU cores\n    - \"SET memory_limit='2GB'           # Set based on data size\n    - \"SET s3_region='us-east-1'        # Use same region as data\n    - \"SET enable_http_metadata_cache=true\"  # Cache HTTP metadata\n</code></pre>"},{"location":"examples/simple-parquet/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"examples/simple-parquet/#1-create-your-configuration","title":"1. Create Your Configuration","text":"<p>Save the configuration above as <code>simple-parquet.yaml</code>.</p>"},{"location":"examples/simple-parquet/#2-set-environment-variables","title":"2. Set Environment Variables","text":"<pre><code>export AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\n</code></pre>"},{"location":"examples/simple-parquet/#3-validate-configuration","title":"3. Validate Configuration","text":"<pre><code>duckalog validate simple-parquet.yaml\n</code></pre> <p>Expected output: <pre><code>\u2705 Configuration is valid\n\u2705 All views defined correctly\n\u2705 Environment variables resolved\n</code></pre></p>"},{"location":"examples/simple-parquet/#4-generate-sql-optional","title":"4. Generate SQL (Optional)","text":"<p>Preview the SQL that will be executed:</p> <pre><code>duckalog generate-sql simple-parquet.yaml --output generated.sql\n\n# View the generated SQL\ncat generated.sql\n</code></pre>"},{"location":"examples/simple-parquet/#5-build-the-catalog","title":"5. Build the Catalog","text":"<pre><code>duckalog build simple-parquet.yaml\n</code></pre> <p>This creates <code>simple_catalog.duckdb</code> with your Parquet views.</p>"},{"location":"examples/simple-parquet/#6-query-your-data","title":"6. Query Your Data","text":"<pre><code># Connect with DuckDB\nduckdb simple_catalog.duckdb\n\n# Example queries:\nSELECT * FROM users LIMIT 10;\n\nSELECT \n  region,\n  COUNT(*) as user_count\nFROM users \nGROUP BY region\nORDER BY user_count DESC;\n\nSELECT * FROM daily_active_users WHERE event_date &gt;= CURRENT_DATE - INTERVAL 7 DAYS;\n</code></pre>"},{"location":"examples/simple-parquet/#7-use-programmatically","title":"7. Use Programmatically","text":"<pre><code>from duckalog import load_config\nimport duckdb\nimport polars as pl\n\n# Load and build catalog\nbuild_catalog(\"simple-parquet.yaml\")\n\n# Connect and query\ncon = duckdb.connect(\"simple_catalog.duckdb\")\n\n# Get DataFrame for analysis\nusers_df = con.execute(\"SELECT * FROM users WHERE region = 'US'\").df()\nprint(users_df.head())\n\n# Complex analytics\nmetrics_df = con.execute(\"\"\"\n    SELECT \n      DATE(timestamp) as date,\n      COUNT(DISTINCT user_id) as daily_users,\n      COUNT(*) as total_events,\n      AVG(value) as avg_event_value\n    FROM raw_events\n    WHERE DATE(timestamp) &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n    GROUP BY DATE(timestamp)\n    ORDER BY date\n\"\"\").df()\n\nprint(metrics_df)\n</code></pre>"},{"location":"examples/simple-parquet/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<p>This example teaches:</p> <ol> <li>Parquet as Data Source: How to directly query Parquet files</li> <li>Cloud Storage Access: S3 configuration and credentials</li> <li>Performance Optimization: Memory and threading settings</li> <li>View Composition: Building analytics from raw data</li> <li>SQL in Views: Complex analytics with standard SQL</li> <li>Environment Variables: Secure credential management</li> <li>Partitioned Data: Handling large datasets with wildcards</li> </ol>"},{"location":"examples/simple-parquet/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/simple-parquet/#common-issues","title":"Common Issues","text":"<p>1. S3 Access Denied: <pre><code># Test S3 access\naws s3 ls s3://your-bucket/data/\n\n# Check permissions - user needs:\n# - s3:GetObject for file access\n# - s3:ListBucket for directory listing\n</code></pre></p> <p>2. Parquet File Not Found: <pre><code># Verify file exists\naws s3 ls s3://your-bucket/data/users.parquet\n\n# Check wildcard patterns\naws s3 ls s3://your-bucket/data/*.parquet\n</code></pre></p> <p>3. Memory Errors: <pre><code># Reduce memory usage\nduckdb:\n  pragmas:\n    - \"SET memory_limit='512MB'\"\n</code></pre></p> <p>4. Slow Queries: <pre><code># Increase parallelism\nduckdb:\n  pragmas:\n    - \"SET threads=8\"  # Match CPU cores\n</code></pre></p>"},{"location":"examples/simple-parquet/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Partitioned Data: Organize Parquet files by date/region</li> <li>Optimize File Sizes: 100MB-1GB per file works well</li> <li>Co-locate Data: Store in same S3 region as queries</li> <li>Use Compression: Parquet's built-in compression is efficient</li> <li>Filter Early: Push down filters to Parquet reading when possible</li> </ol>"},{"location":"examples/simple-parquet/#variations","title":"Variations","text":""},{"location":"examples/simple-parquet/#local-files-only","title":"Local Files Only","text":"<p>For local files without cloud access:</p> <pre><code>version: 1\n\nduckdb:\n  database: local_catalog.duckdb\n  # No extensions needed for local files\n  pragmas:\n    - \"SET memory_limit='1GB'\"\n\nviews:\n  - name: local_data\n    source: parquet\n    uri: \"./data/*.parquet\"  # Local path with wildcard\n</code></pre>"},{"location":"examples/simple-parquet/#date-partitioned-data","title":"Date-Partitioned Data","text":"<p>For time-series data:</p> <pre><code>views:\n  - name: events_2023\n    source: parquet\n    uri: \"s3://bucket/events/year=2023/*.parquet\"\n\n  - name: recent_events\n    sql: |\n      SELECT * FROM events_2023\n      WHERE DATE(timestamp) &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n</code></pre> <p>This example provides a solid foundation for working with Parquet data in Duckalog. Adapt the patterns to your specific data structure and requirements.</p>"},{"location":"guides/","title":"User Guide","text":"<p>Welcome to the Duckalog User Guide. This section covers how to use Duckalog effectively, from basic configuration to advanced patterns and troubleshooting.</p>"},{"location":"guides/#getting-started","title":"Getting Started","text":""},{"location":"guides/#configuration-structure","title":"Configuration Structure","text":"<p>At a high level, a Duckalog config looks like this:</p> <pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='1GB'\"\n\nattachments:\n  duckdb:\n    - alias: refdata\n      path: ./refdata.duckdb\n      read_only: true\n\n  sqlite:\n    - alias: legacy\n      path: ./legacy.db\n\n  postgres:\n    - alias: dw\n      host: \"${env:PG_HOST}\"\n      port: 5432\n      database: dw\n      user: \"${env:PG_USER}\"\n      password: \"${env:PG_PASSWORD}\"\n\niceberg_catalogs:\n  - name: main_ic\n    catalog_type: rest\n    uri: \"https://iceberg-catalog.internal\"\n    warehouse: \"s3://my-warehouse/\"\n    options:\n      token: \"${env:ICEBERG_TOKEN}\"\n\nviews:\n  - name: users\n    source: parquet\n    uri: \"s3://my-bucket/data/users/*.parquet\"\n\n  - name: events_delta\n    source: delta\n    uri: \"s3://my-bucket/delta/events\"\n\n  - name: ic_orders\n    source: iceberg\n    catalog: main_ic\n    table: analytics.orders\n\n  - name: ref_countries\n    source: duckdb\n    database: refdata\n    table: reference.countries\n\n  - name: vip_users\n    sql: |\n      SELECT *\n      FROM users\n      WHERE is_vip = TRUE\n</code></pre>"},{"location":"guides/#environment-variables","title":"Environment Variables","text":"<p>Any string may contain <code>${env:VAR_NAME}</code> placeholders. Duckalog resolves these using the process environment before validation. If a variable is missing, a <code>ConfigError</code> is raised.</p> <p>Example: <pre><code>duckdb:\n  pragmas:\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n</code></pre></p>"},{"location":"guides/#core-concepts","title":"Core Concepts","text":""},{"location":"guides/#attachments","title":"Attachments","text":"<p>Attachments let you expose tables from other databases inside DuckDB.</p> <ul> <li>DuckDB attachments: attach additional <code>.duckdb</code> files.</li> <li>SQLite attachments: attach local SQLite databases.</li> <li>Postgres attachments: connect to external Postgres instances.</li> </ul> <p>Views that use attached databases set <code>source</code> to <code>duckdb</code>, <code>sqlite</code>, or <code>postgres</code> and provide <code>database</code> (attachment alias) and <code>table</code>.</p>"},{"location":"guides/#iceberg-catalogs","title":"Iceberg Catalogs","text":"<p>Iceberg catalogs are configured under <code>iceberg_catalogs</code>. Iceberg views can either:</p> <ul> <li>Use a <code>uri</code> directly, or</li> <li>Refer to a <code>catalog</code> + <code>table</code> combination.</li> </ul> <p>Duckalog validates that any <code>catalog</code> used by a view is defined in <code>iceberg_catalogs</code>.</p>"},{"location":"guides/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"guides/#parquet-only-configuration","title":"Parquet-only Configuration","text":"<p>For simple cases where you only need to create views over Parquet files:</p> <pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='1GB'\"\n    - \"SET s3_region='us-west-2'\"\n\nviews:\n  - name: sales_data\n    source: parquet\n    uri: \"s3://data-bucket/sales/*.parquet\"\n\n  - name: customer_data\n    source: parquet\n    uri: \"s3://data-bucket/customers/*.parquet\"\n\n  - name: sales_by_customer\n    sql: |\n      SELECT \n        c.customer_id,\n        c.name,\n        c.region,\n        SUM(s.amount) as total_sales,\n        COUNT(s.order_id) as order_count\n      FROM customer_data c\n      JOIN sales_data s ON c.customer_id = s.customer_id\n      GROUP BY c.customer_id, c.name, c.region\n</code></pre>"},{"location":"guides/#attachments-only-configuration","title":"Attachments-only Configuration","text":"<p>For joining data from existing databases:</p> <pre><code>version: 1\n\nduckdb:\n  database: unified_catalog.duckdb\n\nattachments:\n  duckdb:\n    - alias: analytics\n      path: ./analytics_db.duckdb\n      read_only: true\n    - alias: warehouse\n      path: ./warehouse_db.duckdb\n      read_only: true\n\n  sqlite:\n    - alias: legacy\n      path: ./legacy_system.db\n\nviews:\n  - name: user_profiles\n    source: duckdb\n    database: analytics\n    table: users\n\n  - name: user_orders\n    source: duckdb\n    database: warehouse\n    table: orders\n\n  - name: legacy_customers\n    source: sqlite\n    database: legacy\n    table: customers\n\n  - name: complete_customer_view\n    sql: |\n      SELECT \n        p.user_id,\n        p.name,\n        p.email,\n        o.total_orders,\n        o.total_spent,\n        l.rating as legacy_rating\n      FROM user_profiles p\n      JOIN user_orders o ON p.user_id = o.user_id\n      LEFT JOIN legacy_customers l ON p.user_id = l.id\n</code></pre>"},{"location":"guides/#iceberg-only-configuration","title":"Iceberg-only Configuration","text":"<p>For working exclusively with Iceberg tables:</p> <pre><code>version: 1\n\nduckdb:\n  database: iceberg_catalog.duckdb\n\niceberg_catalogs:\n  - name: prod_catalog\n    catalog_type: rest\n    uri: \"https://iceberg-catalog.company.com\"\n    warehouse: \"s3://data-warehouse/production/\"\n    options:\n      token: \"${env:ICEBERG_PROD_TOKEN}\"\n\n  - name: staging_catalog\n    catalog_type: rest\n    uri: \"https://iceberg-staging.company.com\"\n    warehouse: \"s3://data-warehouse/staging/\"\n    options:\n      token: \"${env:ICEBERG_STAGING_TOKEN}\"\n\nviews:\n  - name: production_customers\n    source: iceberg\n    catalog: prod_catalog\n    table: analytics.customers\n\n  - name: staging_customers\n    source: iceberg\n    catalog: staging_catalog\n    table: analytics.customers\n\n  - name: customer_comparison\n    sql: |\n      SELECT \n        COALESCE(p.id, s.id) as customer_id,\n        p.name as prod_name,\n        s.name as staging_name,\n        p.updated_at as prod_updated,\n        s.updated_at as staging_updated\n      FROM production_customers p\n      FULL OUTER JOIN staging_customers s ON p.id = s.id\n</code></pre>"},{"location":"guides/#multi-source-configuration","title":"Multi-source Configuration","text":"<p>This example combines multiple data sources in a single catalog:</p> <pre><code>version: 1\n\nduckdb:\n  database: unified_analytics.duckdb\n  pragmas:\n    - \"SET memory_limit='4GB'\"\n    - \"SET threads=4\"\n    - \"SET s3_region='us-east-1'\"\n\nattachments:\n  duckdb:\n    - alias: reference\n      path: ./reference_data.duckdb\n      read_only: true\n\niceberg_catalogs:\n  - name: data_lake\n    catalog_type: rest\n    uri: \"https://iceberg.data-lake.internal\"\n    warehouse: \"s3://enterprise-data-lake/\"\n    options:\n      token: \"${env:ICEBERG_TOKEN}\"\n\nviews:\n  # Reference data from attached database\n  - name: user_segments\n    source: duckdb\n    database: reference\n    table: user_segments\n\n  # Raw events from S3\n  - name: raw_events\n    source: parquet\n    uri: \"s3://events-bucket/raw/*.parquet\"\n\n  # Processed events from Iceberg\n  - name: processed_events\n    source: iceberg\n    catalog: data_lake\n    table: analytics.processed_events\n\n  # Unified analytics view\n  - name: analytics_events\n    sql: |\n      SELECT \n        e.event_id,\n        e.timestamp,\n        e.user_id,\n        e.event_type,\n        e.properties,\n        us.segment_name as user_segment,\n        us.tier as user_tier\n      FROM raw_events e\n      LEFT JOIN user_segments us ON e.user_id = us.user_id\n      WHERE e.timestamp &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n</code></pre>"},{"location":"guides/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/#common-errors","title":"Common Errors","text":""},{"location":"guides/#1-invalid-configuration-syntax","title":"1. Invalid Configuration Syntax","text":"<p>Error message: <code>ConfigError: Invalid configuration file</code></p> <p>Solutions: - Validate YAML syntax first with <code>yamllint</code> or an online YAML validator - Ensure proper indentation (YAML is sensitive to spaces) - Check for unescaped special characters in strings</p>"},{"location":"guides/#2-missing-environment-variables","title":"2. Missing Environment Variables","text":"<p>Error message: <code>ConfigError: Environment variable 'AWS_ACCESS_KEY_ID' not found</code></p> <p>Solutions: - Check that all <code>env:VAR_NAME</code> variables are set - Use <code>duckalog validate config.yaml</code> to check environment variables without building - Consider using a <code>.env</code> file with <code>export</code> commands or a tool like <code>direnv</code></p>"},{"location":"guides/#3-connection-failures","title":"3. Connection Failures","text":"<p>Error message: <code>DuckDB Error: Invalid Input Error: Failed to open file</code></p> <p>Solutions: - Check file paths in attachments section - Ensure credentials for cloud storage are correct - Verify network connectivity and firewall rules - For local files, check file permissions</p>"},{"location":"guides/#4-sql-errors-in-views","title":"4. SQL Errors in Views","text":"<p>Error message: <code>Parser Error: syntax error at or near \"JOIN\"</code></p> <p>Solutions: - Validate SQL syntax with <code>duckalog generate-sql config.yaml</code> before building - Check that all referenced views and tables exist - Verify column names match across join conditions - Use proper DuckDB SQL syntax (similar to PostgreSQL)</p>"},{"location":"guides/#5-iceberg-catalog-issues","title":"5. Iceberg Catalog Issues","text":"<p>Error message: <code>Invalid Input Error: Can't load extension iceberg</code></p> <p>Solutions: - Ensure DuckDB version supports Iceberg extensions - Install required extensions: <code>duckdb.install_extension(\"iceberg\")</code> - Check catalog configuration and credentials - Verify Iceberg catalog server is accessible</p>"},{"location":"guides/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Validate before building: Always run <code>duckalog validate config.yaml</code> first</li> <li>Generate SQL first: Use <code>duckalog generate-sql config.yaml</code> to inspect generated SQL</li> <li>Start simple: Begin with a single view and gradually add complexity</li> <li>Use environment check: Create a simple script to verify required environment variables</li> <li>Check logs: Run with verbose logging to see detailed error information</li> </ol> <pre><code># Enable debug logging\nduckalog build config.yaml --log-level DEBUG\n</code></pre>"},{"location":"guides/#best-practices","title":"Best Practices","text":"<ol> <li>Separate configurations: Use different configs for different environments</li> <li>Version control: Keep your configs in Git alongside your code</li> <li>Modular views: Break complex SQL into multiple simpler views when possible</li> <li>Naming conventions: Use consistent naming for views and attachments</li> <li>Security: Never commit secrets to version control - use environment variables</li> </ol>"},{"location":"guides/#next-steps","title":"Next Steps","text":"<ul> <li>Use <code>duckalog generate-sql</code> to inspect the SQL that will be executed.</li> <li>Use the API reference for details on each public function and model.</li> <li>Check out the examples in the documentation for more advanced use cases.</li> </ul>"},{"location":"guides/path-resolution/","title":"Path Resolution Feature","text":"<p>Duckalog's path resolution feature automatically resolves relative file paths to absolute paths relative to the configuration file location, providing consistent behavior across different working directories while maintaining security and cross-platform compatibility.</p>"},{"location":"guides/path-resolution/#overview","title":"Overview","text":"<p>The path resolution feature addresses common challenges when working with file-based data sources and database attachments:</p> <ul> <li>Portability: Configurations can be moved between environments without breaking file references</li> <li>Consistency: Paths are resolved consistently regardless of the current working directory</li> <li>Security: Built-in validation prevents directory traversal attacks while allowing reasonable parent directory access</li> <li>Flexibility: Works with relative paths, absolute paths, and remote URIs</li> </ul>"},{"location":"guides/path-resolution/#how-it-works","title":"How It Works","text":""},{"location":"guides/path-resolution/#automatic-detection-and-resolution","title":"Automatic Detection and Resolution","text":"<p>When path resolution is enabled, Duckalog automatically:</p> <ol> <li>Detects whether a path is relative or absolute</li> <li>Resolves relative paths against the configuration file's directory</li> <li>Validates the resolved path for security concerns</li> <li>Updates the configuration with resolved absolute paths</li> </ol>"},{"location":"guides/path-resolution/#supported-path-types","title":"Supported Path Types","text":"Path Type Example Resolved Relative <code>data/file.parquet</code> <code>/project/config/data/file.parquet</code> Parent Directory <code>../shared/data.parquet</code> <code>/project/shared/data.parquet</code> Absolute <code>/absolute/path/file.parquet</code> Unchanged Windows <code>C:\\data\\file.parquet</code> Unchanged Remote URI <code>s3://bucket/file.parquet</code> Unchanged"},{"location":"guides/path-resolution/#security-validation","title":"Security Validation","text":"<p>Path resolution includes comprehensive security validation:</p> <ul> <li>Directory Traversal Protection: Blocks excessive parent directory navigation (<code>../../../etc/passwd</code>)</li> <li>Reasonable Traversal: Allows limited parent directory access for legitimate use cases</li> <li>Dangerous Pattern Detection: Prevents access to system directories (<code>/etc/</code>, <code>/usr/</code>, etc.)</li> <li>Cross-Platform Compatibility: Handles Windows and Unix path conventions</li> </ul>"},{"location":"guides/path-resolution/#configuration","title":"Configuration","text":""},{"location":"guides/path-resolution/#enabling-path-resolution","title":"Enabling Path Resolution","text":"<p>Path resolution is controlled by the <code>resolve_paths</code> parameter when loading configuration:</p> <pre><code>from duckalog import load_config\n\n# Enable path resolution (default)\nconfig = load_config(\"catalog.yaml\", resolve_paths=True)\n\n# Disable path resolution\nconfig = load_config(\"catalog.yaml\", resolve_paths=False)\n</code></pre>"},{"location":"guides/path-resolution/#command-line-interface","title":"Command Line Interface","text":"<p>The CLI automatically enables path resolution:</p> <pre><code># Path resolution enabled by default\nduckalog build catalog.yaml\n\n# Generate SQL with path resolution\nduckalog generate-sql catalog.yaml\n</code></pre>"},{"location":"guides/path-resolution/#usage-examples","title":"Usage Examples","text":""},{"location":"guides/path-resolution/#basic-relative-path-resolution","title":"Basic Relative Path Resolution","text":"<p>Project Structure: <pre><code>analytics/\n\u251c\u2500\u2500 catalog.yaml\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 events.parquet\n\u2502   \u2514\u2500\u2500 users.parquet\n\u2514\u2500\u2500 databases/\n    \u2514\u2500\u2500 reference.duckdb\n</code></pre></p> <p>Configuration (<code>catalog.yaml</code>): <pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n\nattachments:\n  duckdb:\n    - alias: refdata\n      path: ./databases/reference.duckdb\n      read_only: true\n\nviews:\n  - name: events\n    source: parquet\n    uri: data/events.parquet\n\n  - name: users\n    source: parquet\n    uri: ./data/users.parquet\n\n  - name: user_events\n    sql: |\n      SELECT \n        u.user_id,\n        u.name,\n        e.event_type,\n        e.timestamp\n      FROM users u\n      JOIN events e ON u.user_id = e.user_id\n</code></pre></p> <p>Result: All relative paths are resolved to absolute paths relative to <code>catalog.yaml</code>.</p>"},{"location":"guides/path-resolution/#parent-directory-access","title":"Parent Directory Access","text":"<p>Project Structure: <pre><code>company/\n\u251c\u2500\u2500 shared/\n\u2502   \u2514\u2500\u2500 reference_data/\n\u2502       \u2514\u2500\u2500 customers.parquet\n\u2514\u2500\u2500 analytics/\n    \u2514\u2500\u2500 catalog.yaml\n    \u2514\u2500\u2500 data/\n        \u2514\u2500\u2500 events.parquet\n</code></pre></p> <p>Configuration (<code>company/analytics/catalog.yaml</code>): <pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n\nviews:\n  - name: events\n    source: parquet\n    uri: ./data/events.parquet\n\n  - name: customers\n    source: parquet\n    uri: ../shared/reference_data/customers.parquet\n\n  - name: customer_events\n    sql: |\n      SELECT \n        c.customer_id,\n        c.name,\n        e.event_type,\n        e.timestamp\n      FROM customers c\n      JOIN events e ON c.customer_id = e.customer_id\n</code></pre></p> <p>Result:  - <code>./data/events.parquet</code> \u2192 <code>/company/analytics/data/events.parquet</code> - <code>../shared/reference_data/customers.parquet</code> \u2192 <code>/company/shared/reference_data/customers.parquet</code></p>"},{"location":"guides/path-resolution/#mixed-path-types","title":"Mixed Path Types","text":"<pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n\nviews:\n  # Local relative path - will be resolved\n  - name: local_data\n    source: parquet\n    uri: ./data/local.parquet\n\n  # Absolute path - unchanged\n  - name: absolute_data\n    source: parquet\n    uri: /absolute/path/data.parquet\n\n  # Remote URI - unchanged\n  - name: remote_data\n    source: parquet\n    uri: s3://my-bucket/data/remote.parquet\n\n  # Windows path - unchanged if on Windows\n  - name: windows_data\n    source: parquet\n    uri: D:\\data\\windows.parquet\n</code></pre>"},{"location":"guides/path-resolution/#attachment-path-resolution","title":"Attachment Path Resolution","text":"<pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n\nattachments:\n  duckdb:\n    - alias: ref_db\n      path: ./reference.databases/analytics.duckdb\n      read_only: true\n\n  sqlite:\n    - alias: legacy_db\n      path: ../legacy/production.db\n\nviews:\n  - name: ref_data\n    source: duckdb\n    database: ref_db\n    table: customers\n\n  - name: legacy_data\n    source: sqlite\n    database: legacy_db\n    table: users\n</code></pre>"},{"location":"guides/path-resolution/#security-features","title":"Security Features","text":""},{"location":"guides/path-resolution/#directory-traversal-protection","title":"Directory Traversal Protection","text":"<p>Path resolution automatically blocks dangerous path patterns:</p> <pre><code># \u274c BLOCKED - Excessive parent directory traversal\nviews:\n  - name: malicious\n    source: parquet\n    uri: ../../../../etc/passwd\n</code></pre> <p>Error: <code>Path resolution violates security rules</code></p>"},{"location":"guides/path-resolution/#dangerous-location-detection","title":"Dangerous Location Detection","text":"<p>Access to system directories is automatically blocked:</p> <pre><code># \u274c BLOCKED - Attempts to access system directories\nviews:\n  - name: system_config\n    source: parquet\n    uri: ../etc/config.parquet  # Resolves to /etc/config.parquet\n</code></pre>"},{"location":"guides/path-resolution/#reasonable-traversal-allowance","title":"Reasonable Traversal Allowance","text":"<p>Limited parent directory access is permitted for legitimate use cases:</p> <pre><code># \u2705 ALLOWED - Reasonable parent directory access\nviews:\n  - name: shared_data\n    source: parquet\n    uri: ../shared/data.parquet  # Allowed: 1 level up\n\n  - name: project_root_data\n    source: parquet\n    uri: ../../project_data/common.parquet  # Allowed: 2 levels up\n</code></pre>"},{"location":"guides/path-resolution/#migration-guide","title":"Migration Guide","text":""},{"location":"guides/path-resolution/#from-relative-paths","title":"From Relative Paths","text":"<p>Before (manual path management): <pre><code>version: 1\n\nviews:\n  - name: data\n    source: parquet\n    uri: /absolute/path/to/data.parquet  # Hard-coded absolute path\n</code></pre></p> <p>After (with path resolution): <pre><code>version: 1\n\nviews:\n  - name: data\n    source: parquet\n    uri: ./data.parquet  # Relative path - automatically resolved\n</code></pre></p>"},{"location":"guides/path-resolution/#migration-steps","title":"Migration Steps","text":"<ol> <li>Update Configuration Files: Replace absolute paths with relative paths where appropriate</li> <li>Test Resolution: Use <code>duckalog validate catalog.yaml</code> to ensure paths resolve correctly</li> <li>Enable Resolution: Ensure <code>resolve_paths=True</code> (default setting)</li> <li>Verify Results: Use <code>duckalog generate-sql catalog.yaml</code> to inspect resolved paths</li> </ol>"},{"location":"guides/path-resolution/#api-usage","title":"API Usage","text":""},{"location":"guides/path-resolution/#programmatic-path-resolution","title":"Programmatic Path Resolution","text":"<pre><code>from duckalog.path_resolution import (\n    resolve_relative_path,\n    is_relative_path,\n    validate_path_security\n)\nfrom pathlib import Path\n\n# Check if a path is relative\nis_rel = is_relative_path(\"data/file.parquet\")  # True\nis_rel = is_relative_path(\"/absolute/file.parquet\")  # False\n\n# Resolve a relative path\nconfig_dir = Path(\"/project/config\")\nresolved = resolve_relative_path(\"data/file.parquet\", config_dir)\n# Returns: \"/project/config/data/file.parquet\"\n\n# Validate path security\nis_safe = validate_path_security(\"data/file.parquet\", config_dir)\n# Returns: True\n</code></pre>"},{"location":"guides/path-resolution/#loading-configuration-with-path-resolution","title":"Loading Configuration with Path Resolution","text":"<pre><code>from duckalog import load_config, ConfigError\n\ntry:\n    # Load with path resolution enabled (default)\n    config = load_config(\"catalog.yaml\")\n\n    # Load with path resolution explicitly disabled\n    config = load_config(\"catalog.yaml\", resolve_paths=False)\n\nexcept ConfigError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"guides/path-resolution/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/path-resolution/#common-issues","title":"Common Issues","text":""},{"location":"guides/path-resolution/#path-resolution-failed","title":"Path Resolution Failed","text":"<p>Error: <code>ConfigError: Path resolution failed</code></p> <p>Solutions: 1. Check if the path exists and is accessible 2. Verify the path doesn't violate security rules 3. Ensure the configuration file path is correct 4. Use absolute paths for system-level files</p>"},{"location":"guides/path-resolution/#security-violation","title":"Security Violation","text":"<p>Error: <code>ValueError: Path resolution violates security rules</code></p> <p>Solutions: 1. Reduce the number of parent directory traversals (<code>../</code>) 2. Avoid system directories (<code>/etc/</code>, <code>/usr/</code>, etc.) 3. Use relative paths within reasonable bounds 4. Consider moving data files to a safer location</p>"},{"location":"guides/path-resolution/#file-not-found-after-resolution","title":"File Not Found After Resolution","text":"<p>Error: <code>DuckDB Error: Failed to open file</code></p> <p>Solutions: 1. Verify the resolved path points to an existing file 2. Check file permissions 3. Ensure the file is not a directory 4. Use <code>validate_file_accessibility()</code> to check before loading</p>"},{"location":"guides/path-resolution/#debugging-path-resolution","title":"Debugging Path Resolution","text":"<pre><code>from duckalog.path_resolution import (\n    is_relative_path,\n    resolve_relative_path,\n    detect_path_type\n)\nfrom pathlib import Path\n\n# Debug path detection\npath = \"data/file.parquet\"\nprint(f\"Path type: {detect_path_type(path)}\")  # \"relative\"\nprint(f\"Is relative: {is_relative_path(path)}\")  # True\n\n# Debug resolution\nconfig_dir = Path(\"/project/config\")\ntry:\n    resolved = resolve_relative_path(path, config_dir)\n    print(f\"Resolved path: {resolved}\")\nexcept ValueError as e:\n    print(f\"Resolution failed: {e}\")\n</code></pre>"},{"location":"guides/path-resolution/#best-practices","title":"Best Practices","text":""},{"location":"guides/path-resolution/#configuration-structure","title":"Configuration Structure","text":"<pre><code>project/\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 catalog.yaml\n\u2502   \u2514\u2500\u2500 catalog-dev.yaml\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2514\u2500\u2500 external/\n\u251c\u2500\u2500 databases/\n\u2502   \u251c\u2500\u2500 reference.duckdb\n\u2502   \u2514\u2500\u2500 cache.duckdb\n\u2514\u2500\u2500 sql/\n    \u251c\u2500\u2500 views/\n    \u2514\u2500\u2500 procedures/\n</code></pre> <p>Recommended Path Patterns: - Use <code>./data/</code> for project-specific data - Use <code>../shared/</code> for shared project resources - Use <code>./databases/</code> for local database files - Avoid deep parent directory navigation</p>"},{"location":"guides/path-resolution/#environment-specific-configurations","title":"Environment-Specific Configurations","text":"<pre><code># config/catalog.yaml\nversion: 1\n\nduckdb:\n  database: catalog.duckdb\n\nviews:\n  - name: local_data\n    source: parquet\n    uri: ./data/local.parquet  # Resolved relative to this config\n</code></pre>"},{"location":"guides/path-resolution/#testing-path-resolution","title":"Testing Path Resolution","text":"<pre><code># Test resolution before building\nfrom duckalog import load_config, generate_sql\n\ntry:\n    # Load and validate with resolution\n    config = load_config(\"catalog.yaml\")\n\n    # Generate SQL to see resolved paths\n    sql = generate_sql(\"catalog.yaml\")\n    print(\"Generated SQL with resolved paths:\")\n    print(sql)\n\nexcept Exception as e:\n    print(f\"Path resolution issue: {e}\")\n</code></pre>"},{"location":"guides/path-resolution/#performance-considerations","title":"Performance Considerations","text":"<p>Path resolution adds minimal overhead to configuration loading:</p> <ul> <li>CPU Impact: Negligible for typical configurations</li> <li>Memory Impact: Minimal additional memory usage</li> <li>I/O Impact: File system access only for path resolution</li> <li>Caching: Resolved paths are cached in configuration objects</li> </ul>"},{"location":"guides/path-resolution/#cross-platform-compatibility","title":"Cross-Platform Compatibility","text":""},{"location":"guides/path-resolution/#windows-vs-unix-paths","title":"Windows vs Unix Paths","text":"<p>Path resolution handles platform differences automatically:</p> <pre><code># Works on both Windows and Unix\nviews:\n  - name: cross_platform_data\n    source: parquet\n    uri: ./data/file.parquet\n</code></pre>"},{"location":"guides/path-resolution/#best-practices-for-cross-platform","title":"Best Practices for Cross-Platform","text":"<ol> <li>Use forward slashes in configuration files (works everywhere)</li> <li>Avoid platform-specific paths when possible</li> <li>Test on target platforms before deployment</li> <li>Use environment variables for platform-specific configurations</li> </ol> <p>Last updated: Duckalog v0.2.0</p>"},{"location":"guides/ui-dashboard/","title":"Duckalog Dashboard (starhtml/starui)","text":"<p>The dashboard is a lightweight, local-only web UI for exploring a Duckalog catalog. It is built entirely in Python using starhtml and starui\u2014no frontend build tools or external CDNs are required.</p>"},{"location":"guides/ui-dashboard/#launch-from-python-recommended","title":"Launch from Python (recommended)","text":"<pre><code>from duckalog.dashboard import run_dashboard\n\nrun_dashboard(\"catalog.yaml\", host=\"127.0.0.1\", port=8787, row_limit=500)\n</code></pre> <p>Pass a <code>Config</code> object instead of a path if you already loaded one.</p>"},{"location":"guides/ui-dashboard/#optional-launch-from-the-cli-if-enabled","title":"Optional: Launch from the CLI (if enabled)","text":"<p>Some installations may expose a <code>duckalog ui</code> command that wraps the same dashboard implementation:</p> <pre><code>duckalog ui catalog.yaml --host 127.0.0.1 --port 8787 --row-limit 500\n</code></pre> <ul> <li>Binds to loopback by default; only expose other hosts if you understand the risk.</li> <li><code>--row-limit</code> caps ad-hoc query results (defaults to 500 rows).</li> <li>The CLI prints the dashboard URL after startup.</li> <li>This command may not be available in all builds; if it is missing, use the Python API example above instead.</li> </ul>"},{"location":"guides/ui-dashboard/#what-you-can-do","title":"What you can do","text":"<ul> <li>Home: See config path, DuckDB database, counts (views/attachments/semantic models), and last build status. Trigger a build.</li> <li>Views: Browse views with source type and location/attachment info; search by name.</li> <li>View detail: Inspect the view definition (SQL or source fields) and any semantic-layer dimensions/measures.</li> <li>Query: Run ad-hoc SQL against the catalog with row-limit enforcement and clear error display.</li> <li>Build: Kick off a catalog build with the same semantics as <code>duckalog build</code>; status is shown on the home page.</li> </ul>"},{"location":"guides/ui-dashboard/#scope-and-limitations","title":"Scope and limitations","text":"<ul> <li>Single-user, local-first; no authentication is provided.</li> <li>No external assets or CDNs; everything is served from the duckalog installation.</li> <li>Focused on tables and text\u2014charts/advanced visuals are out of scope for this version.</li> </ul>"},{"location":"guides/usage/","title":"User Guide","text":"<p>This guide explains how to structure Duckalog configuration files, common configuration patterns, and how to troubleshoot issues.</p>"},{"location":"guides/usage/#configuration-structure","title":"Configuration structure","text":"<p>At a high level, a config looks like this:</p> <pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='1GB'\"\n\nattachments:\n  duckdb:\n    - alias: refdata\n      path: ./refdata.duckdb\n      read_only: true\n\n  sqlite:\n    - alias: legacy\n      path: ./legacy.db\n\n  postgres:\n    - alias: dw\n      host: \"${env:PG_HOST}\"\n      port: 5432\n      database: dw\n      user: \"${env:PG_USER}\"\n      password: \"${env:PG_PASSWORD}\"\n\niceberg_catalogs:\n  - name: main_ic\n    catalog_type: rest\n    uri: \"https://iceberg-catalog.internal\"\n    warehouse: \"s3://my-warehouse/\"\n    options:\n      token: \"${env:ICEBERG_TOKEN}\"\n\nviews:\n  - name: users\n    source: parquet\n    uri: \"s3://my-bucket/data/users/*.parquet\"\n\n  - name: events_delta\n    source: delta\n    uri: \"s3://my-bucket/delta/events\"\n\n  - name: ic_orders\n    source: iceberg\n    catalog: main_ic\n    table: analytics.orders\n\n  - name: ref_countries\n    source: duckdb\n    database: refdata\n    table: reference.countries\n\n  - name: vip_users\n    sql: |\n      SELECT *\n      FROM users\n      WHERE is_vip = TRUE\n</code></pre>"},{"location":"guides/usage/#path-resolution","title":"Path Resolution","text":"<p>Duckalog automatically resolves relative file paths to absolute paths relative to the configuration file location. This ensures consistent behavior across different working directories while maintaining security.</p>"},{"location":"guides/usage/#how-path-resolution-works","title":"How Path Resolution Works","text":"<ul> <li>Automatic Detection: Duckalog detects whether paths are relative or absolute</li> <li>Relative Path Resolution: Resolves paths like <code>data/file.parquet</code> against the config file's directory  </li> <li>Security Validation: Blocks dangerous directory traversal while allowing reasonable parent directory access</li> <li>Cross-Platform Support: Works correctly on Windows, macOS, and Linux</li> </ul>"},{"location":"guides/usage/#configuration","title":"Configuration","text":"<p>Path resolution is enabled by default when loading configurations:</p> <pre><code>from duckalog import load_config\n\n# Path resolution enabled (default)\nconfig = load_config(\"catalog.yaml\")\n\n# Explicitly enable/disable resolution\nconfig = load_config(\"catalog.yaml\", resolve_paths=True)\nconfig = load_config(\"catalog.yaml\", resolve_paths=False)\n</code></pre>"},{"location":"guides/usage/#path-resolution-examples","title":"Path Resolution Examples","text":""},{"location":"guides/usage/#basic-relative-paths","title":"Basic Relative Paths","text":"<p>Project Structure: <pre><code>analytics/\n\u251c\u2500\u2500 catalog.yaml\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 events.parquet\n\u2502   \u2514\u2500\u2500 users.parquet\n\u2514\u2500\u2500 databases/\n    \u2514\u2500\u2500 reference.duckdb\n</code></pre></p> <p>Configuration: <pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n\nattachments:\n  duckdb:\n    - alias: refdata\n      path: ./databases/reference.duckdb  # Resolved to absolute path\n      read_only: true\n\nviews:\n  - name: events\n    source: parquet\n    uri: data/events.parquet  # Resolved relative to catalog.yaml\n\n  - name: users  \n    source: parquet\n    uri: ./data/users.parquet  # Explicit relative path\n</code></pre></p>"},{"location":"guides/usage/#parent-directory-access","title":"Parent Directory Access","text":"<p>Project Structure: <pre><code>company/\n\u251c\u2500\u2500 shared/\n\u2502   \u2514\u2500\u2500 reference_data/\n\u2502       \u2514\u2500\u2500 customers.parquet\n\u2514\u2500\u2500 analytics/\n    \u2514\u2500\u2500 catalog.yaml\n    \u2514\u2500\u2500 data/\n        \u2514\u2500\u2500 events.parquet\n</code></pre></p> <p>Configuration (<code>company/analytics/catalog.yaml</code>): <pre><code>version: 1\n\nviews:\n  - name: events\n    source: parquet\n    uri: ./data/events.parquet           # Resolved to /company/analytics/data/events.parquet\n\n  - name: customers\n    source: parquet\n    uri: ../shared/reference_data/customers.parquet  # Resolved to /company/shared/reference_data/customers.parquet\n</code></pre></p>"},{"location":"guides/usage/#mixed-path-types","title":"Mixed Path Types","text":"<pre><code>version: 1\n\nviews:\n  # Relative path - resolved automatically\n  - name: local_data\n    source: parquet\n    uri: ./data/local.parquet\n\n  # Absolute path - unchanged\n  - name: absolute_data\n    source: parquet\n    uri: /absolute/path/data.parquet\n\n  # Remote URI - unchanged\n  - name: remote_data\n    source: parquet\n    uri: s3://my-bucket/data/remote.parquet\n</code></pre>"},{"location":"guides/usage/#security-features","title":"Security Features","text":"<p>Path resolution includes comprehensive security validation:</p>"},{"location":"guides/usage/#allowed-patterns","title":"Allowed Patterns","text":"<pre><code># \u2705 ALLOWED - Same directory\nuri: data/file.parquet\n\n# \u2705 ALLOWED - Parent directory access (reasonable levels)\nuri: ../shared/data.parquet\nuri: ../../project/common.parquet\n\n# \u2705 ALLOWED - Subdirectories\nuri: ./subdir/nested/file.parquet\n</code></pre>"},{"location":"guides/usage/#blocked-patterns","title":"Blocked Patterns","text":"<pre><code># \u274c BLOCKED - Excessive parent directory traversal\nuri: ../../../../etc/passwd\n\n# \u274c BLOCKED - System directory access\nuri: ../etc/config.parquet\nuri: ../../usr/local/data.parquet\n</code></pre>"},{"location":"guides/usage/#troubleshooting-path-resolution","title":"Troubleshooting Path Resolution","text":""},{"location":"guides/usage/#common-issues","title":"Common Issues","text":"<p>Path resolution failed: <pre><code>PathResolutionError: Path resolution failed: Path resolution violates security rules\n</code></pre></p> <p>Solutions: - Reduce the number of parent directory traversals (<code>../</code>) - Avoid system directories (<code>/etc/</code>, <code>/usr/</code>, etc.) - Use relative paths within reasonable bounds</p> <p>File not found after resolution: <pre><code>DuckDB Error: Failed to open file\n</code></pre></p> <p>Solutions: - Verify the resolved path points to an existing file - Check file permissions - Ensure the file is not a directory</p>"},{"location":"guides/usage/#debugging","title":"Debugging","text":"<pre><code>from duckalog import generate_sql\n\n# Generate SQL to see resolved paths\nsql = generate_sql(\"catalog.yaml\")\nprint(sql)  # Shows absolute paths after resolution\n</code></pre> <p>For complete details, see the Path Resolution Guide.</p>"},{"location":"guides/usage/#environment-variables","title":"Environment variables","text":"<p>Any string may contain <code>${env:VAR_NAME}</code> placeholders. Duckalog resolves these using the process environment before validation. If a variable is missing, a <code>ConfigError</code> is raised.</p> <p>Example:</p> <pre><code>duckdb:\n  pragmas:\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n</code></pre>"},{"location":"guides/usage/#attachments","title":"Attachments","text":"<p>Attachments let you expose tables from other databases inside DuckDB.</p> <ul> <li>DuckDB attachments: attach additional <code>.duckdb</code> files.</li> <li>SQLite attachments: attach local SQLite databases.</li> <li>Postgres attachments: connect to external Postgres instances.</li> </ul> <p>Views that use attached databases set <code>source</code> to <code>duckdb</code>, <code>sqlite</code>, or <code>postgres</code> and provide <code>database</code> (attachment alias) and <code>table</code>.</p>"},{"location":"guides/usage/#iceberg-catalogs","title":"Iceberg catalogs","text":"<p>Iceberg catalogs are configured under <code>iceberg_catalogs</code>. Iceberg views can either:</p> <ul> <li>Use a <code>uri</code> directly, or</li> <li>Refer to a <code>catalog</code> + <code>table</code> combination.</li> </ul> <p>Duckalog validates that any <code>catalog</code> used by a view is defined in <code>iceberg_catalogs</code>.</p>"},{"location":"guides/usage/#common-configuration-patterns","title":"Common Configuration Patterns","text":""},{"location":"guides/usage/#parquet-only-configuration","title":"Parquet-only configuration","text":"<p>For simple cases where you only need to create views over Parquet files:</p> <pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='1GB'\"\n    - \"SET s3_region='us-west-2'\"\n\nviews:\n  - name: sales_data\n    source: parquet\n    uri: \"s3://data-bucket/sales/*.parquet\"\n\n  - name: customer_data\n    source: parquet\n    uri: \"s3://data-bucket/customers/*.parquet\"\n\n  - name: sales_by_customer\n    sql: |\n      SELECT \n        c.customer_id,\n        c.name,\n        c.region,\n        SUM(s.amount) as total_sales,\n        COUNT(s.order_id) as order_count\n      FROM customer_data c\n      JOIN sales_data s ON c.customer_id = s.customer_id\n      GROUP BY c.customer_id, c.name, c.region\n</code></pre>"},{"location":"guides/usage/#attachments-only-configuration","title":"Attachments-only configuration","text":"<p>For joining data from existing databases:</p> <pre><code>version: 1\n\nduckdb:\n  database: unified_catalog.duckdb\n\nattachments:\n  duckdb:\n    - alias: analytics\n      path: ./analytics_db.duckdb\n      read_only: true\n    - alias: warehouse\n      path: ./warehouse_db.duckdb\n      read_only: true\n\n  sqlite:\n    - alias: legacy\n      path: ./legacy_system.db\n\nviews:\n  - name: user_profiles\n    source: duckdb\n    database: analytics\n    table: users\n\n  - name: user_orders\n    source: duckdb\n    database: warehouse\n    table: orders\n\n  - name: legacy_customers\n    source: sqlite\n    database: legacy\n    table: customers\n\n  - name: complete_customer_view\n    sql: |\n      SELECT \n        p.user_id,\n        p.name,\n        p.email,\n        o.total_orders,\n        o.total_spent,\n        l.rating as legacy_rating\n      FROM user_profiles p\n      JOIN user_orders o ON p.user_id = o.user_id\n      LEFT JOIN legacy_customers l ON p.user_id = l.id\n</code></pre>"},{"location":"guides/usage/#iceberg-only-configuration","title":"Iceberg-only configuration","text":"<p>For working exclusively with Iceberg tables:</p> <pre><code>version: 1\n\nduckdb:\n  database: iceberg_catalog.duckdb\n\niceberg_catalogs:\n  - name: prod_catalog\n    catalog_type: rest\n    uri: \"https://iceberg-catalog.company.com\"\n    warehouse: \"s3://data-warehouse/production/\"\n    options:\n      token: \"${env:ICEBERG_PROD_TOKEN}\"\n\n  - name: staging_catalog\n    catalog_type: rest\n    uri: \"https://iceberg-staging.company.com\"\n    warehouse: \"s3://data-warehouse/staging/\"\n    options:\n      token: \"${env:ICEBERG_STAGING_TOKEN}\"\n\nviews:\n  - name: production_customers\n    source: iceberg\n    catalog: prod_catalog\n    table: analytics.customers\n\n  - name: staging_customers\n    source: iceberg\n    catalog: staging_catalog\n    table: analytics.customers\n\n  - name: customer_comparison\n    sql: |\n      SELECT \n        COALESCE(p.id, s.id) as customer_id,\n        p.name as prod_name,\n        s.name as staging_name,\n        p.updated_at as prod_updated,\n        s.updated_at as staging_updated\n      FROM production_customers p\n      FULL OUTER JOIN staging_customers s ON p.id = s.id\n</code></pre>"},{"location":"guides/usage/#multi-source-configuration","title":"Multi-source configuration","text":"<p>This example combines multiple data sources in a single catalog:</p> <pre><code>version: 1\n\nduckdb:\n  database: unified_analytics.duckdb\n  pragmas:\n    - \"SET memory_limit='4GB'\"\n    - \"SET threads=4\"\n    - \"SET s3_region='us-east-1'\"\n\nattachments:\n  duckdb:\n    - alias: reference\n      path: ./reference_data.duckdb\n      read_only: true\n\niceberg_catalogs:\n  - name: data_lake\n    catalog_type: rest\n    uri: \"https://iceberg.data-lake.internal\"\n    warehouse: \"s3://enterprise-data-lake/\"\n    options:\n      token: \"${env:ICEBERG_TOKEN}\"\n\nviews:\n  # Reference data from attached database\n  - name: user_segments\n    source: duckdb\n    database: reference\n    table: user_segments\n\n  # Raw events from S3\n  - name: raw_events\n    source: parquet\n    uri: \"s3://events-bucket/raw/*.parquet\"\n\n  # Processed events from Iceberg\n  - name: processed_events\n    source: iceberg\n    catalog: data_lake\n    table: analytics.processed_events\n\n  # Unified analytics view\n  - name: analytics_events\n    sql: |\n      SELECT \n        e.event_id,\n        e.timestamp,\n        e.user_id,\n        e.event_type,\n        e.properties,\n        us.segment_name as user_segment,\n        us.tier as user_tier\n      FROM raw_events e\n      LEFT JOIN user_segments us ON e.user_id = us.user_id\n      WHERE e.timestamp &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n</code></pre>"},{"location":"guides/usage/#error-handling","title":"Error Handling","text":"<p>Duckalog provides a comprehensive exception hierarchy to help you handle errors gracefully in your applications.</p>"},{"location":"guides/usage/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>All Duckalog exceptions inherit from <code>DuckalogError</code>, making it easy to catch all library errors:</p> <pre><code>from duckalog import DuckalogError, load_config, build_catalog\n\ntry:\n    config = load_config(\"catalog.yaml\")\n    build_catalog(config)\nexcept DuckalogError as e:\n    # Handle any Duckalog-specific error\n    print(f\"Duckalog error: {e}\")\n</code></pre>"},{"location":"guides/usage/#specific-exception-types","title":"Specific Exception Types","text":"<p>For more targeted error handling, catch specific exception types:</p> <pre><code>from duckalog import (\n    ConfigError,        # Configuration issues\n    EngineError,        # Database/build failures  \n    PathResolutionError, # Path resolution problems\n    RemoteConfigError,  # Remote config loading failures\n    SQLFileError,       # SQL file processing issues\n)\n\ntry:\n    config = load_config(\"catalog.yaml\")\nexcept ConfigError as e:\n    print(f\"Configuration error: {e}\")\nexcept PathResolutionError as e:\n    print(f\"Path resolution failed: {e}\")\n    print(f\"Original path: {e.original_path}\")\n    print(f\"Resolved path: {e.resolved_path}\")\nexcept DuckalogError as e:\n    print(f\"Other Duckalog error: {e}\")\n</code></pre>"},{"location":"guides/usage/#best-practices","title":"Best Practices","text":"<ol> <li>Catch specific exceptions first: Handle the most specific errors first, then catch more general ones</li> <li>Use exception chaining: Duckalog preserves original exceptions to help with debugging</li> <li>Log with context: Include relevant information when logging errors</li> <li>Validate early: Use <code>validate_config()</code> to catch configuration errors before building</li> </ol> <pre><code>import logging\nfrom duckalog import ConfigError, EngineError, validate_config, build_catalog\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef create_catalog(config_path):\n    try:\n        # Validate configuration first\n        validate_config(config_path)\n        logger.info(f\"Configuration {config_path} is valid\")\n\n        # Build catalog\n        build_catalog(config_path)\n        logger.info(\"Catalog built successfully\")\n\n    except ConfigError as e:\n        logger.error(f\"Configuration error in {config_path}: {e}\")\n        # Handle configuration issues (missing files, invalid syntax, etc.)\n    except EngineError as e:\n        logger.error(f\"Engine error building catalog: {e}\")\n        # Handle database issues (connection failures, SQL errors, etc.)\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n        raise  # Re-raise unexpected errors\n</code></pre>"},{"location":"guides/usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/usage/#common-errors","title":"Common errors","text":""},{"location":"guides/usage/#1-invalid-configuration-syntax","title":"1. Invalid configuration syntax","text":"<p>Error message: <code>ConfigError: Invalid configuration file</code></p> <p>Solutions: - Validate YAML syntax first with <code>yamllint</code> or an online YAML validator - Ensure proper indentation (YAML is sensitive to spaces) - Check for unescaped special characters in strings</p>"},{"location":"guides/usage/#2-missing-environment-variables","title":"2. Missing environment variables","text":"<p>Error message: <code>ConfigError: Environment variable 'AWS_ACCESS_KEY_ID' not found</code></p> <p>Solutions: - Check that all <code>env:VAR_NAME</code> variables are set - Use <code>duckalog validate config.yaml</code> to check environment variables without building - Consider using a <code>.env</code> file with <code>export</code> commands or a tool like <code>direnv</code></p>"},{"location":"guides/usage/#3-connection-failures","title":"3. Connection failures","text":"<p>Error message: <code>DuckDB Error: Invalid Input Error: Failed to open file</code></p> <p>Solutions: - Check file paths in attachments section - Ensure credentials for cloud storage are correct - Verify network connectivity and firewall rules - For local files, check file permissions</p>"},{"location":"guides/usage/#4-sql-errors-in-views","title":"4. SQL errors in views","text":"<p>Error message: <code>Parser Error: syntax error at or near \"JOIN\"</code></p> <p>Solutions: - Validate SQL syntax with <code>duckalog generate-sql config.yaml</code> before building - Check that all referenced views and tables exist - Verify column names match across join conditions - Use proper DuckDB SQL syntax (similar to PostgreSQL)</p>"},{"location":"guides/usage/#5-iceberg-catalog-issues","title":"5. Iceberg catalog issues","text":"<p>Error message: <code>Invalid Input Error: Can't load extension iceberg</code></p> <p>Solutions: - Ensure DuckDB version supports Iceberg extensions - Install required extensions: <code>duckdb.install_extension(\"iceberg\")</code> - Check catalog configuration and credentials - Verify Iceberg catalog server is accessible</p>"},{"location":"guides/usage/#debugging-tips","title":"Debugging tips","text":"<ol> <li>Validate before building: Always run <code>duckalog validate config.yaml</code> first</li> <li>Generate SQL first: Use <code>duckalog generate-sql config.yaml</code> to inspect generated SQL</li> <li>Start simple: Begin with a single view and gradually add complexity</li> <li>Use environment check: Create a simple script to verify required environment variables</li> <li>Check logs: Run with verbose logging to see detailed error information</li> </ol> <pre><code># Enable debug logging\nduckalog build config.yaml --log-level DEBUG\n</code></pre>"},{"location":"guides/usage/#best-practices_1","title":"Best Practices","text":"<ol> <li>Separate configurations: Use different configs for different environments</li> <li>Version control: Keep your configs in Git alongside your code</li> <li>Modular views: Break complex SQL into multiple simpler views when possible</li> <li>Naming conventions: Use consistent naming for views and attachments</li> <li>Security: Never commit secrets to version control - use environment variables</li> </ol>"},{"location":"guides/usage/#next-steps","title":"Next steps","text":"<ul> <li>Use <code>duckalog generate-sql</code> to inspect the SQL that will be executed.</li> <li>Use the API reference for details on each public function and model.</li> <li>Check out the examples in the documentation for more advanced use cases.</li> </ul>"},{"location":"reference/","title":"API Reference","text":"<p>This section provides comprehensive API documentation for the Duckalog library, generated from the source code using mkdocstrings.</p>"},{"location":"reference/#core-api","title":"Core API","text":""},{"location":"reference/#duckalog","title":"<code>duckalog</code>","text":"<p>Duckalog public API.</p>"},{"location":"reference/#duckalog.AttachmentsConfig","title":"<code>AttachmentsConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Collection of attachment configurations.</p> <p>Attributes:</p> Name Type Description <code>duckdb</code> <code>list[DuckDBAttachment]</code> <p>DuckDB attachment entries.</p> <code>sqlite</code> <code>list[SQLiteAttachment]</code> <p>SQLite attachment entries.</p> <code>postgres</code> <code>list[PostgresAttachment]</code> <p>Postgres attachment entries.</p> <code>duckalog</code> <code>list[DuckalogAttachment]</code> <p>Duckalog config attachment entries.</p>"},{"location":"reference/#duckalog.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level Duckalog configuration.</p> <p>Attributes:</p> Name Type Description <code>version</code> <code>int</code> <p>Positive integer describing the config schema version.</p> <code>duckdb</code> <code>DuckDBConfig</code> <p>DuckDB session and connection settings.</p> <code>views</code> <code>list[ViewConfig]</code> <p>List of view definitions to create in the catalog.</p> <code>attachments</code> <code>AttachmentsConfig</code> <p>Optional attachments to external databases.</p> <code>iceberg_catalogs</code> <code>list[IcebergCatalogConfig]</code> <p>Optional Iceberg catalog definitions.</p> <code>semantic_models</code> <code>list[SemanticModelConfig]</code> <p>Optional semantic model definitions for business metadata.</p>"},{"location":"reference/#duckalog.ConfigError","title":"<code>ConfigError</code>","text":"<p>               Bases: <code>DuckalogError</code></p> <p>Configuration-related errors.</p> <p>This exception is raised when a catalog configuration cannot be read, parsed, interpolated, or validated according to the Duckalog schema.</p> <p>Typical error conditions include:</p> <ul> <li>The config file does not exist or cannot be read.</li> <li>The file is not valid YAML/JSON.</li> <li>Required fields are missing or invalid.</li> <li>An environment variable placeholder cannot be resolved.</li> </ul>"},{"location":"reference/#duckalog.DuckDBAttachment","title":"<code>DuckDBAttachment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for attaching another DuckDB database.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>Alias under which the database will be attached.</p> <code>path</code> <code>str</code> <p>Filesystem path to the DuckDB database file.</p> <code>read_only</code> <code>bool</code> <p>Whether the attachment should be opened in read-only mode. Defaults to <code>True</code> for safety.</p>"},{"location":"reference/#duckalog.DuckDBConfig","title":"<code>DuckDBConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DuckDB connection and session settings.</p> <p>Attributes:</p> Name Type Description <code>database</code> <code>str</code> <p>Path to the DuckDB database file. Defaults to <code>\":memory:\"</code>.</p> <code>install_extensions</code> <code>list[str]</code> <p>Names of extensions to install before use.</p> <code>load_extensions</code> <code>list[str]</code> <p>Names of extensions to load in the session.</p> <code>pragmas</code> <code>list[str]</code> <p>SQL statements (typically <code>SET</code> pragmas) executed after connecting and loading extensions.</p> <code>settings</code> <code>Optional[Literal[str, list[str]]]</code> <p>DuckDB SET statements executed after pragmas. Can be a single string or list of strings.</p> <code>secrets</code> <code>list[SecretConfig]</code> <p>List of secret definitions for external services and databases.</p>"},{"location":"reference/#duckalog.EngineError","title":"<code>EngineError</code>","text":"<p>               Bases: <code>DuckalogError</code></p> <p>Engine-level error raised during catalog builds.</p> <p>This exception wraps lower-level DuckDB errors, such as failures to connect to the database, attach external systems, or execute generated SQL statements.</p>"},{"location":"reference/#duckalog.IcebergCatalogConfig","title":"<code>IcebergCatalogConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for an Iceberg catalog.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Catalog name referenced by Iceberg views.</p> <code>catalog_type</code> <code>str</code> <p>Backend type (for example, <code>rest</code>, <code>hive</code>, <code>glue</code>).</p> <code>uri</code> <code>Optional[str]</code> <p>Optional URI used by certain catalog types.</p> <code>warehouse</code> <code>Optional[str]</code> <p>Optional warehouse location for catalog data.</p> <code>options</code> <code>dict[str, Any]</code> <p>Additional catalog-specific options.</p>"},{"location":"reference/#duckalog.PostgresAttachment","title":"<code>PostgresAttachment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for attaching a Postgres database.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>Alias used inside DuckDB to reference the Postgres database.</p> <code>host</code> <code>str</code> <p>Hostname or IP address of the Postgres server.</p> <code>port</code> <code>int</code> <p>TCP port of the Postgres server.</p> <code>database</code> <code>str</code> <p>Database name to connect to.</p> <code>user</code> <code>str</code> <p>Username for authentication.</p> <code>password</code> <code>str</code> <p>Password for authentication.</p> <code>sslmode</code> <code>Optional[str]</code> <p>Optional SSL mode (for example, <code>require</code>).</p> <code>options</code> <code>dict[str, Any]</code> <p>Extra key/value options passed to the attachment clause.</p>"},{"location":"reference/#duckalog.SQLiteAttachment","title":"<code>SQLiteAttachment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for attaching a SQLite database.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>Alias under which the SQLite database will be attached.</p> <code>path</code> <code>str</code> <p>Filesystem path to the SQLite <code>.db</code> file.</p>"},{"location":"reference/#duckalog.SecretConfig","title":"<code>SecretConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a DuckDB secret.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>SecretType</code> <p>Secret type (s3, azure, gcs, http, postgres, mysql).</p> <code>name</code> <code>Optional[str]</code> <p>Optional name for the secret (defaults to type if not provided).</p> <code>provider</code> <code>SecretProvider</code> <p>Secret provider (config or credential_chain).</p> <code>persistent</code> <code>bool</code> <p>Whether to create a persistent secret. Defaults to False.</p> <code>scope</code> <code>Optional[str]</code> <p>Optional scope prefix for the secret.</p> <code>key_id</code> <code>Optional[str]</code> <p>Access key ID or username for authentication.</p> <code>secret</code> <code>Optional[str]</code> <p>Secret key or password for authentication.</p> <code>region</code> <code>Optional[str]</code> <p>Geographic region for cloud services.</p> <code>endpoint</code> <code>Optional[str]</code> <p>Custom endpoint URL for cloud services.</p> <code>connection_string</code> <code>Optional[str]</code> <p>Full connection string for databases.</p> <code>tenant_id</code> <code>Optional[str]</code> <p>Azure tenant ID for authentication.</p> <code>account_name</code> <code>Optional[str]</code> <p>Azure storage account name.</p> <code>client_id</code> <code>Optional[str]</code> <p>Azure client ID for authentication.</p> <code>client_secret</code> <code>Optional[str]</code> <p>Azure client secret for authentication.</p> <code>service_account_key</code> <code>Optional[str]</code> <p>GCS service account key.</p> <code>json_key</code> <code>Optional[str]</code> <p>GCS JSON key.</p> <code>bearer_token</code> <code>Optional[str]</code> <p>HTTP bearer token for authentication.</p> <code>header</code> <code>Optional[str]</code> <p>HTTP header for authentication.</p> <code>database</code> <code>Optional[str]</code> <p>Database name for database secrets.</p> <code>host</code> <code>Optional[str]</code> <p>Database host for database secrets.</p> <code>port</code> <code>Optional[int]</code> <p>Database port for database secrets.</p> <code>user</code> <code>Optional[str]</code> <p>Database username (alternative to key_id for database types).</p> <code>password</code> <code>Optional[str]</code> <p>Database password (alternative to secret for database types).</p> <code>options</code> <code>dict[str, Any]</code> <p>Additional key-value options for the secret.</p>"},{"location":"reference/#duckalog.ViewConfig","title":"<code>ViewConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Definition of a single catalog view.</p> <p>A view can be defined in several ways: 1. Inline SQL: Using the <code>sql</code> field with raw SQL text 2. SQL File: Using <code>sql_file</code> to reference external SQL files 3. SQL Template: Using <code>sql_template</code> for parameterized SQL files 4. Data Source: Using <code>source</code> + required fields for direct data access 5. Source + SQL: Using <code>source</code> for data access plus <code>sql</code> for transformations</p> <p>For data sources, the required fields depend on the source type: - Parquet/Delta: <code>uri</code> field is required - Iceberg: Either <code>uri</code> OR both <code>catalog</code> and <code>table</code> - DuckDB/SQLite/Postgres: Both <code>database</code> and <code>table</code> are required</p> <p>When using SQL with a data source, the SQL will be applied as a transformation over the data from the specified source.</p> <p>Additional metadata fields such as <code>description</code> and <code>tags</code> do not affect SQL generation but are preserved for documentation and tooling.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique view name within the config.</p> <code>sql</code> <code>Optional[str]</code> <p>Raw SQL text defining the view body.</p> <code>sql_file</code> <code>Optional[SQLFileReference]</code> <p>Direct reference to a SQL file.</p> <code>sql_template</code> <code>Optional[SQLFileReference]</code> <p>Reference to a SQL template file with variable substitution.</p> <code>source</code> <code>Optional[EnvSource]</code> <p>Source type (e.g. <code>\"parquet\"</code>, <code>\"iceberg\"</code>, <code>\"duckdb\"</code>).</p> <code>uri</code> <code>Optional[str]</code> <p>URI for file- or table-based sources (Parquet/Delta/Iceberg).</p> <code>database</code> <code>Optional[str]</code> <p>Attachment alias for attached-database sources.</p> <code>table</code> <code>Optional[str]</code> <p>Table name (optionally schema-qualified) for attached sources.</p> <code>catalog</code> <code>Optional[str]</code> <p>Iceberg catalog name for catalog-based Iceberg views.</p> <code>options</code> <code>dict[str, Any]</code> <p>Source-specific options passed to scan functions.</p> <code>description</code> <code>Optional[str]</code> <p>Optional human-readable description of the view.</p> <code>tags</code> <code>list[str]</code> <p>Optional list of tags for classification.</p>"},{"location":"reference/#duckalog.build_catalog","title":"<code>build_catalog(config_path, db_path=None, dry_run=False, verbose=False, filesystem=None, include_secrets=True)</code>","text":"<p>Build or update a DuckDB catalog from a configuration file.</p> <p>This function is the high-level entry point used by both the CLI and Python API. It loads the config, optionally performs a dry-run SQL generation, or otherwise connects to DuckDB, sets up attachments and Iceberg catalogs, and creates or replaces configured views.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <code>db_path</code> <code>str | None</code> <p>Optional override for <code>duckdb.database</code> in the config. Can be a local path or remote URI (s3://, gs://, gcs://, abfs://, adl://, sftp://).</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If <code>True</code>, do not connect to DuckDB; instead generate and return the full SQL script for all views.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If <code>True</code>, enable more verbose logging via the standard logging module.</p> <code>False</code> <code>filesystem</code> <code>Any | None</code> <p>Optional pre-configured fsspec filesystem object for remote export authentication. If not provided, default authentication will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | None</code> <p>The generated SQL script as a string when <code>dry_run</code> is <code>True</code>,</p> <code>str | None</code> <p>otherwise <code>None</code> when the catalog is applied to DuckDB.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> <code>EngineError</code> <p>If connecting to DuckDB or executing SQL fails, or if remote export fails.</p> Example <p>Build a catalog in-place::</p> <pre><code>from duckalog import build_catalog\n\nbuild_catalog(\"catalog.yaml\")\n</code></pre> <p>Build and export to remote storage::</p> <pre><code>build_catalog(\"catalog.yaml\", db_path=\"s3://my-bucket/catalog.duckdb\")\n</code></pre> <p>Generate SQL without modifying the database::</p> <pre><code>sql = build_catalog(\"catalog.yaml\", dry_run=True)\nprint(sql)\n</code></pre>"},{"location":"reference/#duckalog.connect_and_build_catalog","title":"<code>connect_and_build_catalog(config_path, database_path=None, dry_run=False, verbose=False, read_only=False, **kwargs)</code>","text":"<p>Build a catalog and create a DuckDB connection in one operation.</p> <p>This function combines catalog building with connection creation, providing a streamlined workflow for users who want to start working with their catalog immediately after creating it.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <code>database_path</code> <code>str | None</code> <p>Optional database path override.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, only validates configuration and returns SQL. If False, builds the catalog and creates a connection.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging during build process.</p> <code>False</code> <code>read_only</code> <code>bool</code> <p>Open the resulting connection in read-only mode.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DuckDBPyConnection | str | None</code> <p>A DuckDB connection object for immediate use, or SQL string when dry_run=True.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> <code>EngineError</code> <p>If catalog building or connection fails.</p> <code>FileNotFoundError</code> <p>If the resulting database file doesn't exist (after build).</p> Example <p>Build catalog and start querying immediately::</p> <pre><code>conn = connect_and_build_catalog(\"catalog.yaml\")\ndata = conn.execute(\"SELECT * FROM important_table\").fetchall()\nprint(f\"Found {len(data)} records\")\nconn.close()\n</code></pre> <p>Validate only (dry run)::</p> <p>sql = connect_and_build_catalog(\"catalog.yaml\", dry_run=True) print(\"SQL generation completed, no database created\")</p> <p>Custom database path::</p> <pre><code>conn = connect_and_build_catalog(\"catalog.yaml\", database_path=\"analytics.db\")\nprint(\"Connected to custom database: analytics.db\")\n</code></pre> <p>If <code>dry_run=True</code>, the function only validates the configuration and returns the SQL script without creating any database files or connections.</p>"},{"location":"reference/#duckalog.connect_to_catalog","title":"<code>connect_to_catalog(config_path, database_path=None, read_only=False)</code>","text":"<p>Connect to an existing DuckDB database created by Duckalog.</p> <p>This function provides a direct connection to a DuckDB database that was previously created using Duckalog. It simplifies the workflow for users who just want to start querying their catalog database.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file for an existing catalog.</p> required <code>database_path</code> <code>str | None</code> <p>Optional database path override. If not provided, uses the path from the configuration.</p> <code>None</code> <code>read_only</code> <code>bool</code> <p>Open the connection in read-only mode for safety.</p> <code>False</code> <p>Returns:</p> Type Description <p>An active DuckDB connection object ready for query execution.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> <code>FileNotFoundError</code> <p>If the specified database file doesn't exist.</p> <code>Error</code> <p>If connection or queries fail.</p> Example <p>Connect to an existing catalog::</p> <pre><code>from duckalog import connect_to_catalog\nconn = connect_to_catalog(\"catalog.yaml\")\n\n# Use the connection for queries\nresult = conn.execute(\"SELECT * FROM some_table\").fetchall()\nconn.close()\n</code></pre> <p>With path override::</p> <pre><code>conn = connect_to_catalog(\"catalog.yaml\", database_path=\"analytics.db\")\n</code></pre> <p>With read-only mode::</p> <pre><code>conn = connect_to_catalog(\"catalog.yaml\", read_only=True)\n</code></pre> <p>With context manager::</p> <pre><code>from duckalog import connect_to_catalog_cm\nwith connect_to_catalog_cm(\"catalog.yaml\") as conn:\n    data = conn.execute(\"SELECT * FROM users\").fetchall()\n    print(f\"Found {len(data)} records\")\n# Connection automatically closed here\n</code></pre>"},{"location":"reference/#duckalog.connect_to_catalog_cm","title":"<code>connect_to_catalog_cm(config_path, database_path=None, read_only=False)</code>","text":"<p>Context manager version of connect_to_catalog for automatic connection cleanup.</p> <p>Usage::</p> <pre><code>from duckalog import connect_to_catalog_cm\nwith connect_to_catalog_cm(\"catalog.yaml\") as conn:\n    data = conn.execute(\"SELECT * FROM users\").fetchall()\n    print(f\"Found {len(data)} records\")\n# Connection is automatically closed here\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file for an existing catalog.</p> required <code>database_path</code> <code>str | None</code> <p>Optional database path override.</p> <code>None</code> <code>read_only</code> <code>bool</code> <p>Open the connection in read-only mode for safety.</p> <code>False</code> <p>Yields:</p> Type Description <code>Generator[DuckDBPyConnection]</code> <p>An active DuckDB connection that will be closed automatically.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> <code>FileNotFoundError</code> <p>If the specified database file doesn't exist.</p> <code>Error</code> <p>If connection or queries fail.</p>"},{"location":"reference/#duckalog.create_config_template","title":"<code>create_config_template(format='yaml', output_path=None, database_name='analytics_catalog.duckdb', project_name='my_analytics_project')</code>","text":"<p>Generate a basic, valid Duckalog configuration template.</p> <p>This function creates a configuration template with sensible defaults and educational example content that demonstrates key Duckalog features.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>ConfigFormat</code> <p>Output format for the configuration ('yaml' or 'json'). Defaults to 'yaml'.</p> <code>'yaml'</code> <code>output_path</code> <code>str | None</code> <p>Optional path to write the configuration file. If provided, the template is written to this path and the content is also returned as a string.</p> <code>None</code> <code>database_name</code> <code>str</code> <p>Name for the DuckDB database file.</p> <code>'analytics_catalog.duckdb'</code> <code>project_name</code> <code>str</code> <p>Name used in comments to personalize the template.</p> <code>'my_analytics_project'</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated configuration as a string.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If format is not 'yaml' or 'json'.</p> <code>ConfigError</code> <p>If the generated template fails validation.</p> <code>OSError</code> <p>If writing to output_path fails.</p> Example <p>Generate a YAML template::</p> <pre><code>template = create_config_template(format='yaml')\nprint(template)\n</code></pre> <p>Generate and save a JSON template::</p> <pre><code>template = create_config_template(\n    format='json',\n    output_path='my_config.json'\n)\n</code></pre>"},{"location":"reference/#duckalog.generate_all_views_sql","title":"<code>generate_all_views_sql(config, include_secrets=False)</code>","text":"<p>Generate SQL for all views in a configuration.</p> <p>The output includes a descriptive header with the config version followed by a <code>CREATE OR REPLACE VIEW</code> statement for each view in the order they appear in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The validated :class:<code>Config</code> instance to render.</p> required <code>include_secrets</code> <code>bool</code> <p>Whether to include CREATE SECRET statements for secrets.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>A multi-statement SQL script suitable for use as a catalog definition.</p>"},{"location":"reference/#duckalog.generate_secret_sql","title":"<code>generate_secret_sql(secret)</code>","text":"<p>Generate CREATE SECRET statement for a DuckDB secret.</p> <p>Parameters:</p> Name Type Description Default <code>secret</code> <code>SecretConfig</code> <p>Secret configuration object.</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL CREATE SECRET statement.</p>"},{"location":"reference/#duckalog.generate_sql","title":"<code>generate_sql(config_path)</code>","text":"<p>Generate a full SQL script from a config file.</p> <p>This is a convenience wrapper around :func:<code>load_config</code> and :func:<code>generate_all_views_sql</code> that does not connect to DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A multi-statement SQL script containing <code>CREATE OR REPLACE VIEW</code></p> <code>str</code> <p>statements for all configured views.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> Example <p>from duckalog import generate_sql sql = generate_sql(\"catalog.yaml\") print(\"CREATE VIEW\" in sql) True</p>"},{"location":"reference/#duckalog.generate_view_sql","title":"<code>generate_view_sql(view)</code>","text":"<p>Generate a <code>CREATE OR REPLACE VIEW</code> statement for a single view.</p> <p>Parameters:</p> Name Type Description Default <code>view</code> <code>ViewConfig</code> <p>The :class:<code>ViewConfig</code> to generate SQL for.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A single SQL statement that creates or replaces the view.</p>"},{"location":"reference/#duckalog.load_config","title":"<code>load_config(path, load_sql_files=True, sql_file_loader=None, resolve_paths=True, filesystem=None)</code>","text":"<p>Load, interpolate, and validate a Duckalog configuration file.</p> <p>This helper is the main entry point for turning a YAML or JSON file into a validated :class:<code>Config</code> instance. It applies environment-variable interpolation and enforces the configuration schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a YAML or JSON config file, or a remote URI.</p> required <code>load_sql_files</code> <code>bool</code> <p>Whether to load and process SQL from external files.           If False, SQL file references are left as-is for later processing.</p> <code>True</code> <code>sql_file_loader</code> <code>Optional[Any]</code> <p>Optional SQLFileLoader instance for loading SQL files.             If None, a default loader will be created.</p> <code>None</code> <code>resolve_paths</code> <code>bool</code> <p>Whether to resolve relative paths to absolute paths.           If True, relative paths in view URIs and attachment paths           will be resolved relative to the config file's directory.           For remote configs, this defaults to False.</p> <code>True</code> <code>filesystem</code> <code>Optional[Any]</code> <p>Optional fsspec filesystem object to use for remote operations.        If provided, this filesystem will be used instead of creating        a new one based on URI scheme. Useful for custom        authentication or advanced use cases.</p> <code>None</code> <p>Returns:</p> Type Description <p>A validated :class:<code>Config</code> object.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the file cannot be read, is not valid YAML/JSON, fails schema validation, contains unresolved <code>${env:VAR_NAME}</code> placeholders, or if SQL file loading fails.</p> Example <p>Load a catalog from <code>catalog.yaml</code>::</p> <pre><code>from duckalog import load_config\n\nconfig = load_config(\"catalog.yaml\")\nprint(len(config.views))\n</code></pre> <p>Load a catalog from S3::</p> <pre><code>config = load_config(\"s3://my-bucket/configs/catalog.yaml\")\nprint(len(config.views))\n</code></pre> <p>Load a catalog with custom filesystem::</p> <pre><code>import fsspec\nfs = fsspec.filesystem(\"s3\", key=\"key\", secret=\"secret\", anon=False)\nconfig = load_config(\"s3://my-bucket/configs/catalog.yaml\", filesystem=fs)\nprint(len(config.views))\n</code></pre>"},{"location":"reference/#duckalog.quote_ident","title":"<code>quote_ident(value)</code>","text":"<p>Quote a SQL identifier using double quotes.</p> <p>This helper wraps a string in double quotes and escapes any embedded double quotes according to SQL rules.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Identifier to quote (for example, a view or column name).</p> required <p>Returns:</p> Type Description <code>str</code> <p>The identifier wrapped in double quotes.</p> Example <p>quote_ident(\"events\") '\"events\"'</p>"},{"location":"reference/#duckalog.quote_literal","title":"<code>quote_literal(value)</code>","text":"<p>Quote a SQL string literal using single quotes.</p> <p>This helper wraps a string in single quotes and escapes any embedded single quotes according to SQL rules.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>String literal to quote (for example, a file path, secret, or connection string).</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string wrapped in single quotes with proper escaping.</p> Example <p>quote_literal(\"path/to/file.parquet\") \"'path/to/file.parquet'\" quote_literal(\"user's data\") \"'user''s data'\"</p>"},{"location":"reference/#duckalog.render_options","title":"<code>render_options(options)</code>","text":"<p>Render a mapping of options into scan-function arguments.</p> <p>The resulting string is suitable for appending to a <code>*_scan</code> function call. Keys are sorted alphabetically to keep output deterministic.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>dict[str, Any]</code> <p>Mapping of option name to value (str, bool, int, or float).</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string that starts with <code>,</code> when options are present (for example,</p> <code>str</code> <p><code>\", hive_partitioning=TRUE\"</code>) or an empty string when no options</p> <code>str</code> <p>are provided.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If a value has a type that cannot be rendered safely.</p>"},{"location":"reference/#duckalog.validate_config","title":"<code>validate_config(config_path)</code>","text":"<p>Validate a configuration file without touching DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is missing, malformed, or does not satisfy the schema and interpolation rules.</p> Example <p>from duckalog import validate_config validate_config(\"catalog.yaml\")  # raises on invalid config</p>"},{"location":"reference/#duckalog.validate_generated_config","title":"<code>validate_generated_config(content, format='yaml')</code>","text":"<p>Validate that generated configuration content can be loaded successfully.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Configuration content as string.</p> required <code>format</code> <code>ConfigFormat</code> <p>Format of the content ('yaml' or 'json').</p> <code>'yaml'</code> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration cannot be loaded or is invalid.</p>"},{"location":"reference/#configuration-models","title":"Configuration Models","text":""},{"location":"reference/#config","title":"Config","text":"<p>The main configuration class that represents a Duckalog configuration file.</p>"},{"location":"reference/#view","title":"View","text":"<p>Represents a view definition in the catalog.</p>"},{"location":"reference/#duckdbconfig","title":"DuckDBConfig","text":"<p>Configuration for the DuckDB database instance.</p>"},{"location":"reference/#attachment","title":"Attachment","text":"<p>Configuration for database attachments.</p>"},{"location":"reference/#icebergcatalog","title":"IcebergCatalog","text":"<p>Configuration for Iceberg catalog connections.</p>"},{"location":"reference/#utility-functions","title":"Utility Functions","text":""},{"location":"reference/#load_config","title":"load_config","text":"<p>Load a Duckalog configuration from a file path.</p>"},{"location":"reference/#build_catalog","title":"build_catalog","text":"<p>Build a DuckDB catalog from a configuration file.</p>"},{"location":"reference/#validate_config","title":"validate_config","text":"<p>Validate a Duckalog configuration without building.</p>"},{"location":"reference/#generate_sql","title":"generate_sql","text":"<p>Generate SQL statements from a configuration file.</p>"},{"location":"reference/#command-line-interface","title":"Command Line Interface","text":""},{"location":"reference/#duckalog-build","title":"duckalog build","text":"<p>Build a DuckDB catalog from a configuration file.</p>"},{"location":"reference/#duckalog-validate","title":"duckalog validate","text":"<p>Validate a configuration file.</p>"},{"location":"reference/#duckalog-generate-sql","title":"duckalog generate-sql","text":"<p>Generate SQL statements from a configuration file.</p>"},{"location":"reference/#error-handling","title":"Error Handling","text":""},{"location":"reference/#configerror","title":"ConfigError","text":"<p>Raised when there's an error in the configuration file.</p>"},{"location":"reference/#catalogerror","title":"CatalogError","text":"<p>Raised when there's an error building the catalog.</p>"},{"location":"reference/#advanced-usage","title":"Advanced Usage","text":"<p>For detailed examples and advanced usage patterns, see the User Guide and Examples.</p>"},{"location":"reference/api/","title":"API Reference","text":"<p>This section documents the core API functions and classes in Duckalog, with a focus on path resolution features.</p>"},{"location":"reference/api/#core-configuration-api","title":"Core Configuration API","text":""},{"location":"reference/api/#load_config","title":"load_config()","text":"<p>Load, interpolate, and validate a Duckalog configuration file with optional path resolution.</p> <pre><code>from duckalog import load_config\n\n# Load with path resolution enabled (default)\nconfig = load_config(\"catalog.yaml\")\n\n# Load with explicit path resolution control\nconfig = load_config(\"catalog.yaml\", resolve_paths=True)\nconfig = load_config(\"catalog.yaml\", resolve_paths=False)\n\n# Load without processing SQL files (for validation)\nconfig = load_config(\"catalog.yaml\", load_sql_files=False)\n</code></pre> <p>Parameters: - <code>path</code> (str): Path to YAML or JSON configuration file - <code>load_sql_files</code> (bool, optional): Whether to load SQL from external files. Default: True - <code>sql_file_loader</code> (SQLFileLoader, optional): Custom SQL file loader instance - <code>resolve_paths</code> (bool, optional): Whether to resolve relative paths. Default: True</p> <p>Returns: - <code>Config</code>: Validated configuration object</p> <p>Raises: - <code>ConfigError</code>: Configuration parsing or validation errors (inherits from <code>DuckalogError</code>)</p>"},{"location":"reference/api/#path-resolution-api","title":"Path Resolution API","text":""},{"location":"reference/api/#path-detection-functions","title":"Path Detection Functions","text":""},{"location":"reference/api/#is_relative_path","title":"is_relative_path()","text":"<p>Detect if a path is relative based on platform-specific rules.</p> <pre><code>from duckalog.path_resolution import is_relative_path\n\n# Returns True for relative paths\nassert is_relative_path(\"data/file.parquet\")\nassert is_relative_path(\"../shared/data.parquet\")\n\n# Returns False for absolute paths and URIs\nassert not is_relative_path(\"/absolute/path/file.parquet\")\nassert not is_relative_path(\"s3://bucket/file.parquet\")\n</code></pre> <p>Parameters: - <code>path</code> (str): Path string to check</p> <p>Returns: - <code>bool</code>: True if path is relative, False otherwise</p>"},{"location":"reference/api/#detect_path_type","title":"detect_path_type()","text":"<p>Categorize paths by type for processing.</p> <pre><code>from duckalog.path_resolution import detect_path_type\n\npath_type = detect_path_type(\"../data/file.parquet\")  # Returns: \"relative\"\npath_type = detect_path_type(\"/absolute/file.parquet\")  # Returns: \"absolute\"\npath_type = detect_path_type(\"s3://bucket/file.parquet\")  # Returns: \"remote\"\n</code></pre> <p>Parameters: - <code>path</code> (str): Path string to analyze</p> <p>Returns: - <code>str</code>: One of \"relative\", \"absolute\", \"remote\", \"invalid\"</p>"},{"location":"reference/api/#path-resolution-functions","title":"Path Resolution Functions","text":""},{"location":"reference/api/#resolve_relative_path","title":"resolve_relative_path()","text":"<p>Resolve a relative path to an absolute path relative to configuration directory.</p> <pre><code>from duckalog.path_resolution import resolve_relative_path\nfrom pathlib import Path\n\nconfig_dir = Path(\"/project/config\")\nresolved = resolve_relative_path(\"data/file.parquet\", config_dir)\n# Returns: \"/project/config/data/file.parquet\"\n</code></pre> <p>Parameters: - <code>path</code> (str): Path to resolve (relative or absolute) - <code>config_dir</code> (Path): Directory containing the configuration file</p> <p>Returns: - <code>str</code>: Resolved absolute path</p> <p>Raises: - <code>ValueError</code>: If path resolution fails or violates security rules</p>"},{"location":"reference/api/#security-validation-functions","title":"Security Validation Functions","text":""},{"location":"reference/api/#validate_path_security","title":"validate_path_security()","text":"<p>Validate that resolved paths don't violate security boundaries.</p> <pre><code>from duckalog.path_resolution import validate_path_security\nfrom pathlib import Path\n\nconfig_dir = Path(\"/project/config\")\n\n# Safe paths return True\nassert validate_path_security(\"data/file.parquet\", config_dir)\nassert validate_path_security(\"../shared/data.parquet\", config_dir)\n\n# Dangerous paths return False\nassert not validate_path_security(\"../../../etc/passwd\", config_dir)\n</code></pre> <p>Parameters: - <code>path</code> (str): Path to validate (will be resolved if relative) - <code>config_dir</code> (Path): Directory containing the configuration file</p> <p>Returns: - <code>bool</code>: True if path is safe, False otherwise</p>"},{"location":"reference/api/#validate_file_accessibility","title":"validate_file_accessibility()","text":"<p>Validate that a file path is accessible and readable.</p> <pre><code>from duckalog.path_resolution import validate_file_accessibility\n\naccessible, error = validate_file_accessibility(\"/path/to/file.parquet\")\n\nif accessible:\n    print(\"File is accessible\")\nelse:\n    print(f\"File access error: {error}\")\n</code></pre> <p>Parameters: - <code>path</code> (str): File path to validate</p> <p>Returns: - <code>tuple[bool, str | None]</code>: (is_accessible, error_message)</p>"},{"location":"reference/api/#path-normalization-functions","title":"Path Normalization Functions","text":""},{"location":"reference/api/#normalize_path_for_sql","title":"normalize_path_for_sql()","text":"<p>Normalize a path for use in SQL statements.</p> <pre><code>from duckalog.path_resolution import normalize_path_for_sql\n\nsql_path = normalize_path_for_sql(\"/path/to/file.parquet\")\n# Returns: \"'/path/to/file.parquet'\"\n\nsql_path = normalize_path_for_sql(\"/path/file's_data.parquet\")  \n# Returns: \"'/path/file''s_data.parquet'\" (properly escaped)\n</code></pre> <p>Parameters: - <code>path</code> (str): Absolute path to normalize</p> <p>Returns: - <code>str</code>: SQL-safe quoted path</p>"},{"location":"reference/api/#root-based-path-security-functions","title":"Root-Based Path Security Functions","text":""},{"location":"reference/api/#is_within_allowed_roots","title":"is_within_allowed_roots()","text":"<p>Check if a resolved path is within any of the allowed root directories using robust cross-platform validation.</p> <pre><code>from duckalog.path_resolution import is_within_allowed_roots\nfrom pathlib import Path\n\nconfig_dir = Path(\"/project/config\")\nallowed_roots = [config_dir]\n\n# Check if a path is within allowed roots\nis_safe = is_within_allowed_roots(\"/project/config/data/file.parquet\", allowed_roots)\n# Returns: True\n\nis_unsafe = is_within_allowed_roots(\"/etc/passwd\", allowed_roots)  \n# Returns: False\n\nis_traversal_blocked = is_within_allowed_roots(\"../../../etc/passwd\", allowed_roots)\n# Returns: False (path traversal blocked)\n</code></pre> <p>Parameters: - <code>candidate_path</code> (str): The path to check (will be resolved to absolute) - <code>allowed_roots</code> (list[Path]): List of Path objects representing allowed root directories</p> <p>Returns: - <code>bool</code>: True if the candidate path is within at least one allowed root, False otherwise</p> <p>Raises: - <code>ValueError</code>: If the candidate path cannot be resolved (invalid path)</p> <p>Security Features: - Uses <code>Path.resolve()</code> to follow symlinks and get canonical paths - Uses <code>os.path.commonpath()</code> for robust cross-platform path comparison - Prevents all forms of path traversal attacks regardless of encoding or separators - Handles Windows drive letters and UNC paths correctly</p>"},{"location":"reference/api/#sql-generation-api","title":"SQL Generation API","text":""},{"location":"reference/api/#sql-quoting-functions","title":"SQL Quoting Functions","text":""},{"location":"reference/api/#quote_ident","title":"quote_ident()","text":"<p>Quote a SQL identifier using double quotes with proper escaping.</p> <pre><code>from duckalog import quote_ident\n\n# Quote simple identifiers\nassert quote_ident(\"events\") == '\"events\"'\n\n# Quote identifiers with spaces\nassert quote_ident(\"daily users\") == '\"daily users\"'\n\n# Quote identifiers with quotes (escapes embedded quotes)\nassert quote_ident('user \"events\"') == '\"user \"\"events\"\"\"'\n</code></pre> <p>Parameters: - <code>identifier</code> (str): Identifier to quote</p> <p>Returns: - <code>str</code>: Identifier wrapped in double quotes with proper escaping</p>"},{"location":"reference/api/#quote_literal","title":"quote_literal()","text":"<p>Quote a SQL string literal using single quotes with proper escaping.</p> <pre><code>from duckalog import quote_literal\n\n# Quote simple strings\nassert quote_literal(\"user's data\") == \"'user''s data'\"\n\n# Quote file paths\nassert quote_literal(\"path/to/file.parquet\") == \"'path/to/file.parquet'\"\n\n# Quote SQL statements\nassert quote_literal(\"SELECT * FROM table\") == \"'SELECT * FROM table'\"\n\n# Quote empty strings\nassert quote_literal(\"\") == \"''\"\n</code></pre> <p>Parameters: - <code>value</code> (str): String literal to quote</p> <p>Returns: - <code>str</code>: String wrapped in single quotes with proper escaping</p>"},{"location":"reference/api/#generate_view_sql","title":"generate_view_sql()","text":"<p>Generate a <code>CREATE OR REPLACE VIEW</code> statement for a view configuration.</p> <pre><code>from duckalog import ViewConfig, generate_view_sql\n\nview = ViewConfig(\n    name=\"test_view\",\n    source=\"duckdb\",\n    database=\"my_db\",\n    table=\"users\"\n)\n\nsql = generate_view_sql(view)\n# Returns: CREATE OR REPLACE VIEW \"test_view\" AS SELECT * FROM \"my_db\".\"users\";\n</code></pre> <p>Parameters: - <code>view</code> (ViewConfig): View configuration to generate SQL for</p> <p>Returns: - <code>str</code>: SQL statement creating the view with proper quoting</p>"},{"location":"reference/api/#generate_secret_sql","title":"generate_secret_sql()","text":"<p>Generate <code>CREATE SECRET</code> statement for a secret configuration with proper escaping.</p> <pre><code>from duckalog import SecretConfig, generate_secret_sql\n\nsecret = SecretConfig(\n    type=\"s3\",\n    name=\"prod_s3\",\n    provider=\"config\",\n    key_id=\"user's key\",  # Contains single quote\n    secret=\"secret'with'quotes\"\n)\n\nsql = generate_secret_sql(secret)\n# Returns: CREATE SECRET prod_s3 (TYPE S3, KEY_ID 'user''s key', SECRET 'secret''with''quotes')\n</code></pre> <p>Parameters: - <code>secret</code> (SecretConfig): Secret configuration to generate SQL for</p> <p>Returns: - <code>str</code>: SQL statement creating the secret with proper escaping</p> <p>Note: This function enforces strict type checking for secret options and will raise <code>TypeError</code> for unsupported types (non-bool, int, float, or str values).</p>"},{"location":"reference/api/#configuration-models","title":"Configuration Models","text":""},{"location":"reference/api/#config","title":"Config","text":"<p>Top-level configuration model with path resolution integration.</p> <pre><code>from duckalog import load_config\n\nconfig = load_config(\"catalog.yaml\")\n\n# Access configuration properties\nprint(config.version)\nprint(len(config.views))\n\n# Views have resolved paths when resolution is enabled\nfor view in config.views:\n    print(f\"View: {view.name}, URI: {view.uri}\")\n</code></pre>"},{"location":"reference/api/#viewconfig","title":"ViewConfig","text":"<p>Model for individual view definitions that may contain resolved paths.</p> <pre><code>from duckalog.config import ViewConfig\n\n# View with relative path (will be resolved during config loading)\nview = ViewConfig(\n    name=\"events\",\n    source=\"parquet\", \n    uri=\"data/events.parquet\"  # Relative path\n)\n\n# View with absolute path (unchanged during resolution)\nview = ViewConfig(\n    name=\"remote_data\",\n    source=\"parquet\",\n    uri=\"s3://bucket/events.parquet\"  # Remote URI - unchanged\n)\n</code></pre>"},{"location":"reference/api/#attachmentconfig","title":"AttachmentConfig","text":"<p>Models for database attachments with path resolution support.</p> <pre><code>from duckalog.config import DuckDBAttachment, SQLiteAttachment\n\n# DuckDB attachment with relative path\nduckdb_attach = DuckDBAttachment(\n    alias=\"reference_db\",\n    path=\"./databases/reference.duckdb\",  # Will be resolved\n    read_only=True\n)\n\n# SQLite attachment with relative path\nsqlite_attach = SQLiteAttachment(\n    alias=\"legacy_db\", \n    path=\"../legacy/system.db\"  # Will be resolved\n)\n</code></pre>"},{"location":"reference/api/#exception-classes","title":"Exception Classes","text":"<p>Duckalog uses a consistent exception hierarchy based on <code>DuckalogError</code> as the base class for all library exceptions. This provides a unified error handling interface and makes it easy to catch all Duckalog-specific errors.</p>"},{"location":"reference/api/#base-exception","title":"Base Exception","text":""},{"location":"reference/api/#duckalogerror","title":"DuckalogError","text":"<p>Base exception for all Duckalog-specific errors. You can catch this class to handle any Duckalog-related error, or catch more specific subclasses for targeted error handling.</p> <pre><code>from duckalog import DuckalogError\n\ntry:\n    config = load_config(\"catalog.yaml\")\n    build_catalog(config)\nexcept DuckalogError as e:\n    # Handle any Duckalog-specific error\n    print(f\"Duckalog error: {e}\")\n</code></pre>"},{"location":"reference/api/#configuration-errors","title":"Configuration Errors","text":""},{"location":"reference/api/#configerror","title":"ConfigError","text":"<p>Raised for configuration-related errors including parsing, validation, and path resolution failures. Inherits from <code>DuckalogError</code>.</p> <pre><code>from duckalog import load_config, ConfigError\n\ntry:\n    config = load_config(\"catalog.yaml\")\nexcept ConfigError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre> <p>Common scenarios: - Missing required configuration fields - Invalid YAML/JSON syntax - Unresolved environment variable placeholders (<code>${env:VAR}</code>) - Invalid view definitions</p>"},{"location":"reference/api/#pathresolutionerror","title":"PathResolutionError","text":"<p>Raised specifically when path resolution fails due to security or access issues. Inherits from <code>ConfigError</code>.</p> <pre><code>from duckalog import PathResolutionError, resolve_relative_path\nfrom pathlib import Path\n\ntry:\n    resolved = resolve_relative_path(\"../../../etc/passwd\", Path(\"/project/config\"))\nexcept PathResolutionError as e:\n    print(f\"Path resolution failed: {e}\")\n    print(f\"Original path: {e.original_path}\")\n    print(f\"Resolved path: {e.resolved_path}\")\n</code></pre>"},{"location":"reference/api/#remoteconfigerror","title":"RemoteConfigError","text":"<p>Raised when remote configuration loading fails. Inherits from <code>ConfigError</code>.</p> <pre><code>from duckalog import RemoteConfigError, load_config_from_uri\n\ntry:\n    config = load_config_from_uri(\"s3://bucket/config.yaml\")\nexcept RemoteConfigError as e:\n    print(f\"Remote config error: {e}\")\n</code></pre>"},{"location":"reference/api/#sql-file-errors","title":"SQL File Errors","text":""},{"location":"reference/api/#sqlfileerror","title":"SQLFileError","text":"<p>Base exception for SQL file-related errors. Inherits from <code>DuckalogError</code>.</p> <pre><code>from duckalog import SQLFileError\n\ntry:\n    # Operations that load SQL from files\n    pass\nexcept SQLFileError as e:\n    print(f\"SQL file error: {e}\")\n</code></pre> <p>Subclasses for specific SQL file errors: - <code>SQLFileNotFoundError</code>: Referenced SQL file does not exist - <code>SQLFilePermissionError</code>: SQL file cannot be read due to permissions - <code>SQLFileEncodingError</code>: SQL file has invalid encoding - <code>SQLFileSizeError</code>: SQL file exceeds size limits - <code>SQLTemplateError</code>: Template processing fails</p>"},{"location":"reference/api/#engine-errors","title":"Engine Errors","text":""},{"location":"reference/api/#engineerror","title":"EngineError","text":"<p>Raised during catalog builds when DuckDB operations fail. Inherits from <code>DuckalogError</code>.</p> <pre><code>from duckalog import EngineError, build_catalog\n\ntry:\n    build_catalog(\"catalog.yaml\")\nexcept EngineError as e:\n    print(f\"Engine error: {e}\")\n</code></pre> <p>Common scenarios: - DuckDB connection failures - SQL execution errors - Attachment setup failures - Extension installation/loading errors - Secret creation failures</p>"},{"location":"reference/api/#exception-chaining","title":"Exception Chaining","text":"<p>All Duckalog exceptions support proper exception chaining to preserve the original error context:</p> <pre><code>from duckalog import EngineError\n\ntry:\n    # Some operation that fails\n    raise ValueError(\"Original database connection failed\")\nexcept ValueError as exc:\n    raise EngineError(\"Failed to connect to DuckDB\") from exc\n</code></pre> <p>This preserves the full traceback and makes debugging much easier.</p>"},{"location":"reference/api/#generated-api-documentation","title":"Generated API Documentation","text":"<p>This section is generated from the Duckalog source code using mkdocstrings.</p>"},{"location":"reference/api/#duckalog","title":"<code>duckalog</code>","text":"<p>Duckalog public API.</p>"},{"location":"reference/api/#duckalog.AttachmentsConfig","title":"<code>AttachmentsConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Collection of attachment configurations.</p> <p>Attributes:</p> Name Type Description <code>duckdb</code> <code>list[DuckDBAttachment]</code> <p>DuckDB attachment entries.</p> <code>sqlite</code> <code>list[SQLiteAttachment]</code> <p>SQLite attachment entries.</p> <code>postgres</code> <code>list[PostgresAttachment]</code> <p>Postgres attachment entries.</p> <code>duckalog</code> <code>list[DuckalogAttachment]</code> <p>Duckalog config attachment entries.</p>"},{"location":"reference/api/#duckalog.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level Duckalog configuration.</p> <p>Attributes:</p> Name Type Description <code>version</code> <code>int</code> <p>Positive integer describing the config schema version.</p> <code>duckdb</code> <code>DuckDBConfig</code> <p>DuckDB session and connection settings.</p> <code>views</code> <code>list[ViewConfig]</code> <p>List of view definitions to create in the catalog.</p> <code>attachments</code> <code>AttachmentsConfig</code> <p>Optional attachments to external databases.</p> <code>iceberg_catalogs</code> <code>list[IcebergCatalogConfig]</code> <p>Optional Iceberg catalog definitions.</p> <code>semantic_models</code> <code>list[SemanticModelConfig]</code> <p>Optional semantic model definitions for business metadata.</p>"},{"location":"reference/api/#duckalog.ConfigError","title":"<code>ConfigError</code>","text":"<p>               Bases: <code>DuckalogError</code></p> <p>Configuration-related errors.</p> <p>This exception is raised when a catalog configuration cannot be read, parsed, interpolated, or validated according to the Duckalog schema.</p> <p>Typical error conditions include:</p> <ul> <li>The config file does not exist or cannot be read.</li> <li>The file is not valid YAML/JSON.</li> <li>Required fields are missing or invalid.</li> <li>An environment variable placeholder cannot be resolved.</li> </ul>"},{"location":"reference/api/#duckalog.DuckDBAttachment","title":"<code>DuckDBAttachment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for attaching another DuckDB database.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>Alias under which the database will be attached.</p> <code>path</code> <code>str</code> <p>Filesystem path to the DuckDB database file.</p> <code>read_only</code> <code>bool</code> <p>Whether the attachment should be opened in read-only mode. Defaults to <code>True</code> for safety.</p>"},{"location":"reference/api/#duckalog.DuckDBConfig","title":"<code>DuckDBConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DuckDB connection and session settings.</p> <p>Attributes:</p> Name Type Description <code>database</code> <code>str</code> <p>Path to the DuckDB database file. Defaults to <code>\":memory:\"</code>.</p> <code>install_extensions</code> <code>list[str]</code> <p>Names of extensions to install before use.</p> <code>load_extensions</code> <code>list[str]</code> <p>Names of extensions to load in the session.</p> <code>pragmas</code> <code>list[str]</code> <p>SQL statements (typically <code>SET</code> pragmas) executed after connecting and loading extensions.</p> <code>settings</code> <code>Optional[Literal[str, list[str]]]</code> <p>DuckDB SET statements executed after pragmas. Can be a single string or list of strings.</p> <code>secrets</code> <code>list[SecretConfig]</code> <p>List of secret definitions for external services and databases.</p>"},{"location":"reference/api/#duckalog.EngineError","title":"<code>EngineError</code>","text":"<p>               Bases: <code>DuckalogError</code></p> <p>Engine-level error raised during catalog builds.</p> <p>This exception wraps lower-level DuckDB errors, such as failures to connect to the database, attach external systems, or execute generated SQL statements.</p>"},{"location":"reference/api/#duckalog.IcebergCatalogConfig","title":"<code>IcebergCatalogConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for an Iceberg catalog.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Catalog name referenced by Iceberg views.</p> <code>catalog_type</code> <code>str</code> <p>Backend type (for example, <code>rest</code>, <code>hive</code>, <code>glue</code>).</p> <code>uri</code> <code>Optional[str]</code> <p>Optional URI used by certain catalog types.</p> <code>warehouse</code> <code>Optional[str]</code> <p>Optional warehouse location for catalog data.</p> <code>options</code> <code>dict[str, Any]</code> <p>Additional catalog-specific options.</p>"},{"location":"reference/api/#duckalog.PostgresAttachment","title":"<code>PostgresAttachment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for attaching a Postgres database.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>Alias used inside DuckDB to reference the Postgres database.</p> <code>host</code> <code>str</code> <p>Hostname or IP address of the Postgres server.</p> <code>port</code> <code>int</code> <p>TCP port of the Postgres server.</p> <code>database</code> <code>str</code> <p>Database name to connect to.</p> <code>user</code> <code>str</code> <p>Username for authentication.</p> <code>password</code> <code>str</code> <p>Password for authentication.</p> <code>sslmode</code> <code>Optional[str]</code> <p>Optional SSL mode (for example, <code>require</code>).</p> <code>options</code> <code>dict[str, Any]</code> <p>Extra key/value options passed to the attachment clause.</p>"},{"location":"reference/api/#duckalog.SQLiteAttachment","title":"<code>SQLiteAttachment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for attaching a SQLite database.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>Alias under which the SQLite database will be attached.</p> <code>path</code> <code>str</code> <p>Filesystem path to the SQLite <code>.db</code> file.</p>"},{"location":"reference/api/#duckalog.SecretConfig","title":"<code>SecretConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for a DuckDB secret.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>SecretType</code> <p>Secret type (s3, azure, gcs, http, postgres, mysql).</p> <code>name</code> <code>Optional[str]</code> <p>Optional name for the secret (defaults to type if not provided).</p> <code>provider</code> <code>SecretProvider</code> <p>Secret provider (config or credential_chain).</p> <code>persistent</code> <code>bool</code> <p>Whether to create a persistent secret. Defaults to False.</p> <code>scope</code> <code>Optional[str]</code> <p>Optional scope prefix for the secret.</p> <code>key_id</code> <code>Optional[str]</code> <p>Access key ID or username for authentication.</p> <code>secret</code> <code>Optional[str]</code> <p>Secret key or password for authentication.</p> <code>region</code> <code>Optional[str]</code> <p>Geographic region for cloud services.</p> <code>endpoint</code> <code>Optional[str]</code> <p>Custom endpoint URL for cloud services.</p> <code>connection_string</code> <code>Optional[str]</code> <p>Full connection string for databases.</p> <code>tenant_id</code> <code>Optional[str]</code> <p>Azure tenant ID for authentication.</p> <code>account_name</code> <code>Optional[str]</code> <p>Azure storage account name.</p> <code>client_id</code> <code>Optional[str]</code> <p>Azure client ID for authentication.</p> <code>client_secret</code> <code>Optional[str]</code> <p>Azure client secret for authentication.</p> <code>service_account_key</code> <code>Optional[str]</code> <p>GCS service account key.</p> <code>json_key</code> <code>Optional[str]</code> <p>GCS JSON key.</p> <code>bearer_token</code> <code>Optional[str]</code> <p>HTTP bearer token for authentication.</p> <code>header</code> <code>Optional[str]</code> <p>HTTP header for authentication.</p> <code>database</code> <code>Optional[str]</code> <p>Database name for database secrets.</p> <code>host</code> <code>Optional[str]</code> <p>Database host for database secrets.</p> <code>port</code> <code>Optional[int]</code> <p>Database port for database secrets.</p> <code>user</code> <code>Optional[str]</code> <p>Database username (alternative to key_id for database types).</p> <code>password</code> <code>Optional[str]</code> <p>Database password (alternative to secret for database types).</p> <code>options</code> <code>dict[str, Any]</code> <p>Additional key-value options for the secret.</p>"},{"location":"reference/api/#duckalog.ViewConfig","title":"<code>ViewConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Definition of a single catalog view.</p> <p>A view can be defined in several ways: 1. Inline SQL: Using the <code>sql</code> field with raw SQL text 2. SQL File: Using <code>sql_file</code> to reference external SQL files 3. SQL Template: Using <code>sql_template</code> for parameterized SQL files 4. Data Source: Using <code>source</code> + required fields for direct data access 5. Source + SQL: Using <code>source</code> for data access plus <code>sql</code> for transformations</p> <p>For data sources, the required fields depend on the source type: - Parquet/Delta: <code>uri</code> field is required - Iceberg: Either <code>uri</code> OR both <code>catalog</code> and <code>table</code> - DuckDB/SQLite/Postgres: Both <code>database</code> and <code>table</code> are required</p> <p>When using SQL with a data source, the SQL will be applied as a transformation over the data from the specified source.</p> <p>Additional metadata fields such as <code>description</code> and <code>tags</code> do not affect SQL generation but are preserved for documentation and tooling.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique view name within the config.</p> <code>sql</code> <code>Optional[str]</code> <p>Raw SQL text defining the view body.</p> <code>sql_file</code> <code>Optional[SQLFileReference]</code> <p>Direct reference to a SQL file.</p> <code>sql_template</code> <code>Optional[SQLFileReference]</code> <p>Reference to a SQL template file with variable substitution.</p> <code>source</code> <code>Optional[EnvSource]</code> <p>Source type (e.g. <code>\"parquet\"</code>, <code>\"iceberg\"</code>, <code>\"duckdb\"</code>).</p> <code>uri</code> <code>Optional[str]</code> <p>URI for file- or table-based sources (Parquet/Delta/Iceberg).</p> <code>database</code> <code>Optional[str]</code> <p>Attachment alias for attached-database sources.</p> <code>table</code> <code>Optional[str]</code> <p>Table name (optionally schema-qualified) for attached sources.</p> <code>catalog</code> <code>Optional[str]</code> <p>Iceberg catalog name for catalog-based Iceberg views.</p> <code>options</code> <code>dict[str, Any]</code> <p>Source-specific options passed to scan functions.</p> <code>description</code> <code>Optional[str]</code> <p>Optional human-readable description of the view.</p> <code>tags</code> <code>list[str]</code> <p>Optional list of tags for classification.</p>"},{"location":"reference/api/#duckalog.build_catalog","title":"<code>build_catalog(config_path, db_path=None, dry_run=False, verbose=False, filesystem=None, include_secrets=True)</code>","text":"<p>Build or update a DuckDB catalog from a configuration file.</p> <p>This function is the high-level entry point used by both the CLI and Python API. It loads the config, optionally performs a dry-run SQL generation, or otherwise connects to DuckDB, sets up attachments and Iceberg catalogs, and creates or replaces configured views.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <code>db_path</code> <code>str | None</code> <p>Optional override for <code>duckdb.database</code> in the config. Can be a local path or remote URI (s3://, gs://, gcs://, abfs://, adl://, sftp://).</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If <code>True</code>, do not connect to DuckDB; instead generate and return the full SQL script for all views.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If <code>True</code>, enable more verbose logging via the standard logging module.</p> <code>False</code> <code>filesystem</code> <code>Any | None</code> <p>Optional pre-configured fsspec filesystem object for remote export authentication. If not provided, default authentication will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | None</code> <p>The generated SQL script as a string when <code>dry_run</code> is <code>True</code>,</p> <code>str | None</code> <p>otherwise <code>None</code> when the catalog is applied to DuckDB.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> <code>EngineError</code> <p>If connecting to DuckDB or executing SQL fails, or if remote export fails.</p> Example <p>Build a catalog in-place::</p> <pre><code>from duckalog import build_catalog\n\nbuild_catalog(\"catalog.yaml\")\n</code></pre> <p>Build and export to remote storage::</p> <pre><code>build_catalog(\"catalog.yaml\", db_path=\"s3://my-bucket/catalog.duckdb\")\n</code></pre> <p>Generate SQL without modifying the database::</p> <pre><code>sql = build_catalog(\"catalog.yaml\", dry_run=True)\nprint(sql)\n</code></pre>"},{"location":"reference/api/#duckalog.connect_and_build_catalog","title":"<code>connect_and_build_catalog(config_path, database_path=None, dry_run=False, verbose=False, read_only=False, **kwargs)</code>","text":"<p>Build a catalog and create a DuckDB connection in one operation.</p> <p>This function combines catalog building with connection creation, providing a streamlined workflow for users who want to start working with their catalog immediately after creating it.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <code>database_path</code> <code>str | None</code> <p>Optional database path override.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If True, only validates configuration and returns SQL. If False, builds the catalog and creates a connection.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging during build process.</p> <code>False</code> <code>read_only</code> <code>bool</code> <p>Open the resulting connection in read-only mode.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DuckDBPyConnection | str | None</code> <p>A DuckDB connection object for immediate use, or SQL string when dry_run=True.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> <code>EngineError</code> <p>If catalog building or connection fails.</p> <code>FileNotFoundError</code> <p>If the resulting database file doesn't exist (after build).</p> Example <p>Build catalog and start querying immediately::</p> <pre><code>conn = connect_and_build_catalog(\"catalog.yaml\")\ndata = conn.execute(\"SELECT * FROM important_table\").fetchall()\nprint(f\"Found {len(data)} records\")\nconn.close()\n</code></pre> <p>Validate only (dry run)::</p> <p>sql = connect_and_build_catalog(\"catalog.yaml\", dry_run=True) print(\"SQL generation completed, no database created\")</p> <p>Custom database path::</p> <pre><code>conn = connect_and_build_catalog(\"catalog.yaml\", database_path=\"analytics.db\")\nprint(\"Connected to custom database: analytics.db\")\n</code></pre> <p>If <code>dry_run=True</code>, the function only validates the configuration and returns the SQL script without creating any database files or connections.</p>"},{"location":"reference/api/#duckalog.connect_to_catalog","title":"<code>connect_to_catalog(config_path, database_path=None, read_only=False)</code>","text":"<p>Connect to an existing DuckDB database created by Duckalog.</p> <p>This function provides a direct connection to a DuckDB database that was previously created using Duckalog. It simplifies the workflow for users who just want to start querying their catalog database.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file for an existing catalog.</p> required <code>database_path</code> <code>str | None</code> <p>Optional database path override. If not provided, uses the path from the configuration.</p> <code>None</code> <code>read_only</code> <code>bool</code> <p>Open the connection in read-only mode for safety.</p> <code>False</code> <p>Returns:</p> Type Description <p>An active DuckDB connection object ready for query execution.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> <code>FileNotFoundError</code> <p>If the specified database file doesn't exist.</p> <code>Error</code> <p>If connection or queries fail.</p> Example <p>Connect to an existing catalog::</p> <pre><code>from duckalog import connect_to_catalog\nconn = connect_to_catalog(\"catalog.yaml\")\n\n# Use the connection for queries\nresult = conn.execute(\"SELECT * FROM some_table\").fetchall()\nconn.close()\n</code></pre> <p>With path override::</p> <pre><code>conn = connect_to_catalog(\"catalog.yaml\", database_path=\"analytics.db\")\n</code></pre> <p>With read-only mode::</p> <pre><code>conn = connect_to_catalog(\"catalog.yaml\", read_only=True)\n</code></pre> <p>With context manager::</p> <pre><code>from duckalog import connect_to_catalog_cm\nwith connect_to_catalog_cm(\"catalog.yaml\") as conn:\n    data = conn.execute(\"SELECT * FROM users\").fetchall()\n    print(f\"Found {len(data)} records\")\n# Connection automatically closed here\n</code></pre>"},{"location":"reference/api/#duckalog.connect_to_catalog_cm","title":"<code>connect_to_catalog_cm(config_path, database_path=None, read_only=False)</code>","text":"<p>Context manager version of connect_to_catalog for automatic connection cleanup.</p> <p>Usage::</p> <pre><code>from duckalog import connect_to_catalog_cm\nwith connect_to_catalog_cm(\"catalog.yaml\") as conn:\n    data = conn.execute(\"SELECT * FROM users\").fetchall()\n    print(f\"Found {len(data)} records\")\n# Connection is automatically closed here\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file for an existing catalog.</p> required <code>database_path</code> <code>str | None</code> <p>Optional database path override.</p> <code>None</code> <code>read_only</code> <code>bool</code> <p>Open the connection in read-only mode for safety.</p> <code>False</code> <p>Yields:</p> Type Description <code>Generator[DuckDBPyConnection]</code> <p>An active DuckDB connection that will be closed automatically.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> <code>FileNotFoundError</code> <p>If the specified database file doesn't exist.</p> <code>Error</code> <p>If connection or queries fail.</p>"},{"location":"reference/api/#duckalog.create_config_template","title":"<code>create_config_template(format='yaml', output_path=None, database_name='analytics_catalog.duckdb', project_name='my_analytics_project')</code>","text":"<p>Generate a basic, valid Duckalog configuration template.</p> <p>This function creates a configuration template with sensible defaults and educational example content that demonstrates key Duckalog features.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>ConfigFormat</code> <p>Output format for the configuration ('yaml' or 'json'). Defaults to 'yaml'.</p> <code>'yaml'</code> <code>output_path</code> <code>str | None</code> <p>Optional path to write the configuration file. If provided, the template is written to this path and the content is also returned as a string.</p> <code>None</code> <code>database_name</code> <code>str</code> <p>Name for the DuckDB database file.</p> <code>'analytics_catalog.duckdb'</code> <code>project_name</code> <code>str</code> <p>Name used in comments to personalize the template.</p> <code>'my_analytics_project'</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated configuration as a string.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If format is not 'yaml' or 'json'.</p> <code>ConfigError</code> <p>If the generated template fails validation.</p> <code>OSError</code> <p>If writing to output_path fails.</p> Example <p>Generate a YAML template::</p> <pre><code>template = create_config_template(format='yaml')\nprint(template)\n</code></pre> <p>Generate and save a JSON template::</p> <pre><code>template = create_config_template(\n    format='json',\n    output_path='my_config.json'\n)\n</code></pre>"},{"location":"reference/api/#duckalog.generate_all_views_sql","title":"<code>generate_all_views_sql(config, include_secrets=False)</code>","text":"<p>Generate SQL for all views in a configuration.</p> <p>The output includes a descriptive header with the config version followed by a <code>CREATE OR REPLACE VIEW</code> statement for each view in the order they appear in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The validated :class:<code>Config</code> instance to render.</p> required <code>include_secrets</code> <code>bool</code> <p>Whether to include CREATE SECRET statements for secrets.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>A multi-statement SQL script suitable for use as a catalog definition.</p>"},{"location":"reference/api/#duckalog.generate_secret_sql","title":"<code>generate_secret_sql(secret)</code>","text":"<p>Generate CREATE SECRET statement for a DuckDB secret.</p> <p>Parameters:</p> Name Type Description Default <code>secret</code> <code>SecretConfig</code> <p>Secret configuration object.</p> required <p>Returns:</p> Type Description <code>str</code> <p>SQL CREATE SECRET statement.</p>"},{"location":"reference/api/#duckalog.generate_sql","title":"<code>generate_sql(config_path)</code>","text":"<p>Generate a full SQL script from a config file.</p> <p>This is a convenience wrapper around :func:<code>load_config</code> and :func:<code>generate_all_views_sql</code> that does not connect to DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A multi-statement SQL script containing <code>CREATE OR REPLACE VIEW</code></p> <code>str</code> <p>statements for all configured views.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> Example <p>from duckalog import generate_sql sql = generate_sql(\"catalog.yaml\") print(\"CREATE VIEW\" in sql) True</p>"},{"location":"reference/api/#duckalog.generate_view_sql","title":"<code>generate_view_sql(view)</code>","text":"<p>Generate a <code>CREATE OR REPLACE VIEW</code> statement for a single view.</p> <p>Parameters:</p> Name Type Description Default <code>view</code> <code>ViewConfig</code> <p>The :class:<code>ViewConfig</code> to generate SQL for.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A single SQL statement that creates or replaces the view.</p>"},{"location":"reference/api/#duckalog.load_config","title":"<code>load_config(path, load_sql_files=True, sql_file_loader=None, resolve_paths=True, filesystem=None)</code>","text":"<p>Load, interpolate, and validate a Duckalog configuration file.</p> <p>This helper is the main entry point for turning a YAML or JSON file into a validated :class:<code>Config</code> instance. It applies environment-variable interpolation and enforces the configuration schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a YAML or JSON config file, or a remote URI.</p> required <code>load_sql_files</code> <code>bool</code> <p>Whether to load and process SQL from external files.           If False, SQL file references are left as-is for later processing.</p> <code>True</code> <code>sql_file_loader</code> <code>Optional[Any]</code> <p>Optional SQLFileLoader instance for loading SQL files.             If None, a default loader will be created.</p> <code>None</code> <code>resolve_paths</code> <code>bool</code> <p>Whether to resolve relative paths to absolute paths.           If True, relative paths in view URIs and attachment paths           will be resolved relative to the config file's directory.           For remote configs, this defaults to False.</p> <code>True</code> <code>filesystem</code> <code>Optional[Any]</code> <p>Optional fsspec filesystem object to use for remote operations.        If provided, this filesystem will be used instead of creating        a new one based on URI scheme. Useful for custom        authentication or advanced use cases.</p> <code>None</code> <p>Returns:</p> Type Description <p>A validated :class:<code>Config</code> object.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the file cannot be read, is not valid YAML/JSON, fails schema validation, contains unresolved <code>${env:VAR_NAME}</code> placeholders, or if SQL file loading fails.</p> Example <p>Load a catalog from <code>catalog.yaml</code>::</p> <pre><code>from duckalog import load_config\n\nconfig = load_config(\"catalog.yaml\")\nprint(len(config.views))\n</code></pre> <p>Load a catalog from S3::</p> <pre><code>config = load_config(\"s3://my-bucket/configs/catalog.yaml\")\nprint(len(config.views))\n</code></pre> <p>Load a catalog with custom filesystem::</p> <pre><code>import fsspec\nfs = fsspec.filesystem(\"s3\", key=\"key\", secret=\"secret\", anon=False)\nconfig = load_config(\"s3://my-bucket/configs/catalog.yaml\", filesystem=fs)\nprint(len(config.views))\n</code></pre>"},{"location":"reference/api/#duckalog.quote_ident","title":"<code>quote_ident(value)</code>","text":"<p>Quote a SQL identifier using double quotes.</p> <p>This helper wraps a string in double quotes and escapes any embedded double quotes according to SQL rules.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Identifier to quote (for example, a view or column name).</p> required <p>Returns:</p> Type Description <code>str</code> <p>The identifier wrapped in double quotes.</p> Example <p>quote_ident(\"events\") '\"events\"'</p>"},{"location":"reference/api/#duckalog.quote_literal","title":"<code>quote_literal(value)</code>","text":"<p>Quote a SQL string literal using single quotes.</p> <p>This helper wraps a string in single quotes and escapes any embedded single quotes according to SQL rules.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>String literal to quote (for example, a file path, secret, or connection string).</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string wrapped in single quotes with proper escaping.</p> Example <p>quote_literal(\"path/to/file.parquet\") \"'path/to/file.parquet'\" quote_literal(\"user's data\") \"'user''s data'\"</p>"},{"location":"reference/api/#duckalog.render_options","title":"<code>render_options(options)</code>","text":"<p>Render a mapping of options into scan-function arguments.</p> <p>The resulting string is suitable for appending to a <code>*_scan</code> function call. Keys are sorted alphabetically to keep output deterministic.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>dict[str, Any]</code> <p>Mapping of option name to value (str, bool, int, or float).</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string that starts with <code>,</code> when options are present (for example,</p> <code>str</code> <p><code>\", hive_partitioning=TRUE\"</code>) or an empty string when no options</p> <code>str</code> <p>are provided.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If a value has a type that cannot be rendered safely.</p>"},{"location":"reference/api/#duckalog.validate_config","title":"<code>validate_config(config_path)</code>","text":"<p>Validate a configuration file without touching DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is missing, malformed, or does not satisfy the schema and interpolation rules.</p> Example <p>from duckalog import validate_config validate_config(\"catalog.yaml\")  # raises on invalid config</p>"},{"location":"reference/api/#duckalog.validate_generated_config","title":"<code>validate_generated_config(content, format='yaml')</code>","text":"<p>Validate that generated configuration content can be loaded successfully.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Configuration content as string.</p> required <code>format</code> <code>ConfigFormat</code> <p>Format of the content ('yaml' or 'json').</p> <code>'yaml'</code> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration cannot be loaded or is invalid.</p>"}]}