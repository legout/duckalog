{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Duckalog Documentation","text":"<p>Welcome to the Duckalog documentation. Duckalog is a Python library and CLI for building DuckDB catalogs from declarative YAML/JSON configuration files.</p> <p>Use these docs to:</p> <ul> <li>Understand the core concepts behind Duckalog configs.</li> <li>Get started with the CLI and Python API.</li> <li>Find API reference information generated from the source code.</li> <li>Learn about the system architecture and design patterns.</li> </ul> <p>For a deeper product and technical description, see the Architecture in <code>architecture.md</code>.</p>"},{"location":"#quick-start-guide-a-realistic-example","title":"Quick Start Guide: A Realistic Example","text":"<p>Let's walk through a typical analytics scenario where you need to combine data from multiple sources: Parquet files in S3, a reference database (DuckDB), and an Iceberg table. We'll create a unified catalog that joins these datasources.</p>"},{"location":"#step-1-set-up-your-configuration","title":"Step 1: Set up your configuration","text":"<p>First, create a comprehensive config file called <code>analytics_catalog.yaml</code>:</p> <pre><code>version: 1\n\nduckdb:\n  database: analytics.duckdb\n  pragmas:\n    - \"SET memory_limit='2GB'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n\n# Attach our reference data database\nattachments:\n  duckdb:\n    - alias: refdata\n      path: ./reference_data.duckdb\n      read_only: true\n\n# Configure Iceberg catalog access\niceberg_catalogs:\n  - name: analytics_warehouse\n    catalog_type: rest\n    uri: \"https://iceberg-catalog.example.com\"\n    warehouse: \"s3://our-warehouse/analytics/\"\n    options:\n      token: \"${env:ICEBERG_TOKEN}\"\n\nviews:\n  # Raw events from S3 Parquet files\n  - name: raw_events\n    source: parquet\n    uri: \"s3://our-bucket/events/*.parquet\"\n\n  # Product data from Iceberg\n  - name: products\n    source: iceberg\n    catalog: analytics_warehouse\n    table: analytics.products\n\n  # Customer reference data from attached database\n  - name: customers\n    source: duckdb\n    database: refdata\n    table: customers\n\n  # Enhanced events with joined data\n  - name: enhanced_events\n    sql: |\n      SELECT \n        e.event_id,\n        e.timestamp,\n        e.user_id,\n        e.event_type,\n        c.name as customer_name,\n        c.segment as customer_segment,\n        p.category as product_category,\n        p.price as product_price\n      FROM raw_events e\n      JOIN customers c ON e.customer_id = c.id\n      LEFT JOIN products p ON e.product_id = p.id\n\n  # Daily aggregation\n  - name: daily_metrics\n    sql: |\n      SELECT \n        DATE(timestamp) as event_date,\n        event_type,\n        COUNT(*) as event_count,\n        COUNT(DISTINCT user_id) as unique_users,\n        AVG(product_price) as avg_product_value\n      FROM enhanced_events\n      GROUP BY DATE(timestamp), event_type\n</code></pre>"},{"location":"#step-2-set-environment-variables","title":"Step 2: Set environment variables","text":"<p>Before running Duckalog, set the required environment variables:</p> <pre><code>export AWS_ACCESS_KEY_ID=\"your_aws_key\"\nexport AWS_SECRET_ACCESS_KEY=\"your_aws_secret\"\nexport ICEBERG_TOKEN=\"your_iceberg_token\"\n</code></pre>"},{"location":"#step-3-validate-and-build-your-catalog","title":"Step 3: Validate and build your catalog","text":"<p>First, let's validate the configuration to catch any issues early:</p> <pre><code>duckalog validate analytics_catalog.yaml\n</code></pre> <p>If validation passes, build the catalog:</p> <pre><code>duckalog build analytics_catalog.yaml\n</code></pre> <p>This will create the <code>analytics.duckdb</code> file with all your views properly configured.</p>"},{"location":"#step-4-use-your-catalog","title":"Step 4: Use your catalog","text":"<p>Now you can query your unified data source:</p> <pre><code># Connect directly with DuckDB CLI\nduckdb analytics.duckdb -c \"SELECT * FROM daily_metrics ORDER BY event_date DESC LIMIT 10\"\n\n# Or use duckalog to generate SQL for inspection\nduckalog generate-sql analytics_catalog.yaml --output create_views.sql\n</code></pre>"},{"location":"#step-5-programmatically-interact-with-your-catalog","title":"Step 5: Programmatically interact with your catalog","text":"<p>You can also use the Python API for more advanced workflows:</p> <pre><code>from duckalog import build_catalog, generate_sql, load_config\nimport pandas as pd\nimport duckdb\n\n# Load configuration\nconfig = load_config(\"analytics_catalog.yaml\")\n\n# Generate SQL without execution\nsql = generate_sql(\"analytics_catalog.yaml\")\nprint(\"SQL to be executed:\")\nprint(sql)\n\n# Build catalog programmatically\nbuild_catalog(\"analytics_catalog.yaml\")\n\n# Query with DuckDB directly\ncon = duckdb.connect(\"analytics.duckdb\")\n\n# Get daily metrics as DataFrame\nmetrics_df = con.execute(\"SELECT * FROM daily_metrics WHERE event_date &gt;= CURRENT_DATE - INTERVAL 7 DAYS\").df()\nprint(f\"Last 7 days of metrics: {metrics_df}\")\n\n# Get enhanced events for a specific user\nuser_events = con.execute(\"SELECT * FROM enhanced_events WHERE user_id = 'user123'\").df()\nprint(f\"User events: {user_events}\")\n\ncon.close()\n</code></pre> <p>This example demonstrates how Duckalog can help you create a cohesive analytics layer that combines data from multiple sources, applies business logic, and provides a single point of access for your data consumers.</p>"},{"location":"#quick-reference","title":"Quick Reference","text":""},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>System Architecture - Understanding Duckalog's design, components, and patterns</li> <li>Quick Start - Getting started guide and examples</li> </ul>"},{"location":"#install","title":"Install","text":"<pre><code>pip install duckalog\n</code></pre>"},{"location":"#basic-commands","title":"Basic Commands","text":"<pre><code># Build a catalog from configuration\nduckalog build catalog.yaml\n\n# Generate SQL without executing it\nduckalog generate-sql catalog.yaml --output create_views.sql\n\n# Validate configuration syntax\nduckalog validate catalog.yaml\n</code></pre>"},{"location":"#python-api","title":"Python API","text":"<pre><code>from duckalog import build_catalog, generate_sql, validate_config\n\n# Build or update a catalog\nbuild_catalog(\"catalog.yaml\")\n\n# Generate SQL without execution\nsql = generate_sql(\"catalog.yaml\")\n\n# Validate configuration\nvalidate_config(\"catalog.yaml\")\n</code></pre> <p>For a deeper product and technical description, see the PRD in <code>docs/PRD_Spec.md</code>.</p>"},{"location":"architecture/","title":"Duckalog Architecture","text":"<p>Duckalog is a Python library and CLI for building DuckDB catalogs from declarative YAML/JSON configuration files. This document provides a comprehensive architectural overview of the system, its components, and how they work together to transform configuration files into functional DuckDB catalogs.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<p>Duckalog follows a config-driven, idempotent architecture that transforms declarative configuration into functional DuckDB catalogs. The system is designed around separation of concerns, with clear boundaries between configuration loading, validation, SQL generation, and catalog execution.</p>"},{"location":"architecture/#core-philosophy","title":"Core Philosophy","text":"<ul> <li>Configuration as Code: Catalogs are defined in version-controllable YAML/JSON files</li> <li>Idempotent Operations: Running the same config produces the same catalog every time</li> <li>Multi-Source Integration: Unified interface for S3 Parquet, Delta Lake, Iceberg, and relational databases</li> <li>Environment-Driven: Credentials and connection details sourced from environment variables</li> <li>Separation of Concerns: Clear boundaries between configuration, validation, generation, and execution</li> </ul>"},{"location":"architecture/#high-level-system-architecture","title":"High-Level System Architecture","text":"<pre><code>graph TB\n    subgraph \"External Systems\"\n        S3[S3 Object Storage&lt;br/&gt;Parquet Files]\n        DELTA[Delta Lake&lt;br/&gt;Tables]\n        ICEBERG[Iceberg&lt;br/&gt;Catalog/Tables]\n        DUCKDB_EXT[External DuckDB&lt;br/&gt;Files]\n        SQLITE[SQLite&lt;br/&gt;Databases]\n        POSTGRES[PostgreSQL&lt;br/&gt;Databases]\n    end\n\n    subgraph \"Duckalog System\"\n        CLI[CLI Interface&lt;br/&gt;Commands &amp; Validation]\n        CONFIG[Config Loading&lt;br/&gt;YAML/JSON + Environment]\n        MODEL[Pydantic Models&lt;br/&gt;Validation &amp; Types]\n        SQLGEN[SQL Generation&lt;br/&gt;CREATE VIEW Statements]\n        ENGINE[DuckDB Engine&lt;br/&gt;Execution &amp; Connections]\n    end\n\n    subgraph \"Output\"\n        CATALOG[(DuckDB Catalog&lt;br/&gt;Views &amp; Attachments)]\n    end\n\n    CLI --&gt; CONFIG\n    CONFIG --&gt; MODEL\n    MODEL --&gt; SQLGEN\n    SQLGEN --&gt; ENGINE\n    ENGINE --&gt; CATALOG\n\n    ENGINE -.-&gt;|S3 Parquet Access| S3\n    ENGINE -.-&gt;|Delta Tables| DELTA\n    ENGINE -.-&gt;|Iceberg Integration| ICEBERG\n    ENGINE -.-&gt;|DuckDB Attachments| DUCKDB_EXT\n    ENGINE -.-&gt;|SQLite Connections| SQLITE\n    ENGINE -.-&gt;|PostgreSQL Access| POSTGRES</code></pre>"},{"location":"architecture/#core-architecture-components","title":"Core Architecture Components","text":"<p>The system consists of five primary modules that work together to process configurations and create DuckDB catalogs:</p> <pre><code>graph TB\n    subgraph \"Duckalog Core Components\"\n        CLI[CLI Module&lt;br/&gt;Command parsing &amp; dispatch]\n        CONFIG[Config Module&lt;br/&gt;Loading &amp; env interpolation]\n        MODEL[Model Module&lt;br/&gt;Pydantic validation]\n        SQLGEN[SQL Generation&lt;br/&gt;CREATE VIEW statements]\n        ENGINE[Engine Module&lt;br/&gt;DuckDB execution]\n\n        CLI -.-&gt;|typer commands| CONFIG\n        CONFIG -.-&gt;|validated dict| MODEL\n        MODEL -.-&gt;|typed objects| SQLGEN\n        SQLGEN -.-&gt;|SQL strings| ENGINE\n        ENGINE -.-&gt;|executed SQL| DB[(DuckDB Catalog)]\n    end\n\n    subgraph \"Detailed Component Interactions\"\n        direction LR\n        CONFIG_A[File I/O]\n        CONFIG_B[Env Interpolation]\n        CONFIG_C[Error Handling]\n\n        MODEL_A[Schema Validation]\n        MODEL_B[Type Conversion]\n        MODEL_C[Business Rules]\n\n        SQLGEN_A[Source Detection]\n        SQLGEN_B[Template Selection]\n        SQLGEN_C[Statement Building]\n\n        ENGINE_A[Connection Mgmt]\n        ENGINE_B[Attachment Setup]\n        ENGINE_C[View Creation]\n        ENGINE_D[Transaction Control]\n    end</code></pre>"},{"location":"architecture/#1-configuration-module-configpy","title":"1. Configuration Module (<code>config.py</code>)","text":"<p>Responsibilities: - Load YAML/JSON configuration files - Perform environment variable interpolation - Parse and prepare raw configuration data - Handle file I/O and error management</p> <p>Key Features: - Supports both YAML and JSON formats - Environment variable substitution using <code>${env:VAR_NAME}</code> pattern - Recursive traversal of configuration structures - Comprehensive error handling with descriptive messages</p> <p>Example Flow: <pre><code>sequenceDiagram\n    participant CLI\n    participant Config\n    participant File\n    participant Env\n\n    CLI-&gt;&gt;Config: load_config(\"catalog.yaml\")\n    Config-&gt;&gt;File: Read file content\n    File--&gt;&gt;Config: Raw YAML content\n    Config-&gt;&gt;Config: Parse YAML/JSON\n    Config-&gt;&gt;Config: interpolate_env(raw_dict)\n    Config-&gt;&gt;Env: Get ${env:VAR} values\n    Env--&gt;&gt;Config: Real values\n    Config--&gt;&gt;CLI: Validated Config object</code></pre></p>"},{"location":"architecture/#2-model-module-modelpy","title":"2. Model Module (<code>model.py</code>)","text":"<p>Responsibilities: - Define Pydantic models for configuration schema - Provide data validation and type checking - Ensure configuration consistency - Support extensibility for new configuration types</p> <p>Core Models: - <code>DuckDBConfig</code>: Database file and session settings - <code>AttachmentConfig</code>: External database connections (DuckDB, SQLite, Postgres) - <code>IcebergCatalogConfig</code>: Iceberg catalog connections - <code>ViewConfig</code>: Individual view definitions and source specifications - <code>Config</code>: Root configuration aggregating all components</p> <p>Validation Flow: <pre><code>flowchart LR\n    A[Raw Config Dict] --&gt; B[Pydantic Validation]\n    B --&gt; C{Valid?}\n    C --&gt;|Yes| D[Typed Config Object]\n    C --&gt;|No| E[ConfigError]\n    D --&gt; F[Schema Validation]\n    F --&gt; G[Business Rule Validation]\n    G --&gt; H[Validated Config]</code></pre></p>"},{"location":"architecture/#3-sql-generation-module-sqlgenpy","title":"3. SQL Generation Module (<code>sqlgen.py</code>)","text":"<p>Responsibilities: - Transform typed configuration objects into SQL statements - Generate <code>CREATE VIEW</code> statements for different source types - Handle SQL escaping and identifier quoting - Support complex view definitions with joins and aggregations</p> <p>Source Types Supported: - Parquet: S3-based Parquet file views - Delta Lake: Delta table references - Iceberg: Iceberg table and catalog views - Database: Attached DuckDB/SQLite/Postgres tables - SQL: Raw SQL query views</p> <p>SQL Generation Process: <pre><code>graph TD\n    A[ViewConfig] --&gt; B{Source Type Detection}\n    B --&gt;|parquet| C[Generate Parquet CREATE VIEW]\n    B --&gt;|delta| D[Generate Delta CREATE VIEW]\n    B --&gt;|iceberg| E[Generate Iceberg CREATE VIEW]\n    B --&gt;|duckdb/sqlite/postgres| F[Generate DB CREATE VIEW]\n    B --&gt;|sql| G[Validate &amp; Pass Through SQL]\n\n    C --&gt; H[Add Options &amp; Settings]\n    D --&gt; H\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n    H --&gt; I[Complete SQL Statement]\n\n    subgraph \"Source-Specific Logic\"\n        J[URI Processing]\n        K[Authentication Config]\n        L[Table Reference Resolution]\n        M[Option Normalization]\n    end\n\n    C -.-&gt; J\n    D -.-&gt; J\n    E -.-&gt; L\n    F -.-&gt; L</code></pre></p>"},{"location":"architecture/#4-engine-module-enginepy","title":"4. Engine Module (<code>engine.py</code>)","text":"<p>Responsibilities: - Manage DuckDB connections and sessions - Set up external attachments and catalogs - Execute generated SQL statements - Handle transaction management and error recovery</p> <p>Key Operations: - Connection Management: Open and maintain DuckDB connections - Attachment Setup: Configure DuckDB, SQLite, and Postgres connections - Catalog Configuration: Set up Iceberg catalogs with proper authentication - View Execution: Apply generated SQL to create views in the catalog - Session Management: Configure pragmas and session settings</p> <p>Engine Workflow: <pre><code>sequenceDiagram\n    participant Config\n    participant Engine\n    participant DuckDB\n    participant External\n\n    Config-&gt;&gt;Engine: execute(config)\n    Engine-&gt;&gt;DuckDB: Open connection\n    Engine-&gt;&gt;DuckDB: Apply pragmas\n    Engine-&gt;&gt;DuckDB: Install extensions\n    Config-&gt;&gt;Engine: Process attachments\n    Engine-&gt;&gt;DuckDB: ATTACH external DBs\n    Config-&gt;&gt;Engine: Process catalogs\n    Engine-&gt;&gt;DuckDB: CREATE iceberg catalogs\n    Config-&gt;&gt;Engine: Process views\n    loop For each view\n        Engine-&gt;&gt;SQLGEN: generate_sql(view)\n        SQLGEN--&gt;&gt;Engine: SQL statement\n        Engine-&gt;&gt;DuckDB: Execute SQL\n        DuckDB--&gt;&gt;Engine: Success/Error\n    end\n    Engine--&gt;&gt;Config: Complete catalog</code></pre></p>"},{"location":"architecture/#5-cli-module-clipy","title":"5. CLI Module (<code>cli.py</code>)","text":"<p>Responsibilities: - Provide command-line interface for users - Parse command-line arguments and options - Dispatch to appropriate library functions - Handle user input validation and error reporting</p> <p>Available Commands: - <code>build</code>: Create or update a DuckDB catalog from configuration - <code>generate-sql</code>: Generate SQL statements without executing them - <code>validate</code>: Validate configuration files for syntax and schema correctness</p>"},{"location":"architecture/#data-flow-architecture","title":"Data Flow Architecture","text":"<p>The complete data flow from configuration file to functional catalog follows this pipeline:</p> <pre><code>flowchart TD\n    A[YAML/JSON Config File] --&gt; B[Config Loading]\n    B --&gt; C[Environment Interpolation]\n    C --&gt; D[Pydantic Validation]\n    D --&gt; E[Business Rule Validation]\n    E --&gt; F[SQL Generation]\n    F --&gt; G[Catalog Building]\n    G --&gt; H[(DuckDB Catalog)]\n\n    subgraph \"Validation Stages\"\n        C --&gt; C1[Syntax Check]\n        C1 --&gt; C2[Schema Validation]\n        C2 --&gt; C3[Reference Validation]\n    end\n\n    subgraph \"SQL Generation\"\n        F --&gt; F1[Source Type Detection]\n        F1 --&gt; F2[Template Selection]\n        F2 --&gt; F3[Statement Building]\n        F3 --&gt; F4[Option Processing]\n    end\n\n    subgraph \"Catalog Building\"\n        G --&gt; G1[Connection Setup]\n        G1 --&gt; G2[Attachment Configuration]\n        G2 --&gt; G3[Catalog Setup]\n        G3 --&gt; G4[View Creation]\n    end</code></pre>"},{"location":"architecture/#complete-data-flow-architecture","title":"Complete Data Flow Architecture","text":"<p>Here's the complete end-to-end data flow from configuration file to functional DuckDB catalog:</p> <pre><code>sequenceDiagram\n    participant User\n    participant CLI\n    participant Config\n    participant Model\n    participant SQLGen\n    participant Engine\n    participant DuckDB\n    participant External\n\n    User-&gt;&gt;CLI: duckalog build config.yaml\n    CLI-&gt;&gt;Config: load_config()\n    Config-&gt;&gt;External: Read file\n    Config-&gt;&gt;External: Environment variables\n    Config-&gt;&gt;Model: Raw config dict\n\n    par Parallel Processing\n        Model-&gt;&gt;Model: Schema validation\n        Model-&gt;&gt;Model: Type conversion\n        Model-&gt;&gt;Model: Business rule validation\n    end\n\n    Model-&gt;&gt;SQLGen: Validated ViewConfig list\n\n    par For Each View\n        SQLGen-&gt;&gt;SQLGen: Detect source type\n        SQLGen-&gt;&gt;SQLGen: Generate SQL statement\n        SQLGen-&gt;&gt;SQLGen: Apply source-specific options\n        SQLGen-&gt;&gt;Engine: SQL string\n    end\n\n    Engine-&gt;&gt;DuckDB: Open connection\n    Engine-&gt;&gt;DuckDB: Apply pragmas\n    Engine-&gt;&gt;External: Setup attachments (if any)\n    Engine-&gt;&gt;External: Setup catalogs (if any)\n\n    par For Each Generated SQL\n        Engine-&gt;&gt;DuckDB: Execute CREATE VIEW\n        DuckDB--&gt;&gt;Engine: Success/Error\n    end\n\n    Engine--&gt;&gt;Model: Completion status\n    Model--&gt;&gt;CLI: Success/Error\n    CLI--&gt;&gt;User: Operation complete</code></pre>"},{"location":"architecture/#design-patterns-and-architectural-decisions","title":"Design Patterns and Architectural Decisions","text":""},{"location":"architecture/#1-separation-of-concerns","title":"1. Separation of Concerns","text":"<p>Duckalog strictly separates different responsibilities into distinct modules:</p> <ul> <li>Configuration Layer: File I/O, parsing, environment interpolation</li> <li>Validation Layer: Schema validation, type checking, business rules</li> <li>Generation Layer: SQL statement creation, template management</li> <li>Execution Layer: Database operations, connection management</li> <li>Interface Layer: CLI commands, user interaction</li> </ul> <p>This separation provides several benefits: - Easier testing and debugging - Independent module evolution - Clear extension points - Reduced coupling between components</p>"},{"location":"architecture/#2-config-driven-design","title":"2. Config-Driven Design","text":"<p>The entire system revolves around declarative configuration:</p> <p>Benefits: - Reproducibility: Same config always produces same results - Version Control: Configuration files can be tracked in Git - Testing: Easy to create test scenarios with different configs - Collaboration: Non-developers can understand and modify configurations - Documentation: Config files serve as living documentation</p> <p>Extensibility: New source types can be added by: 1. Extending the <code>ViewConfig</code> Pydantic model 2. Adding SQL generation logic for the new source type 3. Implementing engine-side setup if needed 4. Updating documentation and examples</p>"},{"location":"architecture/#3-idempotent-operations","title":"3. Idempotent Operations","text":"<p>All catalog building operations are designed to be idempotent:</p> <ul> <li>View Replacement: <code>CREATE OR REPLACE VIEW</code> ensures consistent results</li> <li>Attachment Management: Consistent attachment procedures regardless of current state</li> <li>Schema Evolution: Config changes are applied predictably</li> <li>Rollback Safety: Failed operations leave the catalog in a consistent state</li> </ul>"},{"location":"architecture/#4-layered-architecture","title":"4. Layered Architecture","text":"<p>The system follows a strict layered approach:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   CLI Interface     \u2502  \u2190 User interaction, command parsing\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Config Layer      \u2502  \u2190 File I/O, environment handling\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Validation Layer  \u2502  \u2190 Schema validation, type checking\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Generation Layer  \u2502  \u2190 SQL creation, template management\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Execution Layer   \u2502  \u2190 Database operations, connections\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   External Systems  \u2502  \u2190 DuckDB, S3, databases, catalogs\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Each layer depends only on the layer directly below it, ensuring clear dependencies and testability.</p>"},{"location":"architecture/#5-error-handling-and-logging-patterns","title":"5. Error Handling and Logging Patterns","text":"<p>Error Handling Strategy: - Fail Fast: Validate configuration early to catch issues quickly - Descriptive Errors: Provide actionable error messages with context - Graceful Degradation: Continue processing non-dependent items when possible - Error Categorization: Different exception types for different failure modes</p> <p>Logging Approach: - Structured Logging: Use consistent log formats and levels - Security Conscious: Never log sensitive information (passwords, tokens) - Debug Support: Detailed logging available for troubleshooting - User-Friendly: Important operations logged at appropriate levels</p>"},{"location":"architecture/#configuration-schema-architecture","title":"Configuration Schema Architecture","text":"<p>The configuration schema follows a hierarchical structure:</p> <pre><code>graph TD\n    ROOT[Config Root&lt;br/&gt;version, duckdb, attachments, views] --&gt; DB[DuckDB Config&lt;br/&gt;database, pragmas]\n    ROOT --&gt; ATTACH[Attachments Config&lt;br/&gt;duckdb, sqlite, postgres]\n    ROOT --&gt; ICEBERG[Iceberg Catalogs&lt;br/&gt;catalog configs]\n    ROOT --&gt; VIEWS[Views List&lt;br/&gt;view definitions]\n\n    ATTACH --&gt; DUCKDB_ATTACH[DuckDB Attachment&lt;br/&gt;alias, path, read_only]\n    ATTACH --&gt; SQLITE_ATTACH[SQLite Attachment&lt;br/&gt;alias, path]\n    ATTACH --&gt; POSTGRES_ATTACH[Postgres Attachment&lt;br/&gt;alias, host, port, user, password]\n\n    ICEBERG --&gt; IC_CONFIG[Iceberg Catalog Config&lt;br/&gt;name, catalog_type, uri, warehouse]\n\n    VIEWS --&gt; PARQUET_VIEW[Parquet View&lt;br/&gt;source: parquet, uri, options]\n    VIEWS --&gt; DELTA_VIEW[Delta View&lt;br/&gt;source: delta, uri]\n    VIEWS --&gt; ICEBERG_VIEW[Iceberg View&lt;br/&gt;source: iceberg, catalog, table]\n    VIEWS --&gt; DB_VIEW[Database View&lt;br/&gt;source: duckdb/sqlite/postgres, database, table]\n    VIEWS --&gt; SQL_VIEW[SQL View&lt;br/&gt;source: sql, query]</code></pre>"},{"location":"architecture/#component-dependency-graph","title":"Component Dependency Graph","text":"<p>Understanding the dependencies between components helps in maintenance and extension:</p> <pre><code>graph TD\n    subgraph \"Layer 1: Interface\"\n        CLI[CLI Module&lt;br/&gt;Commands: build, validate, generate-sql]\n    end\n\n    subgraph \"Layer 2: Configuration\"\n        CONFIG[Config Module&lt;br/&gt;File loading &amp; env interpolation]\n    end\n\n    subgraph \"Layer 3: Validation\"\n        MODEL[Model Module&lt;br/&gt;Pydantic validation &amp; types]\n    end\n\n    subgraph \"Layer 4: Generation\"\n        SQLGEN[SQL Generation&lt;br/&gt;Statement creation]\n    end\n\n    subgraph \"Layer 5: Execution\"\n        ENGINE[Engine Module&lt;br/&gt;DuckDB operations]\n    end\n\n    subgraph \"External Dependencies\"\n        DUCKDB[DuckDB Engine]\n        YAML[YAML/JSON Parser]\n        PYDANTIC[Pydantic Validator]\n        TYPER[Typer CLI Framework]\n    end\n\n    CLI --&gt; CONFIG\n    CONFIG --&gt; MODEL\n    MODEL --&gt; SQLGEN\n    SQLGEN --&gt; ENGINE\n\n    CONFIG -.-&gt; YAML\n    MODEL -.-&gt; PYDANTIC\n    CLI -.-&gt; TYPER\n    ENGINE -.-&gt; DUCKDB</code></pre>"},{"location":"architecture/#dependency-rules","title":"Dependency Rules:","text":"<ol> <li>No backward dependencies - higher layers never depend on lower layers</li> <li>Clear interfaces - each layer exposes well-defined interfaces</li> <li>Minimal coupling - components only know about their direct dependencies</li> <li>Testable units - each layer can be tested independently</li> </ol>"},{"location":"architecture/#extension-patterns","title":"Extension Patterns","text":""},{"location":"architecture/#adding-new-source-types","title":"Adding New Source Types","text":"<p>To extend Duckalog with a new data source type:</p> <ol> <li> <p>Model Extension:    <pre><code>class NewSourceViewConfig(BaseModel):\n    name: str\n    source: Literal[\"new_source\"]\n    uri: str\n    options: Dict[str, Any] = Field(default_factory=dict)\n</code></pre></p> </li> <li> <p>SQL Generation:    <pre><code>def generate_new_source_sql(view: NewSourceViewConfig) -&gt; str:\n    # Generate appropriate CREATE VIEW statement\n    return f\"CREATE OR REPLACE VIEW {view.name} AS ...\"\n</code></pre></p> </li> <li> <p>Engine Integration (if needed):    <pre><code>def setup_new_source(engine, view: NewSourceViewConfig):\n    # Set up any required connections or configurations\n    pass\n</code></pre></p> </li> </ol>"},{"location":"architecture/#adding-new-attachment-types","title":"Adding New Attachment Types","text":"<p>Similar extension pattern for database attachments:</p> <ol> <li>Extend <code>AttachmentsConfig</code> model</li> <li>Add attachment setup logic in engine</li> <li>Update SQL generation if needed</li> <li>Add validation rules</li> </ol>"},{"location":"architecture/#performance-and-scalability-considerations","title":"Performance and Scalability Considerations","text":""},{"location":"architecture/#current-architecture-supports","title":"Current Architecture Supports:","text":"<ul> <li>Large Configuration Files: Efficient parsing and validation</li> <li>Multiple Views: Batch processing and optimization</li> <li>Concurrent Operations: Thread-safe where appropriate</li> <li>Memory Management: Streaming and chunked processing where needed</li> </ul>"},{"location":"architecture/#scaling-patterns","title":"Scaling Patterns:","text":"<ul> <li>Horizontal Scaling: Multiple catalogs can be processed independently</li> <li>Vertical Scaling: DuckDB's in-memory processing for complex queries</li> <li>External Optimization: Leverage underlying system optimizations (S3, databases)</li> </ul>"},{"location":"architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"architecture/#security-principles","title":"Security Principles:","text":"<ul> <li>Zero Secrets in Config: All sensitive data via environment variables</li> <li>Connection Security: SSL/TLS support for external connections</li> <li>Access Control: DuckDB's built-in security features</li> <li>Audit Trail: Config-driven approach provides built-in change tracking</li> </ul>"},{"location":"architecture/#security-measures","title":"Security Measures:","text":"<ul> <li>Environment variable validation</li> <li>Secure credential handling</li> <li>Connection security options (SSL modes)</li> <li>No credential logging or exposure</li> </ul>"},{"location":"architecture/#development-and-testing-architecture","title":"Development and Testing Architecture","text":""},{"location":"architecture/#testing-strategy","title":"Testing Strategy:","text":"<ul> <li>Unit Tests: Individual module functionality</li> <li>Integration Tests: End-to-end catalog building</li> <li>Configuration Tests: Schema validation and parsing</li> <li>SQL Generation Tests: Output verification for different source types</li> </ul>"},{"location":"architecture/#development-workflow","title":"Development Workflow:","text":"<ol> <li>Configuration-driven development</li> <li>Test-first approach for new features</li> <li>Documentation integration</li> <li>Continuous validation against specifications</li> </ol>"},{"location":"architecture/#future-architecture-considerations","title":"Future Architecture Considerations","text":""},{"location":"architecture/#potential-extensions","title":"Potential Extensions:","text":"<ul> <li>Plugin System: Dynamic loading of new source types</li> <li>Caching Layer: Configuration and SQL result caching</li> <li>Monitoring Integration: Metrics and observability</li> <li>Multi-Catalog Management: Orchestrating multiple catalog deployments</li> </ul>"},{"location":"architecture/#architectural-evolution","title":"Architectural Evolution:","text":"<p>The current architecture is designed to accommodate these future needs without major restructuring, thanks to its separation of concerns and extensibility patterns.</p>"},{"location":"architecture/#conclusion","title":"Conclusion","text":"<p>Duckalog's architecture provides a robust, maintainable, and extensible foundation for building DuckDB catalogs from declarative configurations. The separation of concerns, config-driven design, and idempotent operations make it suitable for both development and production use cases, while the clear extension patterns support future growth and adaptation to new data sources and requirements.</p>"},{"location":"architecture/#getting-started-with-the-architecture","title":"Getting Started with the Architecture","text":""},{"location":"architecture/#for-new-developers","title":"For New Developers","text":"<ol> <li>Start with the system overview to understand Duckalog's purpose and high-level design</li> <li>Review the component descriptions to understand each module's responsibilities</li> <li>Follow the data flow to see how configuration becomes a catalog</li> <li>Examine the design patterns to understand architectural decisions</li> <li>Look at extension examples if you need to add new functionality</li> </ol>"},{"location":"architecture/#for-contributors","title":"For Contributors","text":"<ul> <li>Code contributions should respect the separation of concerns</li> <li>New source types follow the documented extension patterns</li> <li>Architecture changes require OpenSpec proposals</li> <li>Documentation updates should maintain consistency across documents</li> </ul>"},{"location":"architecture/#for-system-integrators","title":"For System Integrators","text":"<ul> <li>API stability is maintained through well-defined interfaces</li> <li>Configuration evolution follows semantic versioning principles</li> <li>Extension points are documented and tested</li> <li>Error handling provides actionable feedback for troubleshooting</li> </ul>"},{"location":"architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>User Guide: How to use Duckalog in practice</li> <li>API Reference: Complete API documentation</li> <li>Examples: See the <code>examples/</code> directory for configuration samples</li> </ul>"},{"location":"examples/","title":"Duckalog Examples","text":"<p>Welcome to the Duckalog examples collection! These practical examples demonstrate real-world usage patterns and help you get started with different Duckalog configurations.</p>"},{"location":"examples/#choosing-an-example","title":"Choosing an Example","text":"<p>Use this guide to find the right example for your use case:</p>"},{"location":"examples/#im-getting-started-with-duckalog","title":"I'm getting started with Duckalog","text":"<p>\u2192 Start with: Simple Parquet Example - Learn the basics of creating DuckDB views over Parquet files - Perfect for simple analytics without complex joins - Covers local files and S3 storage</p>"},{"location":"examples/#i-need-to-combine-multiple-local-databases","title":"I need to combine multiple local databases","text":"<p>\u2192 Use: Local Attachments Example - Attach DuckDB and SQLite databases - Join data across different local databases - Learn read-only attachment patterns</p>"},{"location":"examples/#i-have-data-in-multiple-sources-parquet-databases-cloud-storage","title":"I have data in multiple sources (Parquet + databases + cloud storage)","text":"<p>\u2192 Follow: Multi-Source Analytics Example - Comprehensive example with S3, PostgreSQL, Iceberg, and local databases - Real-world analytics workflow - Complex joins and business logic</p>"},{"location":"examples/#i-need-to-deploy-configs-across-different-environments","title":"I need to deploy configs across different environments","text":"<p>\u2192 Read: Environment Variables Example - Secure credential management - Development, staging, and production configurations - Docker and Kubernetes deployment patterns</p>"},{"location":"examples/#i-want-to-fine-tune-duckdb-performance-and-behavior","title":"I want to fine-tune DuckDB performance and behavior","text":"<p>\u2192 Explore: DuckDB Settings Example - Configure session-level settings beyond pragmas - Optimize threading, memory, and caching - Control DuckDB features and progress output</p>"},{"location":"examples/#i-need-to-manage-credentials-for-cloud-services-and-databases","title":"I need to manage credentials for cloud services and databases","text":"<p>\u2192 Use: DuckDB Secrets Example - Secure credential management for S3, Azure, GCS, and databases - Support for environment variable interpolation - Persistent and temporary secrets with scoping - Integration with attachments and Iceberg catalogs</p>"},{"location":"examples/#quick-comparison","title":"Quick Comparison","text":"Example Difficulty Data Sources Key Learning Simple Parquet \ud83d\udfe2 Beginner Parquet files Local Attachments \ud83d\udfe1 Intermediate DuckDB/SQLite Multi-Source Analytics \ud83d\udd34 Advanced Multiple sources Environment Variables \ud83d\udfe1 Intermediate Any DuckDB Settings \ud83d\udfe1 Intermediate Any DuckDB Secrets \ud83d\udfe1 Intermediate Any"},{"location":"examples/#prerequisites-for-all-examples","title":"Prerequisites for All Examples","text":""},{"location":"examples/#required-software","title":"Required Software","text":"<ul> <li>Python 3.9+ with Duckalog installed:   <pre><code>pip install duckalog\n</code></pre></li> </ul>"},{"location":"examples/#optional-but-recommended","title":"Optional but Recommended","text":"<ul> <li> <p>DuckDB CLI for interactive querying:   <pre><code># Install DuckDB CLI (optional)\n# Visit: https://duckdb.org/docs/installation/\n</code></pre></p> </li> <li> <p>AWS CLI (for S3 examples):   <pre><code>pip install awscli\naws configure\n</code></pre></p> </li> </ul>"},{"location":"examples/#example-categories","title":"Example Categories","text":""},{"location":"examples/#by-data-source","title":"By Data Source","text":"<p>Parquet Files Only - Simple Parquet - Perfect starting point - Multi-Source Analytics - Includes Parquet with other sources</p> <p>Local Databases - Local Attachments - DuckDB and SQLite focus - Multi-Source Analytics - Local databases with cloud sources</p> <p>Cloud Storage &amp; Data Lakes - Simple Parquet - S3 configuration - Multi-Source Analytics - S3 + Iceberg catalogs - Environment Variables - Cloud credential management</p> <p>Enterprise/Production - Multi-Source Analytics - Production-ready patterns - Environment Variables - Deployment and security</p>"},{"location":"examples/#by-use-case","title":"By Use Case","text":"<p>Data Analytics - Simple Parquet - Basic analytics - Local Attachments - Cross-database analytics - Multi-Source Analytics - Enterprise analytics</p> <p>Data Integration - Local Attachments - Local data unification - Multi-Source Analytics - Cloud + local integration</p> <p>Development &amp; Deployment - Environment Variables - Environment-specific configs - Multi-Source Analytics - Production deployment patterns</p>"},{"location":"examples/#common-patterns-across-examples","title":"Common Patterns Across Examples","text":"<p>All examples demonstrate these important Duckalog concepts:</p> <ol> <li>Configuration Structure - Consistent YAML patterns</li> <li>View Composition - Building complex analytics from simple views</li> <li>Performance Optimization - Memory limits, threading, pragmas</li> <li>Error Handling - Validation and troubleshooting</li> <li>Best Practices - Security, maintainability, scalability</li> </ol>"},{"location":"examples/#getting-started","title":"Getting Started","text":"<ol> <li>Choose your example based on your use case above</li> <li>Read the prerequisites in your chosen example</li> <li>Follow the step-by-step guide provided in each example</li> <li>Experiment with the configurations to match your needs</li> <li>Combine patterns from multiple examples as needed</li> </ol>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<p>After working through examples:</p> <ul> <li>Read the User Guide in <code>../guides/index.md</code> for comprehensive documentation</li> <li>Explore the API Reference in <code>../reference/index.md</code> for detailed function documentation</li> <li>Review the Architecture in <code>../architecture.md</code> for high-level design details</li> <li>Join the community for questions and discussions</li> </ul>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>Have a great Duckalog pattern to share? Consider contributing:</p> <ol> <li>Create a new example following the patterns shown here</li> <li>Include clear explanations and real-world scenarios</li> <li>Add troubleshooting sections for common issues</li> <li>Ensure examples work with minimal setup</li> <li>Link from this index page</li> </ol>"},{"location":"examples/#need-help","title":"Need Help?","text":"<ul> <li>Configuration Issues: Check troubleshooting sections in examples</li> <li>API Questions: See API Reference</li> <li>General Usage: Review User Guide</li> <li>Technical Details: Read Architecture</li> </ul> <p>Choose an example above to get started, or explore them in order to build your Duckalog expertise progressively!</p>"},{"location":"examples/duckdb-secrets/","title":"DuckDB Secrets Example","text":"<p>This example demonstrates how to use DuckDB secrets in Duckalog to manage credentials for external services like S3, Azure, GCS, and databases. Secrets provide a secure way to handle authentication without hardcoding credentials in SQL.</p>"},{"location":"examples/duckdb-secrets/#when-to-use-secrets","title":"When to Use Secrets","text":"<p>Choose secrets when you need to: - Access cloud storage services (S3, Azure, GCS) with credentials - Connect to databases (PostgreSQL, MySQL) using authentication - Manage credentials across different environments securely - Support automatic credential fetching (credential chains) - Create persistent secrets that survive database restarts</p>"},{"location":"examples/duckdb-secrets/#basic-secret-configuration","title":"Basic Secret Configuration","text":""},{"location":"examples/duckdb-secrets/#s3-secret-with-static-credentials","title":"S3 Secret with Static Credentials","text":"<p>Create a file called <code>secrets-example.yaml</code>:</p> <pre><code>version: 1\n\nduckdb:\n  database: secrets_catalog.duckdb\n\n  # Extensions required for S3 access\n  install_extensions:\n    - httpfs\n\n  # Secrets for external services\n  secrets:\n    - type: s3\n      name: production_s3\n      key_id: AKIAIOSFODNN7EXAMPLE\n      secret: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n      region: us-west-2\n      endpoint: s3.amazonaws.com  # Optional: custom endpoint\n\nviews:\n  - name: sales_data\n    source: parquet\n    uri: \"s3://my-production-bucket/sales/*.parquet\"\n    description: \"Sales data from S3 with secret authentication\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#azure-storage-secret","title":"Azure Storage Secret","text":"<pre><code>version: 1\n\nduckdb:\n  database: azure_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  secrets:\n    - type: azure\n      name: azure_prod\n      provider: config\n      persistent: true\n      scope: 'prod/'\n      connection_string: DefaultEndpointsProtocol=https;AccountName=myaccount;AccountKey=mykey;EndpointSuffix=core.windows.net\n      # Alternative: Use tenant_id + account_name + secret\n      # tenant_id: my-tenant-id\n      # account_name: mystorageaccount\n      # secret: my-azure-secret\n\nviews:\n  - name: azure_logs\n    source: parquet\n    uri: \"azure://mycontainer/logs/*.parquet\"\n    description: \"Application logs from Azure storage\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#gcs-secret","title":"GCS Secret","text":"<pre><code>version: 1\n\nduckdb:\n  database: gcs_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  secrets:\n    - type: gcs\n      name: gcs_service_account\n      key_id: my-service-account@project.iam.gserviceaccount.com\n      secret: '-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKw...\\n-----END PRIVATE KEY-----'\n      endpoint: storage.googleapis.com\n\nviews:\n  - name: gcs_data\n    source: parquet\n    uri: \"gs://my-bucket/data/*.parquet\"\n    description: \"Data from Google Cloud Storage\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#advanced-secret-configurations","title":"Advanced Secret Configurations","text":""},{"location":"examples/duckdb-secrets/#credential-chain-provider","title":"Credential Chain Provider","text":"<p>For automatic credential detection (useful in AWS environments):</p> <pre><code>version: 1\n\nduckdb:\n  database: auto_cred_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  secrets:\n    - type: s3\n      name: s3_auto\n      provider: credential_chain\n      region: us-east-1\n      # No key_id/secret needed - DuckDB will auto-detect\n\nviews:\n  - name: auto_s3_data\n    source: parquet\n    uri: \"s3://auto-bucket/data/*.parquet\"\n    description: \"S3 data with automatic credentials\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#database-secrets","title":"Database Secrets","text":""},{"location":"examples/duckdb-secrets/#postgresql-secret","title":"PostgreSQL Secret","text":"<pre><code>version: 1\n\nduckdb:\n  database: pg_catalog.duckdb\n\n  secrets:\n    - type: postgres\n      name: analytics_db\n      provider: config\n      persistent: true\n      connection_string: postgresql://user:password@localhost:5432/analytics\n      # Alternative: Individual parameters\n      # host: localhost\n      # port: 5432\n      # database: analytics\n      # key_id: user\n      # secret: password\n\nviews:\n  - name: postgres_users\n    source: postgres\n    database: analytics_db\n    table: users\n    description: \"Users from PostgreSQL with secret authentication\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#mysql-secret","title":"MySQL Secret","text":"<pre><code>version: 1\n\nduckdb:\n  database: mysql_catalog.duckdb\n\n  secrets:\n    - type: mysql\n      name: webapp_db\n      connection_string: mysql://user:password@db.example.com:3306/webapp\n\nviews:\n  - name: mysql_products\n    source: mysql\n    database: webapp_db\n    table: products\n    description: \"Products from MySQL with secret authentication\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#http-basic-auth-secret","title":"HTTP Basic Auth Secret","text":"<pre><code>version: 1\n\nduckdb:\n  database: api_catalog.duckdb\n\n  secrets:\n    - type: http\n      name: api_auth\n      key_id: my-api-username\n      secret: my-api-password\n      # Optional: Add custom headers\n      options:\n        custom_header: \"Bearer-Token\"\n        timeout: 30\n\nviews:\n  - name: api_data\n    sql: |\n      SELECT * FROM read_csv_auto('https://api.example.com/data.csv')\n    description: \"Data from HTTP API with basic authentication\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#environment-variable-integration","title":"Environment Variable Integration","text":""},{"location":"examples/duckdb-secrets/#using-environment-variables-for-security","title":"Using Environment Variables for Security","text":"<pre><code>version: 1\n\nduckdb:\n  database: env_secrets_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  secrets:\n    - type: s3\n      name: secure_s3\n      key_id: ${env:AWS_ACCESS_KEY_ID}\n      secret: ${env:AWS_SECRET_ACCESS_KEY}\n      region: ${env:AWS_DEFAULT_REGION}\n\n    - type: postgres\n      name: db_auth\n      connection_string: ${env:DATABASE_URL}\n\nviews:\n  - name: secure_data\n    source: parquet\n    uri: \"s3://secure-bucket/data/*.parquet\"\n    description: \"Secure data using environment variables\"\n</code></pre> <p>Set environment variables:</p> <pre><code># AWS Credentials\nexport AWS_ACCESS_KEY_ID=\"AKIAIOSFODNN7EXAMPLE\"\nexport AWS_SECRET_ACCESS_KEY=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\nexport AWS_DEFAULT_REGION=\"us-west-2\"\n\n# Database URL\nexport DATABASE_URL=\"postgresql://user:password@localhost:5432/analytics\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#multiple-secrets-and-scoping","title":"Multiple Secrets and Scoping","text":""},{"location":"examples/duckdb-secrets/#multiple-s3-secrets-for-different-environments","title":"Multiple S3 Secrets for Different Environments","text":"<pre><code>version: 1\n\nduckdb:\n  database: multi_env_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  secrets:\n    # Production S3 with persistent secret\n    - type: s3\n      name: prod_s3\n      provider: config\n      persistent: true\n      scope: 's3://prod-bucket/'\n      key_id: ${env:PROD_AWS_KEY}\n      secret: ${env:PROD_AWS_SECRET}\n      region: us-east-1\n\n    # Development S3 with temporary secret\n    - type: s3\n      name: dev_s3\n      provider: config\n      scope: 's3://dev-bucket/'\n      key_id: ${env:DEV_AWS_KEY}\n      secret: ${env:DEV_AWS_SECRET}\n      region: us-west-2\n\n    # Staging S3 with credential chain\n    - type: s3\n      name: staging_s3\n      provider: credential_chain\n      scope: 's3://staging-bucket/'\n      region: us-central-1\n\nviews:\n  - name: production_data\n    source: parquet\n    uri: \"s3://prod-bucket/sales/*.parquet\"\n    description: \"Production sales data\"\n\n  - name: development_data\n    source: parquet\n    uri: \"s3://dev-bucket/experiments/*.parquet\"\n    description: \"Development experiment data\"\n\n  - name: staging_logs\n    source: parquet\n    uri: \"s3://staging-bucket/logs/*.parquet\"\n    description: \"Staging log data\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"examples/duckdb-secrets/#1-create-configuration","title":"1. Create Configuration","text":"<p>Save one of the examples above as <code>secrets-example.yaml</code>.</p>"},{"location":"examples/duckdb-secrets/#2-set-environment-variables-if-using-interpolation","title":"2. Set Environment Variables (if using interpolation)","text":"<pre><code># For S3 secrets\nexport AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_DEFAULT_REGION=\"us-west-2\"\n\n# For database secrets\nexport DATABASE_URL=\"postgresql://user:password@localhost:5432/analytics\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#3-validate-configuration","title":"3. Validate Configuration","text":"<pre><code>duckalog validate secrets-example.yaml\n</code></pre> <p>Expected output: <pre><code>\u2705 Configuration is valid\n\u2705 All secrets defined correctly\n\u2705 Environment variables resolved\n</code></pre></p>"},{"location":"examples/duckdb-secrets/#4-build-catalog","title":"4. Build Catalog","text":"<pre><code>duckalog build secrets-example.yaml\n</code></pre>"},{"location":"examples/duckdb-secrets/#5-verify-secrets","title":"5. Verify Secrets","text":"<pre><code># Connect to the created database\nduckdb secrets_catalog.duckdb\n\n# List created secrets\nSELECT name, type, provider FROM duckdb_secrets();\n\n# Check specific secret details\nSELECT * FROM duckdb_secrets() WHERE name = 'production_s3';\n</code></pre>"},{"location":"examples/duckdb-secrets/#secret-types-reference","title":"Secret Types Reference","text":""},{"location":"examples/duckdb-secrets/#s3-secret-fields","title":"S3 Secret Fields","text":"Field Required Description <code>type</code> Yes Must be <code>\"s3\"</code> <code>key_id</code> For <code>config</code> provider AWS access key ID <code>secret</code> For <code>config</code> provider AWS secret access key <code>region</code> Optional AWS region (e.g., <code>us-west-2</code>) <code>endpoint</code> Optional Custom S3 endpoint <code>scope</code> Optional URL prefix for secret scope <code>provider</code> Optional <code>\"config\"</code> (default) or <code>\"credential_chain\"</code> <code>persistent</code> Optional Whether secret persists across sessions"},{"location":"examples/duckdb-secrets/#azure-secret-fields","title":"Azure Secret Fields","text":"Field Required Description <code>type</code> Yes Must be <code>\"azure\"</code> <code>connection_string</code> Either/or Full connection string <code>tenant_id</code> Either/or Azure AD tenant ID <code>account_name</code> Either/or Storage account name <code>secret</code> For explicit auth Account key or password <code>scope</code> Optional URL prefix for secret scope"},{"location":"examples/duckdb-secrets/#gcs-secret-fields","title":"GCS Secret Fields","text":"Field Required Description <code>type</code> Yes Must be <code>\"gcs\"</code> <code>key_id</code> For <code>config</code> provider Service account email <code>secret</code> For <code>config</code> provider Private key content <code>endpoint</code> Optional Custom GCS endpoint <code>scope</code> Optional URL prefix for secret scope"},{"location":"examples/duckdb-secrets/#database-secret-fields","title":"Database Secret Fields","text":"Field Required Description <code>type</code> Yes <code>\"postgres\"</code> or <code>\"mysql\"</code> <code>connection_string</code> Either/or Full database connection string <code>host</code> Either/or Database host <code>port</code> Either/or Database port <code>database</code> Either/or Database name <code>key_id</code> Either/or Database username <code>secret</code> Either/or Database password"},{"location":"examples/duckdb-secrets/#http-secret-fields","title":"HTTP Secret Fields","text":"Field Required Description <code>type</code> Yes Must be <code>\"http\"</code> <code>key_id</code> Yes Username for basic auth <code>secret</code> Yes Password for basic auth <code>options</code> Optional Additional HTTP headers/options"},{"location":"examples/duckdb-secrets/#best-practices","title":"Best Practices","text":""},{"location":"examples/duckdb-secrets/#security","title":"Security","text":"<ol> <li> <p>Use Environment Variables: Never hardcode secrets in configuration files    <pre><code># Good: Use environment variables\nkey_id: ${env:AWS_ACCESS_KEY_ID}\nsecret: ${env:AWS_SECRET_ACCESS_KEY}\n\n# Bad: Hardcode secrets\nkey_id: AKIAIOSFODNN7EXAMPLE\nsecret: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n</code></pre></p> </li> <li> <p>Separate Environments: Use different secrets for dev/staging/prod</p> </li> <li>Use Persistent Secrets: For long-running applications</li> <li>Limit Secret Scope: Use scope to restrict secret to specific paths</li> <li>Rotate Credentials: Update secrets regularly without changing configuration</li> </ol>"},{"location":"examples/duckdb-secrets/#performance","title":"Performance","text":"<ol> <li>Use Credential Chains: In cloud environments for automatic credential rotation</li> <li>Scope Secrets: Limit secrets to specific buckets/prefixes</li> <li>Persistent vs Temporary: Use persistent secrets for frequently accessed resources</li> <li>Connection Pooling: For database secrets, consider connection pooling</li> </ol>"},{"location":"examples/duckdb-secrets/#organization","title":"Organization","text":"<ol> <li>Name Secrets Clearly: Use descriptive names like <code>prod_s3</code>, <code>dev_postgres</code></li> <li>Group by Environment: Keep production, development, staging secrets separate</li> <li>Document Dependencies: Note which views depend on which secrets</li> <li>Version Control: Keep secret configurations out of version control</li> </ol>"},{"location":"examples/duckdb-secrets/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/duckdb-secrets/#common-issues","title":"Common Issues","text":"<p>1. Secret Not Found Error: <pre><code>Catalog Error: Secret with name 'my_secret' does not exist\n</code></pre> Solution: Check that the secret was created successfully and the name matches exactly.</p> <p>2. Permission Denied: <pre><code>Catalog Error: Permission denied\n</code></pre> Solution: Verify credentials, region, and IAM permissions for cloud services.</p> <p>3. Invalid Secret Configuration: <pre><code>Config Error: S3 config provider requires key_id and secret\n</code></pre> Solution: Ensure all required fields for the secret type are provided.</p> <p>4. Environment Variable Not Set: <pre><code>Config Error: Environment variable 'AWS_ACCESS_KEY_ID' is not set\n</code></pre> Solution: Set the required environment variables before running duckalog.</p>"},{"location":"examples/duckdb-secrets/#debugging-secrets","title":"Debugging Secrets","text":"<pre><code>-- List all secrets\nSELECT name, type, provider, persistent FROM duckdb_secrets();\n\n-- Check specific secret\nSELECT * FROM duckdb_secrets() WHERE name = 'my_secret';\n\n-- Test secret access\nSELECT * FROM read_csv_auto('s3://my-bucket/test.csv') LIMIT 1;\n</code></pre>"},{"location":"examples/duckdb-secrets/#integration-with-other-duckalog-features","title":"Integration with Other Duckalog Features","text":""},{"location":"examples/duckdb-secrets/#secrets-with-attachments","title":"Secrets with Attachments","text":"<pre><code>version: 1\n\nduckdb:\n  database: integrated_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  secrets:\n    - type: postgres\n      name: analytics_db\n      connection_string: ${env:ANALYTICS_DB_URL}\n\n  attachments:\n    postgres:\n      - alias: analytics\n        # Use the secret for authentication\n        # DuckDB will automatically use the postgres secret\n\nviews:\n  - name: analytics_data\n    source: postgres\n    database: analytics_db\n    table: sales\n    description: \"Analytics data using secret authentication\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#secrets-with-iceberg-catalogs","title":"Secrets with Iceberg Catalogs","text":"<pre><code>version: 1\n\nduckdb:\n  database: lakehouse_catalog.duckdb\n  install_extensions:\n    - httpfs\n    - iceberg\n\n  secrets:\n    - type: s3\n      name: lakehouse_s3\n      key_id: ${env:LAKEHOUSE_AWS_KEY}\n      secret: ${env:LAKEHOUSE_AWS_SECRET}\n      region: us-east-1\n\n  iceberg_catalogs:\n    - name: production_iceberg\n      catalog_type: rest\n      uri: https://iceberg.example.com\n      # DuckDB will use S3 secret for S3 paths in this catalog\n\nviews:\n  - name: iceberg_sales\n    source: iceberg\n    catalog: production_iceberg\n    table: sales\n    description: \"Iceberg sales data using S3 secret\"\n</code></pre>"},{"location":"examples/duckdb-secrets/#production-deployment","title":"Production Deployment","text":""},{"location":"examples/duckdb-secrets/#docker-example","title":"Docker Example","text":"<pre><code>FROM python:3.9-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY catalog.yaml .\nCOPY .env .\n\n# Set environment variables\nENV AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\nENV AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\nENV AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}\n\nCMD [\"duckalog\", \"build\", \"catalog.yaml\"]\n</code></pre>"},{"location":"examples/duckdb-secrets/#kubernetes-example","title":"Kubernetes Example","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: duckdb-secrets\ntype: Opaque\nstringData:\n  AWS_ACCESS_KEY_ID: \"AKIAIOSFODNN7EXAMPLE\"\n  AWS_SECRET_ACCESS_KEY: \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: duckalog-config\ndata:\n  catalog.yaml: |\n    version: 1\n    duckdb:\n      database: /data/duckdb.duckdb\n      install_extensions:\n        - httpfs\n      secrets:\n        - type: s3\n          name: k8s_s3\n          key_id: ${AWS_ACCESS_KEY_ID}\n          secret: ${AWS_SECRET_ACCESS_KEY}\n          region: us-west-2\n      views:\n        - name: app_data\n          source: parquet\n          uri: \"s3://app-bucket/data/*.parquet\"\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: duckalog-builder\nspec:\n  containers:\n  - name: duckalog\n    image: my-registry/duckalog:latest\n    envFrom:\n      - secretRef:\n          name: duckdb-secrets\n    volumeMounts:\n      - name: config\n        mountPath: /app/config.yaml\n        subPath: catalog.yaml\n  volumes:\n    - name: config\n      configMap:\n        name: duckalog-config\n</code></pre> <p>This example shows how DuckDB secrets in Duckalog provide comprehensive credential management for external services, enabling secure and scalable data access patterns.</p>"},{"location":"examples/duckdb-settings/","title":"DuckDB Settings Example","text":"<p>This example demonstrates how to use DuckDB settings in Duckalog to configure session behavior that goes beyond pragmas. Settings are applied after pragmas and allow you to control DuckDB's runtime behavior.</p>"},{"location":"examples/duckdb-settings/#when-to-use-settings","title":"When to Use Settings","text":"<p>Choose settings when you need to: - Configure DuckDB session parameters that aren't pragmas - Control memory allocation and threading behavior - Enable/disable specific DuckDB features - Set custom configuration parameters for performance tuning</p>"},{"location":"examples/duckdb-settings/#basic-settings-configuration","title":"Basic Settings Configuration","text":""},{"location":"examples/duckdb-settings/#single-setting-example","title":"Single Setting Example","text":"<p>Create a file called <code>settings-example.yaml</code>:</p> <pre><code>version: 1\n\nduckdb:\n  database: settings_catalog.duckdb\n\n  # Extensions (optional)\n  install_extensions:\n    - httpfs\n\n  # Pragmas (applied before settings)\n  pragmas:\n    - \"PRAGMA enable_optimizer=true\"\n    - \"PRAGMA enable_profiling=true\"\n\n  # Settings (applied after pragmas)\n  settings: \"SET enable_progress_bar = false\"\n\nviews:\n  - name: sample_data\n    sql: |\n      SELECT \n        'Settings Demo' as title,\n        CURRENT_TIMESTAMP as demo_time\n    description: \"Simple demo view to test settings\"\n</code></pre>"},{"location":"examples/duckdb-settings/#multiple-settings-example","title":"Multiple Settings Example","text":"<p>For more comprehensive configuration:</p> <pre><code>version: 1\n\nduckdb:\n  database: advanced_settings.duckdb\n  install_extensions:\n    - httpfs\n    - fts\n\n  pragmas:\n    - \"PRAGMA enable_optimizer=true\"\n    - \"PRAGMA enable_profiling=true\"\n\n  # Multiple settings as a list\n  settings:\n    - \"SET enable_progress_bar = false\"\n    - \"SET threads = 4\"\n    - \"SET memory_limit = '2GB'\"\n    - \"SET enable_http_metadata_cache = true\"\n\nviews:\n  - name: analytics_data\n    source: parquet\n    uri: \"s3://your-bucket/analytics/*.parquet\"\n    description: \"Analytics data with optimized settings\"\n\n  - name: performance_metrics\n    sql: |\n      SELECT \n        COUNT(*) as total_rows,\n        COUNT(DISTINCT user_id) as unique_users,\n        AVG(session_duration) as avg_session\n      FROM analytics_data\n      WHERE event_date &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n    description: \"Performance metrics with optimized settings\"\n</code></pre>"},{"location":"examples/duckdb-settings/#settings-with-environment-variables","title":"Settings with Environment Variables","text":"<p>Just like pragmas, settings support environment variable interpolation:</p> <pre><code>version: 1\n\nduckdb:\n  database: env_settings.duckdb\n\n  settings:\n    - \"SET threads = ${env:THREAD_COUNT}\"\n    - \"SET memory_limit = '${env:MEMORY_LIMIT}'\"\n    - \"SET enable_progress_bar = ${env:ENABLE_PROGRESS:false}\"\n\nviews:\n  - name: config_demo\n    sql: |\n      SELECT \n        current_setting('threads') as threads,\n        current_setting('memory_limit') as memory_limit,\n        current_setting('enable_progress_bar') as progress_bar_enabled\n    description: \"Demo view showing current settings\"\n</code></pre> <p>Set the environment variables:</p> <pre><code>export THREAD_COUNT=\"8\"\nexport MEMORY_LIMIT=\"4GB\"\nexport ENABLE_PROGRESS=\"false\"\n</code></pre>"},{"location":"examples/duckdb-settings/#common-duckdb-settings","title":"Common DuckDB Settings","text":""},{"location":"examples/duckdb-settings/#performance-settings","title":"Performance Settings","text":"<pre><code>duckdb:\n  settings:\n    # Threading and parallelism\n    - \"SET threads = 8\"                    # Number of CPU threads\n    - \"SET enable_progress_bar = false\"    # Disable progress output\n\n    # Memory management\n    - \"SET memory_limit = '4GB'\"           # Maximum memory usage\n\n    # Network and caching\n    - \"SET enable_http_metadata_cache = true\"   # Cache HTTP metadata\n    - \"SET enable_object_cache = true\"          # Cache object files\n</code></pre>"},{"location":"examples/duckdb-settings/#query-optimization-settings","title":"Query Optimization Settings","text":"<pre><code>duckdb:\n  settings:\n    # Query execution\n    - \"SET enable_progress_bar = false\"\n    - \"SET preserve_insertion_order = false\"    # Faster unordered results\n\n    # Join performance\n    - \"SET force_parallelism = true\"             # Force parallel execution\n</code></pre>"},{"location":"examples/duckdb-settings/#development-and-debugging-settings","title":"Development and Debugging Settings","text":"<pre><code>duckdb:\n  settings:\n    # Debugging\n    - \"SET enable_progress_bar = true\"           # Show progress for long queries\n    - \"SET enable_profiling = true\"              # Enable query profiling\n\n    # Output formatting\n    - \"SET max_width = 120\"                     # Output width\n    - \"SET null_display = 'NULL'\"                # How NULL values are displayed\n</code></pre>"},{"location":"examples/duckdb-settings/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"examples/duckdb-settings/#1-create-configuration","title":"1. Create Configuration","text":"<p>Save one of the examples above as <code>settings-example.yaml</code>.</p>"},{"location":"examples/duckdb-settings/#2-set-environment-variables-if-using-interpolation","title":"2. Set Environment Variables (if using interpolation)","text":"<pre><code>export THREAD_COUNT=\"4\"\nexport MEMORY_LIMIT=\"2GB\"\n</code></pre>"},{"location":"examples/duckdb-settings/#3-validate-configuration","title":"3. Validate Configuration","text":"<pre><code>duckalog validate settings-example.yaml\n</code></pre>"},{"location":"examples/duckdb-settings/#4-build-catalog","title":"4. Build Catalog","text":"<pre><code>duckalog build settings-example.yaml\n</code></pre>"},{"location":"examples/duckdb-settings/#5-verify-settings-applied","title":"5. Verify Settings Applied","text":"<pre><code># Connect to the created database\nduckdb settings_catalog.duckdb\n\n# Check current settings\nSELECT name, value FROM duckdb_settings() WHERE name LIKE '%thread%' OR name LIKE '%memory%';\n\n# Or use the current_setting function\nSELECT current_setting('threads') as thread_count;\nSELECT current_setting('memory_limit') as memory_limit;\nSELECT current_setting('enable_progress_bar') as progress_enabled;\n</code></pre>"},{"location":"examples/duckdb-settings/#settings-vs-pragmas","title":"Settings vs Pragmas","text":""},{"location":"examples/duckdb-settings/#when-to-use-pragmas","title":"When to Use Pragmas","text":"<ul> <li>Database-level configuration that affects the entire database file</li> <li>Persistent settings that should be saved with the database</li> <li>Low-level optimizations like <code>PRAGMA enable_optimizer</code></li> </ul>"},{"location":"examples/duckdb-settings/#when-to-use-settings_1","title":"When to Use Settings","text":"<ul> <li>Session-level configuration for the current connection</li> <li>Runtime behavior like progress bars and threading</li> <li>Temporary configuration that shouldn't persist</li> <li>Feature toggles like <code>SET enable_http_metadata_cache</code></li> </ul>"},{"location":"examples/duckdb-settings/#execution-order","title":"Execution Order","text":"<ol> <li>Extensions are installed and loaded</li> <li>Pragmas are executed (database-level)</li> <li>Settings are executed (session-level)</li> <li>Views are created</li> </ol>"},{"location":"examples/duckdb-settings/#advanced-examples","title":"Advanced Examples","text":""},{"location":"examples/duckdb-settings/#conditional-settings-based-on-environment","title":"Conditional Settings Based on Environment","text":"<pre><code>version: 1\n\nduckdb:\n  database: conditional_settings.duckdb\n\n  # Use different settings based on environment\n  settings: \"${env:DUCKDB_SETTINGS:SET enable_progress_bar = false}\"\n\nviews:\n  - name: environment_info\n    sql: |\n      SELECT \n        'Environment: ${env:ENV_NAME:development}' as environment,\n        current_setting('enable_progress_bar') as progress_bar\n    description: \"Show environment and settings\"\n</code></pre>"},{"location":"examples/duckdb-settings/#settings-for-different-workloads","title":"Settings for Different Workloads","text":"<pre><code># Production configuration - optimized for performance\nversion: 1\n\nduckdb:\n  database: production_catalog.duckdb\n  settings:\n    - \"SET enable_progress_bar = false\"\n    - \"SET threads = 16\"\n    - \"SET memory_limit = '8GB'\"\n    - \"SET enable_object_cache = true\"\n\n# Development configuration - more verbose\nversion: 1\n\nduckdb:\n  database: dev_catalog.duckdb\n  settings:\n    - \"SET enable_progress_bar = true\"\n    - \"SET threads = 2\"\n    - \"SET memory_limit = '1GB'\"\n    - \"SET enable_profiling = true\"\n</code></pre>"},{"location":"examples/duckdb-settings/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/duckdb-settings/#common-settings-issues","title":"Common Settings Issues","text":"<p>1. Invalid Setting Name: <pre><code>Error: Catalog Error: unrecognized configuration parameter\n</code></pre> Solution: Check DuckDB documentation for valid setting names, or use <code>PRAGMA table_info(duckdb_settings())</code> to see available settings.</p> <p>2. Setting Value Type Mismatch: <pre><code>Error: Parser Error: Expected type\n</code></pre> Solution: Ensure the value type matches what the setting expects (string, integer, boolean).</p> <p>3. Settings Not Applied: - Check that settings are in the correct format (must start with \"SET \") - Verify settings are applied after pragmas in the execution order - Use <code>current_setting('setting_name')</code> to verify the value</p>"},{"location":"examples/duckdb-settings/#debugging-settings","title":"Debugging Settings","text":"<pre><code>-- List all current settings\nSELECT name, value FROM duckdb_settings();\n\n-- Check specific setting\nSELECT current_setting('threads') as thread_count;\n\n-- Show settings that were changed from defaults\nSELECT name, value, default_value \nFROM duckdb_settings() \nWHERE value != default_value;\n</code></pre>"},{"location":"examples/duckdb-settings/#best-practices","title":"Best Practices","text":"<ol> <li>Use Environment Variables: For sensitive values or deployment-specific settings</li> <li>Document Settings: Add comments explaining why each setting is needed</li> <li>Test Settings: Verify settings work as expected in your environment</li> <li>Separate Configs: Use different configs for development vs production</li> <li>Monitor Performance: Check that settings actually improve performance</li> </ol> <p>This example shows how DuckDB settings in Duckalog provide fine-grained control over DuckDB's behavior, complementing the existing pragmas system for comprehensive configuration management.</p>"},{"location":"examples/environment-vars/","title":"Environment Variables Example","text":"<p>This example demonstrates how to use environment variables effectively in Duckalog configurations. You'll learn security best practices, environment-specific configurations, and credential management patterns that keep your configs portable and secure.</p>"},{"location":"examples/environment-vars/#when-to-use-this-example","title":"When to Use This Example","text":"<p>Choose this example if: - You need to keep credentials out of configuration files - You want different configs for development, staging, and production - You're deploying Duckalog across multiple environments - You need to comply with security policies (no hardcoded secrets) - You want to make configs reusable across different setups</p>"},{"location":"examples/environment-vars/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Duckalog installed: <pre><code>pip install duckalog\n</code></pre></p> </li> <li> <p>Basic understanding of environment variables in your shell: <pre><code># Check if a variable exists\necho $AWS_ACCESS_KEY_ID\n\n# Set a variable\nexport DATABASE_PASSWORD=\"my-secret-password\"\n\n# Unset a variable\nunset DATABASE_PASSWORD\n</code></pre></p> </li> </ol>"},{"location":"examples/environment-vars/#environment-variable-syntax","title":"Environment Variable Syntax","text":"<p>Duckalog supports environment variable interpolation using the <code>${env:VAR_NAME}</code> syntax:</p> <pre><code># Basic syntax\nsome_field: \"${env:VARIABLE_NAME}\"\n\n# With default values (Duckalog specific feature)\nsome_field: \"${env:VARIABLE_NAME:default_value}\"\n\n# Nested references\nconfig:\n  host: \"${env:DB_HOST}\"\n  connection_string: \"postgresql://${env:DB_USER}:${env:DB_PASSWORD}@${env:DB_HOST}:${env:DB_PORT}/${env:DB_NAME}\"\n</code></pre> <p>Important Notes: - Variable names are case-sensitive - Undefined variables will cause validation errors (unless default is provided) - Variables are resolved during configuration loading - No quotes needed around the <code>${env:...}</code> syntax</p>"},{"location":"examples/environment-vars/#security-best-practices","title":"Security Best Practices","text":""},{"location":"examples/environment-vars/#1-never-commit-secrets","title":"1. Never Commit Secrets","text":"<p>\u274c Wrong - Don't do this: <pre><code># This config file should NEVER be committed to version control\nduckdb:\n  pragmas:\n    - \"SET s3_access_key_id='AKIA...'\"  # Hardcoded credentials\n    - \"SET s3_secret_access_key='real-secret-key'\"\npostgres:\n  password: \"super-secret-password\"      # Hardcoded password\n</code></pre></p> <p>\u2705 Correct - Use environment variables: <pre><code># This config file is safe to commit\nduckdb:\n  pragmas:\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\npostgres:\n  password: \"${env:DATABASE_PASSWORD}\"\n</code></pre></p>"},{"location":"examples/environment-vars/#2-use-gitignore","title":"2. Use <code>.gitignore</code>","text":"<p>Create a <code>.gitignore</code> file to prevent accidentally committing sensitive files:</p> <pre><code># Environment files\n.env\n.env.local\n.env.production\n\n# Generated catalogs\n*.duckdb\n*.db\n\n# Logs\n*.log\n\n# Temporary files\ntmp/\ntemp/\n</code></pre>"},{"location":"examples/environment-vars/#3-environment-specific-files","title":"3. Environment-Specific Files","text":"<p>Use <code>.env</code> files for local development (add to <code>.gitignore</code>):</p> <pre><code># .env.local (for local development)\nAWS_ACCESS_KEY_ID=AKIA...\nAWS_SECRET_ACCESS_KEY=...\nDATABASE_PASSWORD=dev-password\n\n# .env.production (for production deployment)\nAWS_ACCESS_KEY_ID=AKIA...\nAWS_SECRET_ACCESS_KEY=...\nDATABASE_PASSWORD=production-password\n</code></pre> <p>Load with: <pre><code># Load environment file\nsource .env.local\n\n# Or use a tool like direnv\n# (add to .gitignore)\necho \"source .env.local\" &gt; .envrc\ndirenv allow\n</code></pre></p>"},{"location":"examples/environment-vars/#basic-environment-configuration","title":"Basic Environment Configuration","text":""},{"location":"examples/environment-vars/#development-configuration","title":"Development Configuration","text":"<p>Create <code>config-development.yaml</code>:</p> <pre><code>version: 1\n\nduckdb:\n  database: dev_catalog.duckdb\n  pragmas:\n    # Development settings - less restrictive\n    - \"SET memory_limit='512MB'\"\n    - \"SET threads=2\"\n    - \"SET search_path='public'\"\n\n# Development database (local or dev environment)\nattachments:\n  postgres:\n    - alias: dev_db\n      host: \"${env:DEV_DB_HOST:localhost}\"\n      port: 5432\n      database: \"${env:DEV_DB_NAME:analytics_dev}\"\n      user: \"${env:DEV_DB_USER:dev_user}\"\n      password: \"${env:DEV_DB_PASSWORD:dev_password}\"\n\n# S3 development bucket\nviews:\n  - name: dev_data\n    source: parquet\n    uri: \"s3://${env:DEV_S3_BUCKET:my-dev-bucket}/data/*.parquet\"\n</code></pre>"},{"location":"examples/environment-vars/#production-configuration","title":"Production Configuration","text":"<p>Create <code>config-production.yaml</code>:</p> <pre><code>version: 1\n\nduckdb:\n  database: prod_catalog.duckdb\n  pragmas:\n    # Production settings - more restrictive and performant\n    - \"SET memory_limit='8GB'\"\n    - \"SET threads=8\"\n    - \"SET temp_directory='/tmp/duckdb_temp'\"\n\n    # Cloud storage for production\n    - \"SET s3_region='${env:AWS_REGION:us-east-1}'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n    - \"SET s3_session_token='${env:AWS_SESSION_TOKEN}'\"\n\n# Production databases\nattachments:\n  postgres:\n    - alias: prod_db\n      host: \"${env:PROD_DB_HOST}\"\n      port: 5432\n      database: \"${env:PROD_DB_NAME}\"\n      user: \"${env:PROD_DB_USER}\"\n      password: \"${env:PROD_DB_PASSWORD}\"\n      sslmode: require\n\n# Production data sources\nviews:\n  - name: production_data\n    source: parquet\n    uri: \"s3://${env:PROD_S3_BUCKET}/data/*.parquet\"\n\n  - name: prod_metrics\n    sql: |\n      SELECT \n        DATE(created_at) as metric_date,\n        COUNT(*) as total_records,\n        AVG(value) as avg_value\n      FROM production_data\n      GROUP BY DATE(created_at)\n      ORDER BY metric_date DESC\n</code></pre>"},{"location":"examples/environment-vars/#common-environment-variable-patterns","title":"Common Environment Variable Patterns","text":""},{"location":"examples/environment-vars/#1-aws-and-cloud-configuration","title":"1. AWS and Cloud Configuration","text":"<pre><code># AWS credentials and region\nduckdb:\n  pragmas:\n    - \"SET s3_region='${env:AWS_REGION:us-east-1}'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n    - \"SET s3_session_token='${env:AWS_SESSION_TOKEN}'\"\n\n# S3 buckets by environment\nviews:\n  - name: events\n    source: parquet\n    uri: \"s3://${env:S3_BUCKET_PREFIX}-events/${env:ENVIRONMENT:dev}/data/*.parquet\"\n</code></pre> <p>Required environment variables: <pre><code>export AWS_REGION=\"us-west-2\"\nexport AWS_ACCESS_KEY_ID=\"AKIA...\"\nexport AWS_SECRET_ACCESS_KEY=\"...\"\nexport AWS_SESSION_TOKEN=\"...\"  # For temporary credentials\nexport S3_BUCKET_PREFIX=\"company\"\nexport ENVIRONMENT=\"prod\"\n</code></pre></p>"},{"location":"examples/environment-vars/#2-database-connections","title":"2. Database Connections","text":"<pre><code>attachments:\n  postgres:\n    - alias: primary\n      host: \"${env:DB_HOST}\"\n      port: \"${env:DB_PORT:5432}\"\n      database: \"${env:DB_NAME}\"\n      user: \"${env:DB_USER}\"\n      password: \"${env:DB_PASSWORD}\"\n      sslmode: \"${env:DB_SSL_MODE:require}\"\n\n  - alias: analytics\n      host: \"${env:ANALYTICS_DB_HOST}\"\n      port: 5432\n      database: \"${env:ANALYTICS_DB_NAME}\"\n      user: \"${env:ANALYTICS_DB_USER}\"\n      password: \"${env:ANALYTICS_DB_PASSWORD}\"\n\n  duckdb:\n    - alias: reference\n      path: \"${env:REFERENCE_DB_PATH:./reference.duckdb}\"\n      read_only: true\n</code></pre> <p>Database environment variables: <pre><code>export DB_HOST=\"prod-db.example.com\"\nexport DB_PORT=\"5432\"\nexport DB_NAME=\"analytics\"\nexport DB_USER=\"analytics_user\"\nexport DB_PASSWORD=\"secure_password\"\nexport DB_SSL_MODE=\"require\"\n\nexport ANALYTICS_DB_HOST=\"analytics-db.example.com\"\nexport ANALYTICS_DB_NAME=\"analytics_warehouse\"\nexport ANALYTICS_DB_USER=\"warehouse_user\"\nexport ANALYTICS_DB_PASSWORD=\"warehouse_password\"\n\nexport REFERENCE_DB_PATH=\"/data/reference.duckdb\"\n</code></pre></p>"},{"location":"examples/environment-vars/#3-iceberg-and-data-lake-configuration","title":"3. Iceberg and Data Lake Configuration","text":"<pre><code>iceberg_catalogs:\n  - name: production_catalog\n    catalog_type: rest\n    uri: \"${env:ICEBERG_URI}\"\n    warehouse: \"s3://${env:ICEBERG_WAREHOUSE_BUCKET}/production/\"\n    options:\n      token: \"${env:ICEBERG_TOKEN}\"\n      region: \"${env:AWS_REGION:us-east-1}\"\n\n  - name: staging_catalog\n    catalog_type: rest\n    uri: \"${env:ICEBERG_STAGING_URI}\"\n    warehouse: \"s3://${env:ICEBERG_WAREHOUSE_BUCKET}/staging/\"\n    options:\n      token: \"${env:ICEBERG_STAGING_TOKEN}\"\n      region: \"${env:AWS_REGION:us-east-1}\"\n</code></pre> <p>Iceberg environment variables: <pre><code>export ICEBERG_URI=\"https://catalog.company.com\"\nexport ICEBERG_WAREHOUSE_BUCKET=\"company-data-warehouse\"\nexport ICEBERG_TOKEN=\"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\"\n\nexport ICEBERG_STAGING_URI=\"https://staging-catalog.company.com\"\nexport ICEBERG_STAGING_TOKEN=\"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\"\n</code></pre></p>"},{"location":"examples/environment-vars/#4-application-configuration","title":"4. Application Configuration","text":"<pre><code># Application-level settings\nduckdb:\n  database: \"${env:CATALOG_DATABASE_NAME:catalog}.duckdb\"\n  pragmas:\n    - \"SET memory_limit='${env:MEMORY_LIMIT:1GB}'\"\n    - \"SET threads='${env:THREAD_COUNT:2}'\"\n    - \"SET timezone='${env:TIMEZONE:UTC}'\"\n\n# File paths\nattachments:\n  duckdb:\n    - alias: reference\n      path: \"${env:REFERENCE_DATA_PATH:./reference_data.duckdb}\"\n\n# S3 configuration\nviews:\n  - name: data_source\n    source: \"${env:DATA_SOURCE_TYPE:parquet}\"\n    uri: \"${env:DATA_URI:s3://default-bucket/data/*.parquet}\"\n</code></pre> <p>Application environment variables: <pre><code>export CATALOG_DATABASE_NAME=\"my_analytics\"\nexport MEMORY_LIMIT=\"4GB\"\nexport THREAD_COUNT=\"8\"\nexport TIMEZONE=\"America/New_York\"\n\nexport REFERENCE_DATA_PATH=\"/data/reference.duckdb\"\n\nexport DATA_SOURCE_TYPE=\"parquet\"\nexport DATA_URI=\"s3://my-bucket/production-data/*.parquet\"\n</code></pre></p>"},{"location":"examples/environment-vars/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"examples/environment-vars/#development-vs-production-strategy","title":"Development vs Production Strategy","text":"<p>Create a base configuration with environment overlays:</p> <p><code>base-config.yaml</code>: <pre><code>version: 1\n\nduckdb:\n  database: \"${env:CATALOG_NAME:analytics}.duckdb\"\n\nviews:\n  - name: user_data\n    source: parquet\n    uri: \"${env:DATA_BUCKET}/users/*.parquet\"\n\n  - name: event_data\n    source: parquet\n    uri: \"${env:DATA_BUCKET}/events/*.parquet\"\n\n  - name: analytics_summary\n    sql: |\n      SELECT \n        DATE(e.timestamp) as event_date,\n        COUNT(*) as total_events,\n        COUNT(DISTINCT e.user_id) as unique_users\n      FROM event_data e\n      JOIN user_data u ON e.user_id = u.id\n      GROUP BY DATE(e.timestamp)\n</code></pre></p> <p>Development environment (<code>.env.dev</code>): <pre><code>export CATALOG_NAME=\"dev_analytics\"\nexport DATA_BUCKET=\"s3://company-dev-data\"\nexport AWS_ACCESS_KEY_ID=\"dev-key\"\nexport AWS_SECRET_ACCESS_KEY=\"dev-secret\"\n</code></pre></p> <p>Production environment (<code>.env.prod</code>): <pre><code>export CATALOG_NAME=\"prod_analytics\"\nexport DATA_BUCKET=\"s3://company-prod-data\"\nexport AWS_ACCESS_KEY_ID=\"prod-key\"\nexport AWS_SECRET_ACCESS_KEY=\"prod-secret\"\n</code></pre></p>"},{"location":"examples/environment-vars/#docker-and-container-deployment","title":"Docker and Container Deployment","text":"<p><code>Dockerfile</code>: <pre><code>FROM python:3.9-slim\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\n# Copy application\nCOPY . /app\nWORKDIR /app\n\n# Use environment variables for configuration\nENV CONFIG_FILE=\"/app/config-production.yaml\"\nENV LOG_LEVEL=\"INFO\"\n\nCMD [\"duckalog\", \"build\", \"$CONFIG_FILE\"]\n</code></pre></p> <p>Docker Compose (development): <pre><code># docker-compose.yml\nversion: '3.8'\nservices:\n  duckalog:\n    build: .\n    environment:\n      - CONFIG_FILE=config-development.yaml\n      - CATALOG_NAME=dev_catalog\n      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n      - DB_PASSWORD=${DB_PASSWORD}\n    volumes:\n      - ./data:/app/data\n      - ./configs:/app/configs\n</code></pre></p> <p>Kubernetes ConfigMap and Secret: <pre><code># configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: duckalog-config\ndata:\n  config-production.yaml: |\n    version: 1\n    duckdb:\n      database: \"/data/catalog.duckdb\"\n      pragmas:\n        - \"SET memory_limit='8GB'\"\n        - \"SET threads=8\"\n    views:\n      - name: production_data\n        source: parquet\n        uri: \"s3://${env:PRODUCTION_BUCKET}/data/*.parquet\"\n\n---\n# secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: duckalog-secrets\ntype: Opaque\nstringData:\n  AWS_ACCESS_KEY_ID: \"AKIA...\"\n  AWS_SECRET_ACCESS_KEY: \"...\"\n  PRODUCTION_BUCKET: \"company-prod-data\"\n</code></pre></p>"},{"location":"examples/environment-vars/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"examples/environment-vars/#1-set-up-environment-variables","title":"1. Set Up Environment Variables","text":"<p>Option A: Direct environment variables <pre><code># Set variables for current session\nexport AWS_ACCESS_KEY_ID=\"AKIA...\"\nexport AWS_SECRET_ACCESS_KEY=\"...\"\nexport DATABASE_PASSWORD=\"secret\"\nexport ENVIRONMENT=\"development\"\n\n# Verify they're set\necho $AWS_ACCESS_KEY_ID\n</code></pre></p> <p>Option B: Using .env file <pre><code># Create .env file\ncat &gt; .env &lt;&lt; EOF\nAWS_ACCESS_KEY_ID=AKIA...\nAWS_SECRET_ACCESS_KEY=...\nDATABASE_PASSWORD=secret\nENVIRONMENT=development\nEOF\n\n# Load the file\nsource .env\n\n# Add to .gitignore\necho \".env\" &gt;&gt; .gitignore\n</code></pre></p> <p>Option C: Using direnv (recommended) <pre><code># Install direnv\n# Then in your project directory:\necho \"source .env\" &gt; .envrc\ndirenv allow\n# direnv automatically loads .env when you cd into the directory\n</code></pre></p>"},{"location":"examples/environment-vars/#2-create-configuration-with-environment-variables","title":"2. Create Configuration with Environment Variables","text":"<pre><code># config.yaml\nversion: 1\n\nduckdb:\n  database: \"${env:CATALOG_NAME:analytics}.duckdb\"\n  pragmas:\n    - \"SET memory_limit='${env:MEMORY_LIMIT:1GB}'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n\nattachments:\n  postgres:\n    - alias: main_db\n      host: \"${env:DB_HOST}\"\n      database: \"${env:DB_NAME}\"\n      user: \"${env:DB_USER}\"\n      password: \"${env:DB_PASSWORD}\"\n\nviews:\n  - name: sample_data\n    source: parquet\n    uri: \"s3://${env:DATA_BUCKET}/sample/*.parquet\"\n</code></pre>"},{"location":"examples/environment-vars/#3-validate-configuration","title":"3. Validate Configuration","text":"<pre><code># Check for missing environment variables\nduckalog validate config.yaml\n\n# If variables are missing, you'll see errors like:\n# ConfigError: Environment variable 'AWS_ACCESS_KEY_ID' not found\n</code></pre>"},{"location":"examples/environment-vars/#4-generate-sql-with-environment-variables","title":"4. Generate SQL with Environment Variables","text":"<pre><code># The SQL will be generated with environment variables resolved\nduckalog generate-sql config.yaml --output generated.sql\n\n# Check the output to see resolved values (be careful with secrets!)\ncat generated.sql\n</code></pre>"},{"location":"examples/environment-vars/#5-build-catalog","title":"5. Build Catalog","text":"<pre><code># Build with environment variables\nduckalog build config.yaml\n\n# This will create catalog.duckdb (or whatever name you specified)\n</code></pre>"},{"location":"examples/environment-vars/#6-use-in-scripts","title":"6. Use in Scripts","text":"<pre><code># build_catalog.py\nimport os\nfrom duckalog import build_catalog, validate_config\n\n# Set environment based on command line argument\nenv = os.environ.get('ENVIRONMENT', 'development')\nconfig_file = f'config-{env}.yaml'\n\n# Validate first\ntry:\n    validate_config(config_file)\n    print(f\"\u2705 Configuration validated for {env} environment\")\nexcept Exception as e:\n    print(f\"\u274c Configuration error: {e}\")\n    exit(1)\n\n# Build catalog\nbuild_catalog(config_file)\nprint(f\"\u2705 Catalog built for {env} environment\")\n</code></pre> <pre><code># Run for different environments\nENVIRONMENT=development python build_catalog.py\nENVIRONMENT=production python build_catalog.py\n</code></pre>"},{"location":"examples/environment-vars/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/environment-vars/#common-issues","title":"Common Issues","text":"<p>1. Missing Environment Variables <pre><code># Check what's set\nenv | grep -E \"(AWS|DB|DATA)\"\n\n# Check specific variable\necho \"AWS_ACCESS_KEY_ID: $AWS_ACCESS_KEY_ID\"\n\n# List all variables (be careful with secrets!)\nenv\n</code></pre></p> <p>2. Variable Not Expanding <pre><code># Make sure you're using the correct syntax\ngood: \"${env:VARIABLE_NAME}\"\nbad: \"$VARIABLE_NAME\"                    # Wrong\nbad: \"${VARIABLE_NAME}\"                  # Missing env: prefix\nbad: \"${env:}\"                           # Empty variable name\n</code></pre></p> <p>3. Default Values Not Working <pre><code># Correct syntax for defaults\nwith_default: \"${env:MISSING_VAR:default_value}\"\nwithout_default: \"${env:ANOTHER_VAR}\"    # Will error if not set\n\n# Note: Default values must be strings\ncorrect: \"${env:PORT:5432}\"              # String default\nincorrect: \"${env:PORT:5432}\"            # Still string, but looks like number\n</code></pre></p> <p>4. Special Characters in Values <pre><code># For passwords with special characters, quotes might be needed\npassword: \"${env:DB_PASSWORD}\"           # Usually works\npassword: \"'${env:DB_PASSWORD}'\"         # Force quotes if needed\n\n# If your password contains quotes, escape them\npassword: \"${env:COMPLEX_PASSWORD:my\\\"special'pass}\"\n</code></pre></p>"},{"location":"examples/environment-vars/#debug-commands","title":"Debug Commands","text":"<p>Check environment variable resolution: <pre><code># Create a simple config to test variables\ncat &gt; test_vars.yaml &lt;&lt; EOF\nversion: 1\ntest_field: \"${env:TEST_VAR:default_value}\"\nEOF\n\n# Validate and see what happens\nduckalog validate test_vars.yaml\n\n# Set the variable and test again\nexport TEST_VAR=\"resolved_value\"\nduckalog validate test_vars.yaml\n</code></pre></p> <p>List all environment variables used in config: <pre><code># Use grep to find env variable usage\ngrep -o '\\${env:[^}]*}' config.yaml | sort | uniq\n</code></pre></p>"},{"location":"examples/environment-vars/#security-testing","title":"Security Testing","text":"<p>Check for accidentally committed secrets: <pre><code># Search for potential secrets in config files\ngrep -r \"password\\|secret\\|key\\|token\" *.yaml | grep -v env:\n\n# Check git history for secrets\ngit log -p --grep=\"password\\|secret\\|key\" --all\n\n# Use git-secrets or similar tools\ngit-secrets --scan\n</code></pre></p>"},{"location":"examples/environment-vars/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"examples/environment-vars/#1-environment-variable-validation","title":"1. Environment Variable Validation","text":"<pre><code># validate_env.py\nimport os\nimport sys\n\nrequired_vars = [\n    'AWS_ACCESS_KEY_ID',\n    'AWS_SECRET_ACCESS_KEY',\n    'DATABASE_PASSWORD'\n]\n\nmissing_vars = []\nfor var in required_vars:\n    if not os.environ.get(var):\n        missing_vars.append(var)\n\nif missing_vars:\n    print(f\"\u274c Missing required environment variables:\")\n    for var in missing_vars:\n        print(f\"   - {var}\")\n    print(\"\\nSet them with:\")\n    print(f\"export {' '.join(f'{var}=...' for var in missing_vars)}\")\n    sys.exit(1)\n\nprint(\"\u2705 All required environment variables are set\")\n</code></pre>"},{"location":"examples/environment-vars/#2-dynamic-configuration-selection","title":"2. Dynamic Configuration Selection","text":"<pre><code># dynamic_config.py\nimport os\nfrom pathlib import Path\n\ndef get_config_for_environment():\n    env = os.environ.get('ENVIRONMENT', 'development')\n\n    # Map environment to config file\n    config_map = {\n        'development': 'configs/config-dev.yaml',\n        'staging': 'configs/config-staging.yaml',\n        'production': 'configs/config-prod.yaml'\n    }\n\n    config_file = config_map.get(env)\n    if not config_file or not Path(config_file).exists():\n        raise FileNotFoundError(f\"No config found for environment: {env}\")\n\n    return config_file\n\n# Usage\nconfig = get_config_for_environment()\nprint(f\"Using configuration: {config}\")\n</code></pre>"},{"location":"examples/environment-vars/#3-secret-rotation-support","title":"3. Secret Rotation Support","text":"<pre><code># config-with-rotation.yaml\nversion: 1\n\nduckdb:\n  pragmas:\n    # Support both old and new AWS credentials during rotation\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID_NEW:${env:AWS_ACCESS_KEY_ID}}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY_NEW:${env:AWS_SECRET_ACCESS_KEY}}'\"\n\nattachments:\n  postgres:\n    - alias: main\n      password: \"${env:DB_PASSWORD_NEW:${env:DB_PASSWORD}}\"\n</code></pre> <p>This environment variable pattern enables secure, portable, and maintainable Duckalog configurations across all your deployment environments.</p>"},{"location":"examples/local-attachments/","title":"Local Attachments Example","text":"<p>This example demonstrates how to attach and work with local DuckDB and SQLite databases in Duckalog. It's ideal for combining data from multiple local databases or integrating existing data stores into your analytics pipeline.</p>"},{"location":"examples/local-attachments/#when-to-use-this-example","title":"When to Use This Example","text":"<p>Choose this example if:</p> <ul> <li>You have multiple local DuckDB databases to combine</li> <li>You need to integrate existing SQLite databases</li> <li>You want to join data across different local databases</li> <li>You're working with read-only reference data</li> <li>You need to build a unified view of local data sources</li> <li>You want to avoid duplicating data by referencing instead of copying</li> </ul>"},{"location":"examples/local-attachments/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Duckalog installed: <pre><code>pip install duckalog\n</code></pre></p> </li> <li> <p>Sample local databases - Create test data for demonstration:    <pre><code># Create sample DuckDB databases\nimport duckdb\n\n# Reference data database\ncon_ref = duckdb.connect(\"reference_data.duckdb\")\ncon_ref.execute(\"\"\"\n  CREATE TABLE users AS\n  SELECT \n    user_id,\n    name,\n    email,\n    signup_date,\n    country,\n    segment\n  FROM range(1, 101) t(user_id)\n  CROSS JOIN (VALUES \n    ('Alice', 'alice@example.com', '2023-01-15', 'US', 'enterprise'),\n    ('Bob', 'bob@example.com', '2023-02-20', 'UK', 'smb'),\n    ('Carol', 'carol@example.com', '2023-03-10', 'DE', 'enterprise'),\n    ('David', 'david@example.com', '2023-04-05', 'US', 'startup'),\n    ('Eve', 'eve@example.com', '2023-05-12', 'FR', 'smb')\n  ) u(name, email, signup_date, country, segment)\n  WHERE user_id &lt;= 5\n\"\"\")\n\n# Analytics database with sales data\ncon_analytics = duckdb.connect(\"analytics_data.duckdb\")\ncon_analytics.execute(\"\"\"\n  CREATE TABLE sales AS\n  SELECT \n    sale_id,\n    user_id,\n    product_name,\n    amount,\n    sale_date,\n    region\n  FROM range(1, 501) t(sale_id)\n  CROSS JOIN (VALUES \n    ('Laptop', 1200.00, '2023-01-01', 'North'),\n    ('Mouse', 25.00, '2023-01-02', 'South'),\n    ('Keyboard', 75.00, '2023-01-03', 'East'),\n    ('Monitor', 300.00, '2023-01-04', 'West'),\n    ('Desk', 450.00, '2023-01-05', 'North')\n  ) p(product_name, amount, sale_date, region)\n  CROSS JOIN (SELECT user_id FROM range(1, 6)) u(user_id)\n  WHERE sale_id &lt;= 100\n\"\"\")\n\ncon_ref.close()\ncon_analytics.close()\n\n# Create sample SQLite database\nimport sqlite3\n\ncon_sqlite = sqlite3.connect(\"legacy_system.db\")\ncon_sqlite.execute(\"\"\"\n  CREATE TABLE customer_preferences (\n    user_id INTEGER PRIMARY KEY,\n    preferred_categories TEXT,\n    communication_style TEXT,\n    last_contact_date DATE\n  )\n\"\"\")\n\npreferences_data = [\n  (1, 'electronics,books', 'email', '2023-06-15'),\n  (2, 'home,garden', 'phone', '2023-06-10'),\n  (3, 'technology,software', 'email', '2023-06-20'),\n  (4, 'gaming,electronics', 'slack', '2023-06-18'),\n  (5, 'office,productivity', 'email', '2023-06-12')\n]\n\ncon_sqlite.executemany(\n  \"INSERT INTO customer_preferences VALUES (?, ?, ?, ?)\",\n  preferences_data\n)\n\ncon_sqlite.commit()\ncon_sqlite.close()\n\nprint(\"Created sample databases: reference_data.duckdb, analytics_data.duckdb, legacy_system.db\")\n</code></pre></p> </li> </ol>"},{"location":"examples/local-attachments/#basic-attachment-configuration","title":"Basic Attachment Configuration","text":""},{"location":"examples/local-attachments/#single-database-attachment","title":"Single Database Attachment","text":"<p>Create a file called <code>local-attachments.yaml</code>:</p> <pre><code>version: 1\n\n# DuckDB configuration\nduckdb:\n  database: unified_catalog.duckdb\n  pragmas:\n    # Performance settings for local work\n    - \"SET memory_limit='1GB'\"\n    - \"SET threads=2\"\n    - \"SET temp_directory='/tmp/duckdb_temp'\"  # For large operations\n\n# Attach local databases\nattachments:\n  duckdb:\n    - alias: reference       # How you'll reference this database\n      path: ./reference_data.duckdb   # Path to DuckDB file\n      read_only: true        # Prevent modifications (recommended)\n\n    - alias: analytics\n      path: ./analytics_data.duckdb\n      read_only: true\n\n  sqlite:\n    - alias: legacy         # SQLite database reference\n      path: ./legacy_system.db\n\n# Views that use attached databases\nviews:\n  - name: user_reference\n    source: duckdb          # Attached DuckDB database\n    database: reference     # Alias from attachments section\n    table: users            # Table name in attached database\n    description: \"User reference data from local DuckDB\"\n\n  - name: sales_data\n    source: duckdb\n    database: analytics\n    table: sales\n    description: \"Sales data from analytics database\"\n\n  - name: customer_prefs\n    source: sqlite          # SQLite attachment\n    database: legacy        # SQLite alias\n    table: customer_preferences\n    description: \"Customer preferences from legacy system\"\n</code></pre> <p>Key configuration elements: - <code>attachments</code> section defines external databases - <code>alias</code> provides reference name used in views - <code>read_only: true</code> prevents accidental data modification - Views reference attached databases by alias - Different source types: <code>duckdb</code>, <code>sqlite</code>, <code>postgres</code></p>"},{"location":"examples/local-attachments/#advanced-attachment-patterns","title":"Advanced Attachment Patterns","text":""},{"location":"examples/local-attachments/#multiple-tables-from-same-database","title":"Multiple Tables from Same Database","text":"<pre><code>views:\n  # Access multiple tables from the same attachment\n  - name: user_profiles\n    source: duckdb\n    database: reference\n    table: users\n    description: \"User profiles\"\n\n  - name: user_stats\n    source: duckdb\n    database: reference\n    table: user_statistics  # Different table, same database\n    description: \"User statistics\"\n\n  - name: sales_summary\n    source: duckdb\n    database: analytics\n    table: sales\n    description: \"Raw sales transactions\"\n</code></pre>"},{"location":"examples/local-attachments/#cross-database-joins","title":"Cross-Database Joins","text":"<pre><code>  # Join data across attached databases\n  - name: user_sales_enriched\n    sql: |\n      SELECT \n        u.user_id,\n        u.name,\n        u.email,\n        u.country,\n        u.segment,\n        p.preferred_categories,\n        p.communication_style,\n        COUNT(s.sale_id) as total_sales,\n        SUM(s.amount) as total_revenue,\n        AVG(s.amount) as avg_sale_amount\n      FROM user_profiles u\n      LEFT JOIN customer_prefs p ON u.user_id = p.user_id\n      LEFT JOIN sales_data s ON u.user_id = s.user_id\n      GROUP BY u.user_id, u.name, u.email, u.country, u.segment, \n               p.preferred_categories, p.communication_style\n      ORDER BY total_revenue DESC NULLS LAST\n    description: \"Unified user view combining all data sources\"\n\n  - name: sales_by_region\n    sql: |\n      SELECT \n        u.country,\n        u.segment,\n        COUNT(DISTINCT u.user_id) as user_count,\n        COUNT(s.sale_id) as total_sales,\n        SUM(s.amount) as total_revenue,\n        AVG(s.amount) as avg_sale_value\n      FROM user_profiles u\n      JOIN sales_data s ON u.user_id = s.user_id\n      GROUP BY u.country, u.segment\n      ORDER BY total_revenue DESC\n    description: \"Sales performance by region and segment\"\n</code></pre>"},{"location":"examples/local-attachments/#read-only-vs-read-write-attachments","title":"Read-Only vs Read-Write Attachments","text":""},{"location":"examples/local-attachments/#read-only-attachments-recommended","title":"Read-Only Attachments (Recommended)","text":"<pre><code>attachments:\n  duckdb:\n    - alias: reference\n      path: ./reference_data.duckdb\n      read_only: true        # Safe - prevents modifications\n</code></pre> <p>Benefits: - Prevents accidental data corruption - Allows safe concurrent access - Better for production use - Clear data provenance</p> <p>Use when: - Reference data that shouldn't change - Historical data - Data shared across multiple processes - Production environments</p>"},{"location":"examples/local-attachments/#read-write-attachments-use-carefully","title":"Read-Write Attachments (Use Carefully)","text":"<pre><code>attachments:\n  duckdb:\n    - alias: staging\n      path: ./staging_data.duckdb\n      read_only: false       # Allows modifications\n\n  sqlite:\n    - alias: legacy\n      path: ./legacy_system.db\n      read_only: false\n</code></pre> <p>Considerations: - Use only when you need to modify source data - Be aware of potential concurrency issues - Ensure proper backup strategies - Consider performance implications</p> <p>Use when: - Staging area for data processing - Temporary transformations - Controlled ETL processes</p>"},{"location":"examples/local-attachments/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"examples/local-attachments/#1-create-configuration-file","title":"1. Create Configuration File","text":"<p>Save the configuration above as <code>local-attachments.yaml</code>.</p>"},{"location":"examples/local-attachments/#2-validate-configuration","title":"2. Validate Configuration","text":"<pre><code>duckalog validate local-attachments.yaml\n</code></pre> <p>Expected output: <pre><code>\u2705 Configuration is valid\n\u2705 All database attachments found\n\u2705 All views defined correctly\n</code></pre></p>"},{"location":"examples/local-attachments/#3-generate-sql-optional","title":"3. Generate SQL (Optional)","text":"<p>Preview the SQL that will be executed:</p> <pre><code>duckalog generate-sql local-attachments.yaml --output attachments.sql\ncat attachments.sql\n</code></pre>"},{"location":"examples/local-attachments/#4-build-the-catalog","title":"4. Build the Catalog","text":"<pre><code>duckalog build local-attachments.yaml\n</code></pre> <p>This creates <code>unified_catalog.duckdb</code> with views over your attached databases.</p>"},{"location":"examples/local-attachments/#5-query-your-data","title":"5. Query Your Data","text":"<pre><code># Connect with DuckDB\nduckdb unified_catalog.duckdb\n\n# Example queries:\n# Simple view access\nSELECT * FROM user_reference LIMIT 10;\n\n# Cross-database join\nSELECT \n  u.name,\n  u.country,\n  COUNT(s.sale_id) as sales_count,\n  SUM(s.amount) as total_revenue\nFROM user_reference u\nLEFT JOIN sales_data s ON u.user_id = s.user_id\nGROUP BY u.user_id, u.name, u.country\nORDER BY total_revenue DESC;\n\n# Complex analytics across sources\nSELECT * FROM user_sales_enriched WHERE total_sales &gt; 0;\n</code></pre>"},{"location":"examples/local-attachments/#6-use-programmatically","title":"6. Use Programmatically","text":"<pre><code>from duckalog import load_config, build_catalog\nimport duckdb\n\n# Build catalog\nbuild_catalog(\"local-attachments.yaml\")\n\n# Connect to unified catalog\ncon = duckdb.connect(\"unified_catalog.duckdb\")\n\n# Simple queries\nusers_df = con.execute(\"SELECT * FROM user_reference WHERE country = 'US'\").df()\nprint(\"US Users:\")\nprint(users_df)\n\n# Cross-database analytics\nanalytics_df = con.execute(\"\"\"\n    SELECT \n      country,\n      segment,\n      COUNT(*) as user_count,\n      SUM(total_revenue) as segment_revenue\n    FROM user_sales_enriched\n    GROUP BY country, segment\n    ORDER BY segment_revenue DESC\n\"\"\").df()\nprint(\"\\nRevenue by Country/Segment:\")\nprint(analytics_df)\n\n# Find power users\npower_users = con.execute(\"\"\"\n    SELECT name, email, total_sales, total_revenue\n    FROM user_sales_enriched\n    WHERE total_sales &gt;= 5\n    ORDER BY total_revenue DESC\n\"\"\").df()\nprint(\"\\nPower Users:\")\nprint(power_users)\n</code></pre>"},{"location":"examples/local-attachments/#performance-optimization","title":"Performance Optimization","text":""},{"location":"examples/local-attachments/#memory-management","title":"Memory Management","text":"<pre><code>duckdb:\n  pragmas:\n    - \"SET memory_limit='2GB'\"      # Adjust based on available RAM\n    - \"SET threads=4\"               # Match CPU cores\n    - \"SET temp_directory='/tmp/duckdb_temp'\"  # For large operations\n    - \"SET wal_autocheckpoint='1GB'\" # For read-write attachments\n</code></pre>"},{"location":"examples/local-attachments/#large-dataset-considerations","title":"Large Dataset Considerations","text":"<pre><code># For large databases, consider selective views\nviews:\n  - name: recent_users\n    sql: |\n      SELECT * FROM user_reference\n      WHERE signup_date &gt;= CURRENT_DATE - INTERVAL 365 DAYS\n    description: \"Users from last year\"\n\n  - name: high_value_sales\n    sql: |\n      SELECT * FROM sales_data\n      WHERE amount &gt;= 100.0\n    description: \"Sales above $100\"\n</code></pre>"},{"location":"examples/local-attachments/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<p>This example teaches:</p> <ol> <li>Database Attachment: How to connect external DuckDB/SQLite databases</li> <li>Read-Only Safety: Best practices for safe data access</li> <li>Cross-Database Joins: Joining data across multiple local databases</li> <li>Alias Management: Organizing and referencing attached databases</li> <li>Performance Optimization: Memory and threading for local work</li> <li>Data Unification: Combining disparate data sources</li> <li>SQL Composition: Building complex analytics across sources</li> </ol>"},{"location":"examples/local-attachments/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/local-attachments/#common-issues","title":"Common Issues","text":"<p>1. Database File Not Found: <pre><code># Check file exists and path is correct\nls -la reference_data.duckdb\nls -la analytics_data.duckdb\nls -la legacy_system.db\n\n# Verify current directory\npwd\n</code></pre></p> <p>2. Permission Errors: <pre><code># Check read permissions\nls -l *.duckdb *.db\n\n# Fix permissions if needed\nchmod 644 *.duckdb *.db\n\n# For read-write access, ensure write permissions\nchmod 666 *.duckdb *.db\n</code></pre></p> <p>3. Schema/Table Not Found: <pre><code>-- Check available tables in attached database\nSHOW TABLES FROM reference;\nSHOW TABLES FROM analytics;\nSHOW TABLES FROM legacy;\n</code></pre></p> <p>4. Memory Issues: <pre><code># Reduce memory usage\nduckdb:\n  pragmas:\n    - \"SET memory_limit='512MB'\"\n    - \"SET threads=2\"\n</code></pre></p> <p>5. Slow Cross-Database Queries: <pre><code># Optimize for local performance\nduckdb:\n  pragmas:\n    - \"SET threads=8\"                    # Use more cores\n    - \"SET temp_directory='/fast/ssd/'\"  # Use fast storage\n</code></pre></p>"},{"location":"examples/local-attachments/#debug-commands","title":"Debug Commands","text":"<pre><code>-- List all attached databases\nDATABASE_LIST;\n\n-- Show schema for attached database\nDESCRIBE reference.users;\n\n-- Check query plan for optimization\nEXPLAIN SELECT * FROM user_sales_enriched;\n\n-- Monitor memory usage\nPRAGMA memory_limit;\nPRAGMA threads;\n</code></pre>"},{"location":"examples/local-attachments/#variations","title":"Variations","text":""},{"location":"examples/local-attachments/#backup-and-restore-pattern","title":"Backup and Restore Pattern","text":"<pre><code># Create backup before building catalog\n- name: backup_step\n  sql: |\n    -- Backup critical reference data\n    CREATE TABLE backup_users AS SELECT * FROM user_reference;\n</code></pre>"},{"location":"examples/local-attachments/#incremental-data-loading","title":"Incremental Data Loading","text":"<pre><code># Load only new/changed data\nviews:\n  - name: updated_sales\n    sql: |\n      SELECT * FROM sales_data\n      WHERE sale_date &gt;= (SELECT MAX(sale_date) FROM backup_sales)\n</code></pre>"},{"location":"examples/local-attachments/#data-quality-checks","title":"Data Quality Checks","text":"<pre><code># Add data quality views\n- name: data_quality_report\n  sql: |\n    SELECT \n      'user_reference' as table_name,\n      COUNT(*) as record_count,\n      COUNT(DISTINCT user_id) as unique_users,\n      COUNT(*) - COUNT(email) as missing_emails\n    FROM user_reference\n    UNION ALL\n    SELECT \n      'sales_data' as table_name,\n      COUNT(*) as record_count,\n      COUNT(DISTINCT user_id) as unique_users,\n      COUNT(*) - COUNT(amount) as missing_amounts\n    FROM sales_data\n</code></pre> <p>This example provides a solid foundation for working with local database attachments in Duckalog. The patterns shown here can be extended to handle more complex scenarios and larger datasets.</p>"},{"location":"examples/local-attachments/#security-considerations","title":"Security Considerations","text":""},{"location":"examples/local-attachments/#file-permissions","title":"File Permissions","text":"<pre><code># Set appropriate permissions for databases\nchmod 640 reference_data.duckdb        # Read/write for owner, read for group\nchmod 644 legacy_system.db             # Read-only for all\n\n# For sensitive data, consider encryption\nchmod 600 sensitive_data.duckdb        # Owner read/write only\n</code></pre>"},{"location":"examples/local-attachments/#data-access-control","title":"Data Access Control","text":"<pre><code># Use views to control data access\nviews:\n  - name: public_user_data\n    sql: |\n      SELECT user_id, name, country  -- Exclude sensitive fields\n      FROM user_reference\n</code></pre> <p>This attachment pattern enables powerful data integration while maintaining security and performance. Adapt these patterns to your specific local data landscape and requirements.</p>"},{"location":"examples/multi-source-analytics/","title":"Multi-Source Analytics Example","text":"<p>This example demonstrates how to use Duckalog to build a comprehensive analytics catalog that combines data from multiple sources into a unified view. You'll learn how to configure attachments, Iceberg catalogs, and cloud storage to create powerful analytical queries.</p>"},{"location":"examples/multi-source-analytics/#business-scenario","title":"Business Scenario","text":"<p>Imagine you're building an analytics platform for a growing company. Your data lives in several places: - Raw events stored as Parquet files in S3 (high-volume, cost-effective storage) - Reference data in local DuckDB databases (fast, local access) - Legacy customer data in SQLite (existing systems) - Product information in PostgreSQL (operational systems) - Processed event data in Iceberg tables (data warehouse)</p> <p>This example shows how to unify all these sources into a single DuckDB catalog that enables rich analytics across your entire data landscape.</p>"},{"location":"examples/multi-source-analytics/#prerequisites","title":"Prerequisites","text":"<p>Before running this example, ensure you have:</p> <ol> <li> <p>Python 3.9+ with Duckalog installed:    <pre><code>pip install duckalog\n</code></pre></p> </li> <li> <p>Required environment variables (see Environment Variables section below)</p> </li> <li> <p>Sample data sources (or use the provided examples with mock paths):</p> </li> <li>S3 bucket with Parquet files</li> <li>Iceberg catalog access (e.g., AWS Glue, Tabular)</li> <li>PostgreSQL database</li> <li>Local DuckDB/SQLite databases (can be created for testing)</li> </ol>"},{"location":"examples/multi-source-analytics/#complete-configuration","title":"Complete Configuration","text":"<p>Here's the full configuration with explanations for each section:</p>"},{"location":"examples/multi-source-analytics/#base-configuration","title":"Base Configuration","text":"<pre><code># Configuration file version\nversion: 1\n\n# DuckDB database settings\nduckdb:\n  # Output database file\n  database: multi_source.duckdb\n\n  # Extensions needed for cloud and data lake access\n  install_extensions:\n    - httpfs          # For HTTP/S3 file access\n    - iceberg         # For Iceberg table support\n\n  # Performance and behavior settings\n  pragmas:\n    - \"SET memory_limit='2GB'\"      # Limit memory usage\n    - \"SET threads=4\"               # Parallel processing\n    - \"SET timezone='UTC'\"          # Consistent time handling\n\n    # Cloud storage configuration\n    # These settings enable S3 access for Parquet files\n    - \"SET s3_region='us-west-2'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n    - \"SET s3_session_token='${env:AWS_SESSION_TOKEN}'\"  # Optional for temp credentials\n</code></pre> <p>Key Concepts Demonstrated: - Extension installation for extended functionality - Memory and threading optimization for performance - Environment variable injection for credentials - Timezone consistency for global data</p>"},{"location":"examples/multi-source-analytics/#database-attachments","title":"Database Attachments","text":"<pre><code># External database attachments\nattachments:\n  # Local DuckDB databases (read-only for safety)\n  duckdb:\n    - alias: reference         # How you'll reference this database in views\n      path: ./reference_data.duckdb     # Local file path\n      read_only: true         # Prevent accidental modifications\n\n    - alias: historical\n      path: ./historical_data.duckdb\n      read_only: true\n\n  # Legacy system in SQLite format\n  sqlite:\n    - alias: legacy           # Reference name for SQL queries\n      path: ./legacy_system.db  # Local SQLite database file\n\n  # Production and staging PostgreSQL databases\n  postgres:\n    - alias: prod_db         # Production database reference\n      host: \"${env:PROD_DB_HOST}\"    # Environment variable for security\n      port: 5432\n      database: analytics_prod\n      user: \"${env:PROD_DB_USER}\"\n      password: \"${env:PROD_DB_PASSWORD}\"\n      sslmode: require        # Secure connection\n\n    - alias: staging_db      # Staging environment\n      host: \"${env:STAGING_DB_HOST}\"\n      port: 5432\n      database: analytics_staging\n      user: \"${env:STAGING_DB_USER}\"\n      password: \"${env:STAGING_DB_PASSWORD}\"\n      sslmode: require\n</code></pre> <p>Key Concepts Demonstrated: - Multiple database attachment types (DuckDB, SQLite, PostgreSQL) - Read-only attachments for safety - Environment variable usage for credentials - SSL configuration for secure connections - Environment-specific configurations (prod vs staging)</p>"},{"location":"examples/multi-source-analytics/#iceberg-catalogs","title":"Iceberg Catalogs","text":"<pre><code># Iceberg data lake catalogs\niceberg_catalogs:\n  - name: production_iceberg    # Reference name used in views\n    catalog_type: rest          # REST catalog (vs Hadoop, Spark, etc.)\n    uri: \"https://iceberg-catalog.production.company.com\"  # Catalog endpoint\n    warehouse: \"s3://company-data-warehouse/production/\"   # Storage location\n    options:\n      token: \"${env:ICEBERG_PROD_TOKEN}\"  # Authentication\n\n  - name: staging_iceberg       # Staging environment catalog\n    catalog_type: rest\n    uri: \"https://iceberg-catalog.staging.company.com\"\n    warehouse: \"s3://company-data-warehouse/staging/\"\n    options:\n      token: \"${env:ICEBERG_STAGING_TOKEN}\"\n</code></pre> <p>Key Concepts Demonstrated: - REST catalog configuration (most common for cloud data lakes) - Environment-specific catalog separation - Token-based authentication - Warehouse path configuration</p>"},{"location":"examples/multi-source-analytics/#views-definition","title":"Views Definition","text":"<p>Views are the core of your analytics - they define how data is accessed and transformed:</p> <pre><code># Data access and transformation views\nviews:\n  # Raw data from different sources\n  - name: raw_events                    # View name for querying\n    source: parquet                     # Data source type\n    uri: \"s3://company-data-lake/events/raw/*.parquet\"  # File pattern\n    description: \"Raw events from data lake\"\n\n  - name: processed_events\n    source: iceberg                     # Iceberg table access\n    catalog: production_iceberg         # Reference to configured catalog\n    table: analytics.processed_events   # Schema.table format\n    description: \"Processed events from production Iceberg catalog\"\n\n  - name: user_profiles\n    source: duckdb                      # Attached DuckDB database\n    database: reference                 # Alias from attachments section\n    table: users                        # Table name in attached database\n    description: \"User reference data\"\n\n  - name: legacy_customers\n    source: sqlite                      # SQLite attachment\n    database: legacy                    # SQLite alias\n    table: customers\n    description: \"Legacy customer data from SQLite\"\n\n  - name: product_data\n    source: postgres                    # PostgreSQL attachment\n    database: prod_db                   # Production database alias\n    table: products\n    description: \"Product data from production Postgres\"\n</code></pre> <p>Key Concepts Demonstrated: - Multiple source types in unified catalog - Schema.table naming for structured data - Descriptive metadata for documentation</p>"},{"location":"examples/multi-source-analytics/#advanced-analytics-views","title":"Advanced Analytics Views","text":"<p>These views demonstrate complex analytics across multiple data sources:</p> <pre><code>  # Enriched analytics views\n  - name: enriched_events\n    sql: |\n      SELECT\n        e.event_id,\n        e.timestamp,\n        e.event_type,\n        e.user_id,\n        e.session_id,\n        e.properties,\n        u.name as user_name,           # Join user reference data\n        u.email,\n        u.segment,\n        p.name as product_name,        # Join product catalog\n        p.category as product_category\n      FROM raw_events e\n      LEFT JOIN user_profiles u ON e.user_id = u.id\n      LEFT JOIN product_data p ON e.properties-&gt;&gt;'product_id' = p.id\n    description: \"Events enriched with user and product data\"\n\n  - name: event_metrics\n    sql: |\n      SELECT\n        DATE(timestamp) as event_date,     # Daily aggregation\n        event_type,\n        COUNT(*) as event_count,\n        COUNT(DISTINCT user_id) as unique_users,\n        COUNT(DISTINCT session_id) as unique_sessions\n      FROM enriched_events\n      GROUP BY DATE(timestamp), event_type\n      ORDER BY event_date DESC, event_count DESC\n    description: \"Daily event metrics for reporting\"\n\n  - name: user_activity_summary\n    sql: |\n      SELECT\n        u.id as user_id,\n        u.name,\n        u.email,\n        u.segment,\n        COUNT(DISTINCT DATE(ee.timestamp)) as active_days,\n        COUNT(DISTINCT ee.session_id) as total_sessions,\n        COUNT(*) as total_events,\n        MAX(ee.timestamp) as last_activity\n      FROM user_profiles u\n      LEFT JOIN enriched_events ee ON u.id = ee.user_id\n      GROUP BY u.id, u.name, u.email, u.segment\n      ORDER BY total_events DESC\n    description: \"User engagement summary\"\n\n  - name: product_performance\n    sql: |\n      SELECT\n        p.id as product_id,\n        p.name as product_name,\n        p.category,\n        COUNT(*) as event_count,\n        COUNT(DISTINCT ee.user_id) as unique_users,\n        MAX(ee.timestamp) as last_mentioned\n      FROM product_data p\n      JOIN enriched_events ee ON p.id = ee.properties-&gt;&gt;'product_id'\n      GROUP BY p.id, p.name, p.category\n      ORDER BY event_count DESC\n    description: \"Product engagement analysis\"\n</code></pre> <p>Key Concepts Demonstrated: - SQL view composition (building on previous views) - Cross-source joins (Parquet + DuckDB + PostgreSQL) - JSON field access (<code>properties-&gt;&gt;'product_id'</code>) - Aggregation and window functions - Complex business logic implementation</p>"},{"location":"examples/multi-source-analytics/#executive-reporting","title":"Executive Reporting","text":"<pre><code>  - name: daily_kpi_report\n    sql: |\n      WITH daily_metrics AS (\n        SELECT\n          DATE(timestamp) as event_date,\n          COUNT(*) as total_events,\n          COUNT(DISTINCT user_id) as daily_active_users,\n          COUNT(DISTINCT session_id) as daily_sessions\n        FROM enriched_events\n        GROUP BY DATE(timestamp)\n      )\n      SELECT\n        event_date,\n        total_events,\n        daily_active_users,\n        daily_sessions,\n        ROUND(total_events * 1.0 / daily_sessions, 2) as events_per_session,\n        LAG(daily_active_users) OVER (ORDER BY event_date) as prev_day_users,\n        ROUND((daily_active_users - LAG(daily_active_users) OVER (ORDER BY event_date)) * 100.0 /\n              LAG(daily_active_users) OVER (ORDER BY event_date), 2) as user_growth_pct\n      FROM daily_metrics\n      ORDER BY event_date DESC\n    description: \"Executive KPI dashboard data\"\n</code></pre> <p>Key Concepts Demonstrated: - Common Table Expressions (CTEs) - Window functions for trend analysis - Percentage growth calculations - Executive-level metrics aggregation</p>"},{"location":"examples/multi-source-analytics/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"examples/multi-source-analytics/#1-prepare-environment-variables","title":"1. Prepare Environment Variables","text":"<p>Create a <code>.env</code> file or export variables in your shell:</p> <pre><code># AWS credentials for S3 access\nexport AWS_ACCESS_KEY_ID=\"your_aws_access_key\"\nexport AWS_SECRET_ACCESS_KEY=\"your_aws_secret_key\"\nexport AWS_SESSION_TOKEN=\"your_session_token\"  # Optional\n\n# PostgreSQL credentials\nexport PROD_DB_HOST=\"prod-db.company.com\"\nexport PROD_DB_USER=\"analytics_user\"\nexport PROD_DB_PASSWORD=\"secure_password\"\n\nexport STAGING_DB_HOST=\"staging-db.company.com\"\nexport STAGING_DB_USER=\"analytics_staging\"\nexport STAGING_DB_PASSWORD=\"staging_password\"\n\n# Iceberg catalog tokens\nexport ICEBERG_PROD_TOKEN=\"your_production_catalog_token\"\nexport ICEBERG_STAGING_TOKEN=\"your_staging_catalog_token\"\n</code></pre>"},{"location":"examples/multi-source-analytics/#2-validate-configuration","title":"2. Validate Configuration","text":"<p>Before building, validate your configuration:</p> <pre><code>duckalog validate docs/examples/multi-source-analytics-config.yaml\n</code></pre> <p>This checks: - YAML syntax - Environment variable resolution - View definitions - Attachment configurations</p>"},{"location":"examples/multi-source-analytics/#3-generate-sql-optional","title":"3. Generate SQL (Optional)","text":"<p>Preview the SQL that will be executed:</p> <pre><code>duckalog generate-sql docs/examples/multi-source-analytics-config.yaml --output preview.sql\ncat preview.sql\n</code></pre>"},{"location":"examples/multi-source-analytics/#4-build-the-catalog","title":"4. Build the Catalog","text":"<p>Create your unified analytics catalog:</p> <pre><code>duckalog build docs/examples/multi-source-analytics-config.yaml\n</code></pre> <p>This will: - Install required DuckDB extensions - Create <code>multi_source.duckdb</code> database - Set up all attachments - Create all views and joins</p>"},{"location":"examples/multi-source-analytics/#5-query-your-data","title":"5. Query Your Data","text":"<p>Connect to the unified catalog:</p> <pre><code># Using DuckDB CLI\nduckdb multi_source.duckdb\n\n# Example queries in DuckDB:\nSELECT * FROM daily_kpi_report ORDER BY event_date DESC LIMIT 10;\n\nSELECT \n  event_type,\n  COUNT(*) as events,\n  COUNT(DISTINCT user_id) as users\nFROM enriched_events \nWHERE DATE(timestamp) &gt;= CURRENT_DATE - INTERVAL 7 DAYS\nGROUP BY event_type\nORDER BY events DESC;\n</code></pre>"},{"location":"examples/multi-source-analytics/#6-use-programmatically","title":"6. Use Programmatically","text":"<pre><code>from duckalog import load_config\nimport duckdb\n\n# Load configuration\nconfig = load_config(\"docs/examples/multi-source-analytics-config.yaml\")\n\n# Connect to the created catalog\ncon = duckdb.connect(\"multi_source.duckdb\")\n\n# Run analytics queries\ndf = con.execute(\"\"\"\n    SELECT * FROM user_activity_summary \n    WHERE active_days &gt;= 7 \n    ORDER BY total_events DESC\n\"\"\").df()\n\nprint(df.head())\n</code></pre>"},{"location":"examples/multi-source-analytics/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<p>This example teaches several important Duckalog patterns:</p> <ol> <li>Multi-Source Unification: How to bring together Parquet, DuckDB, SQLite, PostgreSQL, and Iceberg</li> <li>Environment-Based Configuration: Using environment variables for different deployments</li> <li>Progressive Data Enrichment: Building from raw data to enriched analytics</li> <li>Performance Optimization: Memory limits, threading, and read-only attachments</li> <li>Security Best Practices: No hardcoded credentials, read-only when possible</li> <li>SQL Composition: Building complex analytics through view composition</li> <li>Business Logic: Implementing real analytics patterns (KPI reporting, user engagement)</li> </ol>"},{"location":"examples/multi-source-analytics/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/multi-source-analytics/#common-issues","title":"Common Issues","text":"<p>Missing Environment Variables: <pre><code># Check what variables are missing\nduckalog validate docs/examples/multi-source-analytics-config.yaml\n# Look for \"Environment variable 'VAR_NAME' not found\" errors\n</code></pre></p> <p>S3 Connection Errors: <pre><code># Verify AWS credentials\naws s3 ls s3://company-data-lake/events/raw/\n# Check S3 region and credentials in config\n</code></pre></p> <p>Database Connection Failures: <pre><code># Test PostgreSQL connection manually\npsql -h prod-db.company.com -U analytics_user -d analytics_prod\n# Verify SSL settings and firewall rules\n</code></pre></p> <p>Iceberg Catalog Issues: <pre><code># Check catalog accessibility\ncurl -H \"Authorization: Bearer $ICEBERG_PROD_TOKEN\" \\\n     \"https://iceberg-catalog.production.company.com/v1/config\"\n</code></pre></p>"},{"location":"examples/multi-source-analytics/#performance-tips","title":"Performance Tips","text":"<ol> <li>Memory Limits: Adjust <code>memory_limit</code> based on available RAM</li> <li>Threading: Set <code>threads</code> to number of CPU cores</li> <li>Read-Only Attachments: Use <code>read_only: true</code> when possible</li> <li>Query Optimization: Use <code>generate-sql</code> to optimize complex views</li> <li>Index Consideration: Ensure source tables have appropriate indexes</li> </ol>"},{"location":"examples/multi-source-analytics/#variations-and-customizations","title":"Variations and Customizations","text":""},{"location":"examples/multi-source-analytics/#environment-specific-configs","title":"Environment-Specific Configs","text":"<p>Create separate configs for different environments:</p> <p><code>config-production.yaml</code>: <pre><code>iceberg_catalogs:\n  - name: main\n    catalog_type: rest\n    uri: \"https://iceberg.company.com/prod\"\n    # Production settings\n</code></pre></p> <p><code>config-staging.yaml</code>: <pre><code>iceberg_catalogs:\n  - name: main\n    catalog_type: rest\n    uri: \"https://iceberg.company.com/staging\"\n    # Staging settings\n</code></pre></p>"},{"location":"examples/multi-source-analytics/#incremental-loading","title":"Incremental Loading","text":"<p>For large datasets, consider incremental views:</p> <pre><code>- name: recent_events\n  sql: |\n    SELECT * FROM enriched_events\n    WHERE DATE(timestamp) &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n</code></pre>"},{"location":"examples/multi-source-analytics/#custom-aggregations","title":"Custom Aggregations","text":"<p>Add domain-specific aggregations:</p> <pre><code>- name: cohort_analysis\n  sql: |\n    SELECT \n      DATE_TRUNC('week', u.signup_date) as cohort_week,\n      COUNT(DISTINCT u.id) as cohort_size,\n      COUNT(DISTINCT ee.user_id) as retained_users\n    FROM user_profiles u\n    LEFT JOIN enriched_events ee ON u.id = ee.user_id\n    GROUP BY DATE_TRUNC('week', u.signup_date)\n</code></pre> <p>This example demonstrates Duckalog's power to unify diverse data sources into a coherent analytics platform. Adapt the patterns shown here to your specific data landscape and business requirements.</p>"},{"location":"examples/simple-parquet/","title":"Simple Parquet Example","text":"<p>This example shows how to create DuckDB views over Parquet files stored in cloud storage. It's perfect for getting started with Duckalog when your data is already in Parquet format.</p>"},{"location":"examples/simple-parquet/#when-to-use-this-example","title":"When to Use This Example","text":"<p>Choose this example if: - Your data is in Parquet files (local or S3) - You want to query Parquet data with SQL - You're working with partitioned data - You need simple, fast analytics without complex joins - You want to combine multiple Parquet datasets</p>"},{"location":"examples/simple-parquet/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Duckalog installed: <pre><code>pip install duckalog\n</code></pre></p> </li> <li> <p>Sample Parquet files - You can use your own or create test data:    <pre><code># Create sample Parquet files for testing\nimport pandas as pd\nimport duckdb\n\n# Create sample data\nusers_df = pd.DataFrame({\n    'user_id': range(1, 101),\n    'name': [f'User {i}' for i in range(1, 101)],\n    'signup_date': pd.date_range('2023-01-01', periods=100),\n    'region': ['US', 'EU', 'APAC'] * 33 + ['US']\n})\n\nevents_df = pd.DataFrame({\n    'event_id': range(1, 1001),\n    'user_id': [i % 100 + 1 for i in range(1000)],\n    'event_type': ['page_view', 'click', 'purchase'] * 333 + ['page_view'],\n    'timestamp': pd.date_range('2023-01-01', periods=1000, freq='H'),\n    'value': [i * 0.5 for i in range(1000)]\n})\n\n# Save as Parquet\nusers_df.to_parquet('./users.parquet')\nevents_df.to_parquet('./events.parquet')\n\n# Or upload to S3 (if you have AWS credentials configured)\nimport boto3\ns3 = boto3.client('s3')\nusers_df.to_parquet('s3://your-bucket/data/users.parquet')\nevents_df.to_parquet('s3://your-bucket/data/events.parquet')\n</code></pre></p> </li> </ol>"},{"location":"examples/simple-parquet/#basic-configuration-pattern","title":"Basic Configuration Pattern","text":""},{"location":"examples/simple-parquet/#single-view-example","title":"Single View Example","text":"<p>Create a file called <code>simple-parquet.yaml</code>:</p> <pre><code>version: 1\n\n# DuckDB configuration\nduckdb:\n  database: simple_catalog.duckdb\n  install_extensions:\n    - httpfs  # Required for cloud storage access\n\n  pragmas:\n    # Performance settings\n    - \"SET memory_limit='1GB'\"\n    - \"SET threads=2\"\n\n    # S3 configuration (if using cloud storage)\n    - \"SET s3_region='us-east-1'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n\n# View definitions\nviews:\n  # Simple single-table view\n  - name: users\n    source: parquet\n    uri: \"s3://your-bucket/data/users.parquet\"  # Or local: \"./users.parquet\"\n    description: \"User reference data from Parquet\"\n</code></pre> <p>Key configuration elements: - <code>source: parquet</code> - Specifies Parquet file source - <code>uri</code> - Path to Parquet file (supports wildcards for partitioned data) - Extension installation for cloud access - S3 credentials via environment variables</p>"},{"location":"examples/simple-parquet/#multi-view-example-with-joins","title":"Multi-View Example with Joins","text":"<p>For more complex analytics, define multiple views:</p> <pre><code>version: 1\n\nduckdb:\n  database: analytics_catalog.duckdb\n  install_extensions:\n    - httpfs\n\n  pragmas:\n    - \"SET memory_limit='2GB'\"\n    - \"SET s3_region='us-east-1'\"\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n\nviews:\n  # Raw data views\n  - name: raw_users\n    source: parquet\n    uri: \"s3://your-bucket/data/users/*.parquet\"  # Supports partitioned data\n    description: \"User data from partitioned Parquet files\"\n\n  - name: raw_events\n    source: parquet\n    uri: \"s3://your-bucket/data/events/*.parquet\"\n    description: \"Event data from partitioned Parquet files\"\n\n  # Derived views with analytics\n  - name: daily_active_users\n    sql: |\n      SELECT \n        DATE(timestamp) as event_date,\n        COUNT(DISTINCT user_id) as active_users,\n        COUNT(*) as total_events\n      FROM raw_events\n      GROUP BY DATE(timestamp)\n      ORDER BY event_date DESC\n    description: \"Daily active user metrics\"\n\n  - name: user_summary\n    sql: |\n      SELECT \n        u.user_id,\n        u.name,\n        u.region,\n        u.signup_date,\n        COUNT(DISTINCT DATE(e.timestamp)) as active_days,\n        COUNT(e.event_id) as total_events,\n        MAX(e.timestamp) as last_activity\n      FROM raw_users u\n      LEFT JOIN raw_events e ON u.user_id = e.user_id\n      GROUP BY u.user_id, u.name, u.region, u.signup_date\n      ORDER BY total_events DESC NULLS LAST\n    description: \"User engagement summary\"\n\n  - name: popular_content\n    sql: |\n      SELECT \n        event_type,\n        COUNT(*) as event_count,\n        COUNT(DISTINCT user_id) as unique_users,\n        AVG(value) as avg_value\n      FROM raw_events\n      WHERE event_type IN ('page_view', 'click', 'purchase')\n      GROUP BY event_type\n      ORDER BY event_count DESC\n    description: \"Most popular content types\"\n</code></pre>"},{"location":"examples/simple-parquet/#s3-configuration-deep-dive","title":"S3 Configuration Deep Dive","text":""},{"location":"examples/simple-parquet/#environment-variables","title":"Environment Variables","text":"<p>Set your AWS credentials as environment variables:</p> <pre><code># Option 1: Direct environment variables\nexport AWS_ACCESS_KEY_ID=\"AKIA...\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_SESSION_TOKEN=\"optional-session-token\"  # For temporary credentials\n\n# Option 2: Using AWS CLI\naws configure set aws_access_key_id AKIA...\naws configure set aws_secret_access_key your-secret-key\n\n# Option 3: Using IAM roles (for EC2/ECS)\n# IAM role attached to instance/container\n</code></pre>"},{"location":"examples/simple-parquet/#s3-uri-patterns","title":"S3 URI Patterns","text":"<p>Duckalog supports various S3 URI patterns:</p> <pre><code># Single file\nuri: \"s3://bucket/path/file.parquet\"\n\n# All files in directory\nuri: \"s3://bucket/path/*.parquet\"\n\n# Partitioned data with pattern\nuri: \"s3://bucket/year=2023/month=01/*.parquet\"\n\n# Specific date range\nuri: \"s3://bucket/data/2023-01-*.parquet\"\n\n# Multiple patterns (use SQL view composition)\nuri: \"s3://bucket/events/2023-*-*.parquet\"\n</code></pre>"},{"location":"examples/simple-parquet/#performance-considerations","title":"Performance Considerations","text":"<pre><code>duckdb:\n  pragmas:\n    # Optimize for Parquet reading\n    - \"SET threads=4\"                    # Match your CPU cores\n    - \"SET memory_limit='2GB'           # Set based on data size\n    - \"SET s3_region='us-east-1'        # Use same region as data\n    - \"SET enable_http_metadata_cache=true\"  # Cache HTTP metadata\n</code></pre>"},{"location":"examples/simple-parquet/#step-by-step-usage","title":"Step-by-Step Usage","text":""},{"location":"examples/simple-parquet/#1-create-your-configuration","title":"1. Create Your Configuration","text":"<p>Save the configuration above as <code>simple-parquet.yaml</code>.</p>"},{"location":"examples/simple-parquet/#2-set-environment-variables","title":"2. Set Environment Variables","text":"<pre><code>export AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\n</code></pre>"},{"location":"examples/simple-parquet/#3-validate-configuration","title":"3. Validate Configuration","text":"<pre><code>duckalog validate simple-parquet.yaml\n</code></pre> <p>Expected output: <pre><code>\u2705 Configuration is valid\n\u2705 All views defined correctly\n\u2705 Environment variables resolved\n</code></pre></p>"},{"location":"examples/simple-parquet/#4-generate-sql-optional","title":"4. Generate SQL (Optional)","text":"<p>Preview the SQL that will be executed:</p> <pre><code>duckalog generate-sql simple-parquet.yaml --output generated.sql\n\n# View the generated SQL\ncat generated.sql\n</code></pre>"},{"location":"examples/simple-parquet/#5-build-the-catalog","title":"5. Build the Catalog","text":"<pre><code>duckalog build simple-parquet.yaml\n</code></pre> <p>This creates <code>simple_catalog.duckdb</code> with your Parquet views.</p>"},{"location":"examples/simple-parquet/#6-query-your-data","title":"6. Query Your Data","text":"<pre><code># Connect with DuckDB\nduckdb simple_catalog.duckdb\n\n# Example queries:\nSELECT * FROM users LIMIT 10;\n\nSELECT \n  region,\n  COUNT(*) as user_count\nFROM users \nGROUP BY region\nORDER BY user_count DESC;\n\nSELECT * FROM daily_active_users WHERE event_date &gt;= CURRENT_DATE - INTERVAL 7 DAYS;\n</code></pre>"},{"location":"examples/simple-parquet/#7-use-programmatically","title":"7. Use Programmatically","text":"<pre><code>from duckalog import load_config\nimport duckdb\nimport pandas as pd\n\n# Load and build catalog\nbuild_catalog(\"simple-parquet.yaml\")\n\n# Connect and query\ncon = duckdb.connect(\"simple_catalog.duckdb\")\n\n# Get DataFrame for analysis\nusers_df = con.execute(\"SELECT * FROM users WHERE region = 'US'\").df()\nprint(users_df.head())\n\n# Complex analytics\nmetrics_df = con.execute(\"\"\"\n    SELECT \n      DATE(timestamp) as date,\n      COUNT(DISTINCT user_id) as daily_users,\n      COUNT(*) as total_events,\n      AVG(value) as avg_event_value\n    FROM raw_events\n    WHERE DATE(timestamp) &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n    GROUP BY DATE(timestamp)\n    ORDER BY date\n\"\"\").df()\n\nprint(metrics_df)\n</code></pre>"},{"location":"examples/simple-parquet/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<p>This example teaches:</p> <ol> <li>Parquet as Data Source: How to directly query Parquet files</li> <li>Cloud Storage Access: S3 configuration and credentials</li> <li>Performance Optimization: Memory and threading settings</li> <li>View Composition: Building analytics from raw data</li> <li>SQL in Views: Complex analytics with standard SQL</li> <li>Environment Variables: Secure credential management</li> <li>Partitioned Data: Handling large datasets with wildcards</li> </ol>"},{"location":"examples/simple-parquet/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/simple-parquet/#common-issues","title":"Common Issues","text":"<p>1. S3 Access Denied: <pre><code># Test S3 access\naws s3 ls s3://your-bucket/data/\n\n# Check permissions - user needs:\n# - s3:GetObject for file access\n# - s3:ListBucket for directory listing\n</code></pre></p> <p>2. Parquet File Not Found: <pre><code># Verify file exists\naws s3 ls s3://your-bucket/data/users.parquet\n\n# Check wildcard patterns\naws s3 ls s3://your-bucket/data/*.parquet\n</code></pre></p> <p>3. Memory Errors: <pre><code># Reduce memory usage\nduckdb:\n  pragmas:\n    - \"SET memory_limit='512MB'\"\n</code></pre></p> <p>4. Slow Queries: <pre><code># Increase parallelism\nduckdb:\n  pragmas:\n    - \"SET threads=8\"  # Match CPU cores\n</code></pre></p>"},{"location":"examples/simple-parquet/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Partitioned Data: Organize Parquet files by date/region</li> <li>Optimize File Sizes: 100MB-1GB per file works well</li> <li>Co-locate Data: Store in same S3 region as queries</li> <li>Use Compression: Parquet's built-in compression is efficient</li> <li>Filter Early: Push down filters to Parquet reading when possible</li> </ol>"},{"location":"examples/simple-parquet/#variations","title":"Variations","text":""},{"location":"examples/simple-parquet/#local-files-only","title":"Local Files Only","text":"<p>For local files without cloud access:</p> <pre><code>version: 1\n\nduckdb:\n  database: local_catalog.duckdb\n  # No extensions needed for local files\n  pragmas:\n    - \"SET memory_limit='1GB'\"\n\nviews:\n  - name: local_data\n    source: parquet\n    uri: \"./data/*.parquet\"  # Local path with wildcard\n</code></pre>"},{"location":"examples/simple-parquet/#date-partitioned-data","title":"Date-Partitioned Data","text":"<p>For time-series data:</p> <pre><code>views:\n  - name: events_2023\n    source: parquet\n    uri: \"s3://bucket/events/year=2023/*.parquet\"\n\n  - name: recent_events\n    sql: |\n      SELECT * FROM events_2023\n      WHERE DATE(timestamp) &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n</code></pre> <p>This example provides a solid foundation for working with Parquet data in Duckalog. Adapt the patterns to your specific data structure and requirements.</p>"},{"location":"guides/","title":"User Guide","text":"<p>Welcome to the Duckalog User Guide. This section covers how to use Duckalog effectively, from basic configuration to advanced patterns and troubleshooting.</p>"},{"location":"guides/#getting-started","title":"Getting Started","text":""},{"location":"guides/#configuration-structure","title":"Configuration Structure","text":"<p>At a high level, a Duckalog config looks like this:</p> <pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='1GB'\"\n\nattachments:\n  duckdb:\n    - alias: refdata\n      path: ./refdata.duckdb\n      read_only: true\n\n  sqlite:\n    - alias: legacy\n      path: ./legacy.db\n\n  postgres:\n    - alias: dw\n      host: \"${env:PG_HOST}\"\n      port: 5432\n      database: dw\n      user: \"${env:PG_USER}\"\n      password: \"${env:PG_PASSWORD}\"\n\niceberg_catalogs:\n  - name: main_ic\n    catalog_type: rest\n    uri: \"https://iceberg-catalog.internal\"\n    warehouse: \"s3://my-warehouse/\"\n    options:\n      token: \"${env:ICEBERG_TOKEN}\"\n\nviews:\n  - name: users\n    source: parquet\n    uri: \"s3://my-bucket/data/users/*.parquet\"\n\n  - name: events_delta\n    source: delta\n    uri: \"s3://my-bucket/delta/events\"\n\n  - name: ic_orders\n    source: iceberg\n    catalog: main_ic\n    table: analytics.orders\n\n  - name: ref_countries\n    source: duckdb\n    database: refdata\n    table: reference.countries\n\n  - name: vip_users\n    sql: |\n      SELECT *\n      FROM users\n      WHERE is_vip = TRUE\n</code></pre>"},{"location":"guides/#environment-variables","title":"Environment Variables","text":"<p>Any string may contain <code>${env:VAR_NAME}</code> placeholders. Duckalog resolves these using the process environment before validation. If a variable is missing, a <code>ConfigError</code> is raised.</p> <p>Example: <pre><code>duckdb:\n  pragmas:\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n</code></pre></p>"},{"location":"guides/#core-concepts","title":"Core Concepts","text":""},{"location":"guides/#attachments","title":"Attachments","text":"<p>Attachments let you expose tables from other databases inside DuckDB.</p> <ul> <li>DuckDB attachments: attach additional <code>.duckdb</code> files.</li> <li>SQLite attachments: attach local SQLite databases.</li> <li>Postgres attachments: connect to external Postgres instances.</li> </ul> <p>Views that use attached databases set <code>source</code> to <code>duckdb</code>, <code>sqlite</code>, or <code>postgres</code> and provide <code>database</code> (attachment alias) and <code>table</code>.</p>"},{"location":"guides/#iceberg-catalogs","title":"Iceberg Catalogs","text":"<p>Iceberg catalogs are configured under <code>iceberg_catalogs</code>. Iceberg views can either:</p> <ul> <li>Use a <code>uri</code> directly, or</li> <li>Refer to a <code>catalog</code> + <code>table</code> combination.</li> </ul> <p>Duckalog validates that any <code>catalog</code> used by a view is defined in <code>iceberg_catalogs</code>.</p>"},{"location":"guides/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"guides/#parquet-only-configuration","title":"Parquet-only Configuration","text":"<p>For simple cases where you only need to create views over Parquet files:</p> <pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='1GB'\"\n    - \"SET s3_region='us-west-2'\"\n\nviews:\n  - name: sales_data\n    source: parquet\n    uri: \"s3://data-bucket/sales/*.parquet\"\n\n  - name: customer_data\n    source: parquet\n    uri: \"s3://data-bucket/customers/*.parquet\"\n\n  - name: sales_by_customer\n    sql: |\n      SELECT \n        c.customer_id,\n        c.name,\n        c.region,\n        SUM(s.amount) as total_sales,\n        COUNT(s.order_id) as order_count\n      FROM customer_data c\n      JOIN sales_data s ON c.customer_id = s.customer_id\n      GROUP BY c.customer_id, c.name, c.region\n</code></pre>"},{"location":"guides/#attachments-only-configuration","title":"Attachments-only Configuration","text":"<p>For joining data from existing databases:</p> <pre><code>version: 1\n\nduckdb:\n  database: unified_catalog.duckdb\n\nattachments:\n  duckdb:\n    - alias: analytics\n      path: ./analytics_db.duckdb\n      read_only: true\n    - alias: warehouse\n      path: ./warehouse_db.duckdb\n      read_only: true\n\n  sqlite:\n    - alias: legacy\n      path: ./legacy_system.db\n\nviews:\n  - name: user_profiles\n    source: duckdb\n    database: analytics\n    table: users\n\n  - name: user_orders\n    source: duckdb\n    database: warehouse\n    table: orders\n\n  - name: legacy_customers\n    source: sqlite\n    database: legacy\n    table: customers\n\n  - name: complete_customer_view\n    sql: |\n      SELECT \n        p.user_id,\n        p.name,\n        p.email,\n        o.total_orders,\n        o.total_spent,\n        l.rating as legacy_rating\n      FROM user_profiles p\n      JOIN user_orders o ON p.user_id = o.user_id\n      LEFT JOIN legacy_customers l ON p.user_id = l.id\n</code></pre>"},{"location":"guides/#iceberg-only-configuration","title":"Iceberg-only Configuration","text":"<p>For working exclusively with Iceberg tables:</p> <pre><code>version: 1\n\nduckdb:\n  database: iceberg_catalog.duckdb\n\niceberg_catalogs:\n  - name: prod_catalog\n    catalog_type: rest\n    uri: \"https://iceberg-catalog.company.com\"\n    warehouse: \"s3://data-warehouse/production/\"\n    options:\n      token: \"${env:ICEBERG_PROD_TOKEN}\"\n\n  - name: staging_catalog\n    catalog_type: rest\n    uri: \"https://iceberg-staging.company.com\"\n    warehouse: \"s3://data-warehouse/staging/\"\n    options:\n      token: \"${env:ICEBERG_STAGING_TOKEN}\"\n\nviews:\n  - name: production_customers\n    source: iceberg\n    catalog: prod_catalog\n    table: analytics.customers\n\n  - name: staging_customers\n    source: iceberg\n    catalog: staging_catalog\n    table: analytics.customers\n\n  - name: customer_comparison\n    sql: |\n      SELECT \n        COALESCE(p.id, s.id) as customer_id,\n        p.name as prod_name,\n        s.name as staging_name,\n        p.updated_at as prod_updated,\n        s.updated_at as staging_updated\n      FROM production_customers p\n      FULL OUTER JOIN staging_customers s ON p.id = s.id\n</code></pre>"},{"location":"guides/#multi-source-configuration","title":"Multi-source Configuration","text":"<p>This example combines multiple data sources in a single catalog:</p> <pre><code>version: 1\n\nduckdb:\n  database: unified_analytics.duckdb\n  pragmas:\n    - \"SET memory_limit='4GB'\"\n    - \"SET threads=4\"\n    - \"SET s3_region='us-east-1'\"\n\nattachments:\n  duckdb:\n    - alias: reference\n      path: ./reference_data.duckdb\n      read_only: true\n\niceberg_catalogs:\n  - name: data_lake\n    catalog_type: rest\n    uri: \"https://iceberg.data-lake.internal\"\n    warehouse: \"s3://enterprise-data-lake/\"\n    options:\n      token: \"${env:ICEBERG_TOKEN}\"\n\nviews:\n  # Reference data from attached database\n  - name: user_segments\n    source: duckdb\n    database: reference\n    table: user_segments\n\n  # Raw events from S3\n  - name: raw_events\n    source: parquet\n    uri: \"s3://events-bucket/raw/*.parquet\"\n\n  # Processed events from Iceberg\n  - name: processed_events\n    source: iceberg\n    catalog: data_lake\n    table: analytics.processed_events\n\n  # Unified analytics view\n  - name: analytics_events\n    sql: |\n      SELECT \n        e.event_id,\n        e.timestamp,\n        e.user_id,\n        e.event_type,\n        e.properties,\n        us.segment_name as user_segment,\n        us.tier as user_tier\n      FROM raw_events e\n      LEFT JOIN user_segments us ON e.user_id = us.user_id\n      WHERE e.timestamp &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n</code></pre>"},{"location":"guides/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/#common-errors","title":"Common Errors","text":""},{"location":"guides/#1-invalid-configuration-syntax","title":"1. Invalid Configuration Syntax","text":"<p>Error message: <code>ConfigError: Invalid configuration file</code></p> <p>Solutions: - Validate YAML syntax first with <code>yamllint</code> or an online YAML validator - Ensure proper indentation (YAML is sensitive to spaces) - Check for unescaped special characters in strings</p>"},{"location":"guides/#2-missing-environment-variables","title":"2. Missing Environment Variables","text":"<p>Error message: <code>ConfigError: Environment variable 'AWS_ACCESS_KEY_ID' not found</code></p> <p>Solutions: - Check that all <code>env:VAR_NAME</code> variables are set - Use <code>duckalog validate config.yaml</code> to check environment variables without building - Consider using a <code>.env</code> file with <code>export</code> commands or a tool like <code>direnv</code></p>"},{"location":"guides/#3-connection-failures","title":"3. Connection Failures","text":"<p>Error message: <code>DuckDB Error: Invalid Input Error: Failed to open file</code></p> <p>Solutions: - Check file paths in attachments section - Ensure credentials for cloud storage are correct - Verify network connectivity and firewall rules - For local files, check file permissions</p>"},{"location":"guides/#4-sql-errors-in-views","title":"4. SQL Errors in Views","text":"<p>Error message: <code>Parser Error: syntax error at or near \"JOIN\"</code></p> <p>Solutions: - Validate SQL syntax with <code>duckalog generate-sql config.yaml</code> before building - Check that all referenced views and tables exist - Verify column names match across join conditions - Use proper DuckDB SQL syntax (similar to PostgreSQL)</p>"},{"location":"guides/#5-iceberg-catalog-issues","title":"5. Iceberg Catalog Issues","text":"<p>Error message: <code>Invalid Input Error: Can't load extension iceberg</code></p> <p>Solutions: - Ensure DuckDB version supports Iceberg extensions - Install required extensions: <code>duckdb.install_extension(\"iceberg\")</code> - Check catalog configuration and credentials - Verify Iceberg catalog server is accessible</p>"},{"location":"guides/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Validate before building: Always run <code>duckalog validate config.yaml</code> first</li> <li>Generate SQL first: Use <code>duckalog generate-sql config.yaml</code> to inspect generated SQL</li> <li>Start simple: Begin with a single view and gradually add complexity</li> <li>Use environment check: Create a simple script to verify required environment variables</li> <li>Check logs: Run with verbose logging to see detailed error information</li> </ol> <pre><code># Enable debug logging\nduckalog build config.yaml --log-level DEBUG\n</code></pre>"},{"location":"guides/#best-practices","title":"Best Practices","text":"<ol> <li>Separate configurations: Use different configs for different environments</li> <li>Version control: Keep your configs in Git alongside your code</li> <li>Modular views: Break complex SQL into multiple simpler views when possible</li> <li>Naming conventions: Use consistent naming for views and attachments</li> <li>Security: Never commit secrets to version control - use environment variables</li> </ol>"},{"location":"guides/#next-steps","title":"Next Steps","text":"<ul> <li>Use <code>duckalog generate-sql</code> to inspect the SQL that will be executed.</li> <li>Use the API reference for details on each public function and model.</li> <li>Check out the examples in the documentation for more advanced use cases.</li> </ul>"},{"location":"guides/usage/","title":"User Guide","text":"<p>This guide explains how to structure Duckalog configuration files, common configuration patterns, and how to troubleshoot issues.</p>"},{"location":"guides/usage/#configuration-structure","title":"Configuration structure","text":"<p>At a high level, a config looks like this:</p> <pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='1GB'\"\n\nattachments:\n  duckdb:\n    - alias: refdata\n      path: ./refdata.duckdb\n      read_only: true\n\n  sqlite:\n    - alias: legacy\n      path: ./legacy.db\n\n  postgres:\n    - alias: dw\n      host: \"${env:PG_HOST}\"\n      port: 5432\n      database: dw\n      user: \"${env:PG_USER}\"\n      password: \"${env:PG_PASSWORD}\"\n\niceberg_catalogs:\n  - name: main_ic\n    catalog_type: rest\n    uri: \"https://iceberg-catalog.internal\"\n    warehouse: \"s3://my-warehouse/\"\n    options:\n      token: \"${env:ICEBERG_TOKEN}\"\n\nviews:\n  - name: users\n    source: parquet\n    uri: \"s3://my-bucket/data/users/*.parquet\"\n\n  - name: events_delta\n    source: delta\n    uri: \"s3://my-bucket/delta/events\"\n\n  - name: ic_orders\n    source: iceberg\n    catalog: main_ic\n    table: analytics.orders\n\n  - name: ref_countries\n    source: duckdb\n    database: refdata\n    table: reference.countries\n\n  - name: vip_users\n    sql: |\n      SELECT *\n      FROM users\n      WHERE is_vip = TRUE\n</code></pre>"},{"location":"guides/usage/#environment-variables","title":"Environment variables","text":"<p>Any string may contain <code>${env:VAR_NAME}</code> placeholders. Duckalog resolves these using the process environment before validation. If a variable is missing, a <code>ConfigError</code> is raised.</p> <p>Example:</p> <pre><code>duckdb:\n  pragmas:\n    - \"SET s3_access_key_id='${env:AWS_ACCESS_KEY_ID}'\"\n    - \"SET s3_secret_access_key='${env:AWS_SECRET_ACCESS_KEY}'\"\n</code></pre>"},{"location":"guides/usage/#attachments","title":"Attachments","text":"<p>Attachments let you expose tables from other databases inside DuckDB.</p> <ul> <li>DuckDB attachments: attach additional <code>.duckdb</code> files.</li> <li>SQLite attachments: attach local SQLite databases.</li> <li>Postgres attachments: connect to external Postgres instances.</li> </ul> <p>Views that use attached databases set <code>source</code> to <code>duckdb</code>, <code>sqlite</code>, or <code>postgres</code> and provide <code>database</code> (attachment alias) and <code>table</code>.</p>"},{"location":"guides/usage/#iceberg-catalogs","title":"Iceberg catalogs","text":"<p>Iceberg catalogs are configured under <code>iceberg_catalogs</code>. Iceberg views can either:</p> <ul> <li>Use a <code>uri</code> directly, or</li> <li>Refer to a <code>catalog</code> + <code>table</code> combination.</li> </ul> <p>Duckalog validates that any <code>catalog</code> used by a view is defined in <code>iceberg_catalogs</code>.</p>"},{"location":"guides/usage/#common-configuration-patterns","title":"Common Configuration Patterns","text":""},{"location":"guides/usage/#parquet-only-configuration","title":"Parquet-only configuration","text":"<p>For simple cases where you only need to create views over Parquet files:</p> <pre><code>version: 1\n\nduckdb:\n  database: catalog.duckdb\n  pragmas:\n    - \"SET memory_limit='1GB'\"\n    - \"SET s3_region='us-west-2'\"\n\nviews:\n  - name: sales_data\n    source: parquet\n    uri: \"s3://data-bucket/sales/*.parquet\"\n\n  - name: customer_data\n    source: parquet\n    uri: \"s3://data-bucket/customers/*.parquet\"\n\n  - name: sales_by_customer\n    sql: |\n      SELECT \n        c.customer_id,\n        c.name,\n        c.region,\n        SUM(s.amount) as total_sales,\n        COUNT(s.order_id) as order_count\n      FROM customer_data c\n      JOIN sales_data s ON c.customer_id = s.customer_id\n      GROUP BY c.customer_id, c.name, c.region\n</code></pre>"},{"location":"guides/usage/#attachments-only-configuration","title":"Attachments-only configuration","text":"<p>For joining data from existing databases:</p> <pre><code>version: 1\n\nduckdb:\n  database: unified_catalog.duckdb\n\nattachments:\n  duckdb:\n    - alias: analytics\n      path: ./analytics_db.duckdb\n      read_only: true\n    - alias: warehouse\n      path: ./warehouse_db.duckdb\n      read_only: true\n\n  sqlite:\n    - alias: legacy\n      path: ./legacy_system.db\n\nviews:\n  - name: user_profiles\n    source: duckdb\n    database: analytics\n    table: users\n\n  - name: user_orders\n    source: duckdb\n    database: warehouse\n    table: orders\n\n  - name: legacy_customers\n    source: sqlite\n    database: legacy\n    table: customers\n\n  - name: complete_customer_view\n    sql: |\n      SELECT \n        p.user_id,\n        p.name,\n        p.email,\n        o.total_orders,\n        o.total_spent,\n        l.rating as legacy_rating\n      FROM user_profiles p\n      JOIN user_orders o ON p.user_id = o.user_id\n      LEFT JOIN legacy_customers l ON p.user_id = l.id\n</code></pre>"},{"location":"guides/usage/#iceberg-only-configuration","title":"Iceberg-only configuration","text":"<p>For working exclusively with Iceberg tables:</p> <pre><code>version: 1\n\nduckdb:\n  database: iceberg_catalog.duckdb\n\niceberg_catalogs:\n  - name: prod_catalog\n    catalog_type: rest\n    uri: \"https://iceberg-catalog.company.com\"\n    warehouse: \"s3://data-warehouse/production/\"\n    options:\n      token: \"${env:ICEBERG_PROD_TOKEN}\"\n\n  - name: staging_catalog\n    catalog_type: rest\n    uri: \"https://iceberg-staging.company.com\"\n    warehouse: \"s3://data-warehouse/staging/\"\n    options:\n      token: \"${env:ICEBERG_STAGING_TOKEN}\"\n\nviews:\n  - name: production_customers\n    source: iceberg\n    catalog: prod_catalog\n    table: analytics.customers\n\n  - name: staging_customers\n    source: iceberg\n    catalog: staging_catalog\n    table: analytics.customers\n\n  - name: customer_comparison\n    sql: |\n      SELECT \n        COALESCE(p.id, s.id) as customer_id,\n        p.name as prod_name,\n        s.name as staging_name,\n        p.updated_at as prod_updated,\n        s.updated_at as staging_updated\n      FROM production_customers p\n      FULL OUTER JOIN staging_customers s ON p.id = s.id\n</code></pre>"},{"location":"guides/usage/#multi-source-configuration","title":"Multi-source configuration","text":"<p>This example combines multiple data sources in a single catalog:</p> <pre><code>version: 1\n\nduckdb:\n  database: unified_analytics.duckdb\n  pragmas:\n    - \"SET memory_limit='4GB'\"\n    - \"SET threads=4\"\n    - \"SET s3_region='us-east-1'\"\n\nattachments:\n  duckdb:\n    - alias: reference\n      path: ./reference_data.duckdb\n      read_only: true\n\niceberg_catalogs:\n  - name: data_lake\n    catalog_type: rest\n    uri: \"https://iceberg.data-lake.internal\"\n    warehouse: \"s3://enterprise-data-lake/\"\n    options:\n      token: \"${env:ICEBERG_TOKEN}\"\n\nviews:\n  # Reference data from attached database\n  - name: user_segments\n    source: duckdb\n    database: reference\n    table: user_segments\n\n  # Raw events from S3\n  - name: raw_events\n    source: parquet\n    uri: \"s3://events-bucket/raw/*.parquet\"\n\n  # Processed events from Iceberg\n  - name: processed_events\n    source: iceberg\n    catalog: data_lake\n    table: analytics.processed_events\n\n  # Unified analytics view\n  - name: analytics_events\n    sql: |\n      SELECT \n        e.event_id,\n        e.timestamp,\n        e.user_id,\n        e.event_type,\n        e.properties,\n        us.segment_name as user_segment,\n        us.tier as user_tier\n      FROM raw_events e\n      LEFT JOIN user_segments us ON e.user_id = us.user_id\n      WHERE e.timestamp &gt;= CURRENT_DATE - INTERVAL 30 DAYS\n</code></pre>"},{"location":"guides/usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/usage/#common-errors","title":"Common errors","text":""},{"location":"guides/usage/#1-invalid-configuration-syntax","title":"1. Invalid configuration syntax","text":"<p>Error message: <code>ConfigError: Invalid configuration file</code></p> <p>Solutions: - Validate YAML syntax first with <code>yamllint</code> or an online YAML validator - Ensure proper indentation (YAML is sensitive to spaces) - Check for unescaped special characters in strings</p>"},{"location":"guides/usage/#2-missing-environment-variables","title":"2. Missing environment variables","text":"<p>Error message: <code>ConfigError: Environment variable 'AWS_ACCESS_KEY_ID' not found</code></p> <p>Solutions: - Check that all <code>env:VAR_NAME</code> variables are set - Use <code>duckalog validate config.yaml</code> to check environment variables without building - Consider using a <code>.env</code> file with <code>export</code> commands or a tool like <code>direnv</code></p>"},{"location":"guides/usage/#3-connection-failures","title":"3. Connection failures","text":"<p>Error message: <code>DuckDB Error: Invalid Input Error: Failed to open file</code></p> <p>Solutions: - Check file paths in attachments section - Ensure credentials for cloud storage are correct - Verify network connectivity and firewall rules - For local files, check file permissions</p>"},{"location":"guides/usage/#4-sql-errors-in-views","title":"4. SQL errors in views","text":"<p>Error message: <code>Parser Error: syntax error at or near \"JOIN\"</code></p> <p>Solutions: - Validate SQL syntax with <code>duckalog generate-sql config.yaml</code> before building - Check that all referenced views and tables exist - Verify column names match across join conditions - Use proper DuckDB SQL syntax (similar to PostgreSQL)</p>"},{"location":"guides/usage/#5-iceberg-catalog-issues","title":"5. Iceberg catalog issues","text":"<p>Error message: <code>Invalid Input Error: Can't load extension iceberg</code></p> <p>Solutions: - Ensure DuckDB version supports Iceberg extensions - Install required extensions: <code>duckdb.install_extension(\"iceberg\")</code> - Check catalog configuration and credentials - Verify Iceberg catalog server is accessible</p>"},{"location":"guides/usage/#debugging-tips","title":"Debugging tips","text":"<ol> <li>Validate before building: Always run <code>duckalog validate config.yaml</code> first</li> <li>Generate SQL first: Use <code>duckalog generate-sql config.yaml</code> to inspect generated SQL</li> <li>Start simple: Begin with a single view and gradually add complexity</li> <li>Use environment check: Create a simple script to verify required environment variables</li> <li>Check logs: Run with verbose logging to see detailed error information</li> </ol> <pre><code># Enable debug logging\nduckalog build config.yaml --log-level DEBUG\n</code></pre>"},{"location":"guides/usage/#best-practices","title":"Best Practices","text":"<ol> <li>Separate configurations: Use different configs for different environments</li> <li>Version control: Keep your configs in Git alongside your code</li> <li>Modular views: Break complex SQL into multiple simpler views when possible</li> <li>Naming conventions: Use consistent naming for views and attachments</li> <li>Security: Never commit secrets to version control - use environment variables</li> </ol>"},{"location":"guides/usage/#next-steps","title":"Next steps","text":"<ul> <li>Use <code>duckalog generate-sql</code> to inspect the SQL that will be executed.</li> <li>Use the API reference for details on each public function and model.</li> <li>Check out the examples in the documentation for more advanced use cases.</li> </ul>"},{"location":"reference/","title":"API Reference","text":"<p>This section provides comprehensive API documentation for the Duckalog library, generated from the source code using mkdocstrings.</p>"},{"location":"reference/#core-api","title":"Core API","text":""},{"location":"reference/#duckalog","title":"<code>duckalog</code>","text":"<p>Duckalog public API.</p>"},{"location":"reference/#duckalog.AttachmentsConfig","title":"<code>AttachmentsConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Collection of attachment configurations.</p> <p>Attributes:</p> Name Type Description <code>duckdb</code> <code>List[DuckDBAttachment]</code> <p>DuckDB attachment entries.</p> <code>sqlite</code> <code>List[SQLiteAttachment]</code> <p>SQLite attachment entries.</p> <code>postgres</code> <code>List[PostgresAttachment]</code> <p>Postgres attachment entries.</p>"},{"location":"reference/#duckalog.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level Duckalog configuration.</p> <p>Attributes:</p> Name Type Description <code>version</code> <code>int</code> <p>Positive integer describing the config schema version.</p> <code>duckdb</code> <code>DuckDBConfig</code> <p>DuckDB session and connection settings.</p> <code>views</code> <code>List[ViewConfig]</code> <p>List of view definitions to create in the catalog.</p> <code>attachments</code> <code>AttachmentsConfig</code> <p>Optional attachments to external databases.</p> <code>iceberg_catalogs</code> <code>List[IcebergCatalogConfig]</code> <p>Optional Iceberg catalog definitions.</p> <code>semantic_models</code> <code>List[SemanticModelConfig]</code> <p>Optional semantic model definitions for business metadata.</p>"},{"location":"reference/#duckalog.ConfigError","title":"<code>ConfigError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Configuration-related error.</p> <p>This exception is raised when a catalog configuration cannot be read, parsed, interpolated, or validated according to the Duckalog schema.</p> <p>Typical error conditions include:</p> <ul> <li>The config file does not exist or cannot be read.</li> <li>The file is not valid YAML/JSON.</li> <li>Required fields are missing or invalid.</li> <li>An environment variable placeholder cannot be resolved.</li> </ul>"},{"location":"reference/#duckalog.DuckDBAttachment","title":"<code>DuckDBAttachment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for attaching another DuckDB database.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>Alias under which the database will be attached.</p> <code>path</code> <code>str</code> <p>Filesystem path to the DuckDB database file.</p> <code>read_only</code> <code>bool</code> <p>Whether the attachment should be opened in read-only mode. Defaults to <code>True</code> for safety.</p>"},{"location":"reference/#duckalog.DuckDBConfig","title":"<code>DuckDBConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DuckDB connection and session settings.</p> <p>Attributes:</p> Name Type Description <code>database</code> <code>str</code> <p>Path to the DuckDB database file. Defaults to <code>\":memory:\"</code>.</p> <code>install_extensions</code> <code>List[str]</code> <p>Names of extensions to install before use.</p> <code>load_extensions</code> <code>List[str]</code> <p>Names of extensions to load in the session.</p> <code>pragmas</code> <code>List[str]</code> <p>SQL statements (typically <code>SET</code> pragmas) executed after connecting and loading extensions.</p> <code>settings</code> <code>Union[str, List[str], None]</code> <p>DuckDB SET statements executed after pragmas. Can be a single string or list of strings.</p> <code>secrets</code> <code>List[SecretConfig]</code> <p>List of secret definitions for external services and databases.</p>"},{"location":"reference/#duckalog.EngineError","title":"<code>EngineError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Engine-level error raised during catalog builds.</p> <p>This exception wraps lower-level DuckDB errors, such as failures to connect to the database, attach external systems, or execute generated SQL statements.</p>"},{"location":"reference/#duckalog.IcebergCatalogConfig","title":"<code>IcebergCatalogConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for an Iceberg catalog.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Catalog name referenced by Iceberg views.</p> <code>catalog_type</code> <code>str</code> <p>Backend type (for example, <code>rest</code>, <code>hive</code>, <code>glue</code>).</p> <code>uri</code> <code>Optional[str]</code> <p>Optional URI used by certain catalog types.</p> <code>warehouse</code> <code>Optional[str]</code> <p>Optional warehouse location for catalog data.</p> <code>options</code> <code>Dict[str, Any]</code> <p>Additional catalog-specific options.</p>"},{"location":"reference/#duckalog.PostgresAttachment","title":"<code>PostgresAttachment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for attaching a Postgres database.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>Alias used inside DuckDB to reference the Postgres database.</p> <code>host</code> <code>str</code> <p>Hostname or IP address of the Postgres server.</p> <code>port</code> <code>int</code> <p>TCP port of the Postgres server.</p> <code>database</code> <code>str</code> <p>Database name to connect to.</p> <code>user</code> <code>str</code> <p>Username for authentication.</p> <code>password</code> <code>str</code> <p>Password for authentication.</p> <code>sslmode</code> <code>Optional[str]</code> <p>Optional SSL mode (for example, <code>require</code>).</p> <code>options</code> <code>Dict[str, Any]</code> <p>Extra key/value options passed to the attachment clause.</p>"},{"location":"reference/#duckalog.SQLiteAttachment","title":"<code>SQLiteAttachment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for attaching a SQLite database.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>Alias under which the SQLite database will be attached.</p> <code>path</code> <code>str</code> <p>Filesystem path to the SQLite <code>.db</code> file.</p>"},{"location":"reference/#duckalog.ViewConfig","title":"<code>ViewConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Definition of a single catalog view.</p> <p>A view can be defined in several ways: 1. Inline SQL: Using the <code>sql</code> field with raw SQL text 2. SQL File: Using <code>sql_file</code> to reference external SQL files 3. SQL Template: Using <code>sql_template</code> for parameterized SQL files 4. Data Source: Using <code>source</code> + required fields for direct data access 5. Source + SQL: Using <code>source</code> for data access plus <code>sql</code> for transformations</p> <p>For data sources, the required fields depend on the source type: - Parquet/Delta: <code>uri</code> field is required - Iceberg: Either <code>uri</code> OR both <code>catalog</code> and <code>table</code> - DuckDB/SQLite/Postgres: Both <code>database</code> and <code>table</code> are required</p> <p>When using SQL with a data source, the SQL will be applied as a transformation over the data from the specified source.</p> <p>Additional metadata fields such as <code>description</code> and <code>tags</code> do not affect SQL generation but are preserved for documentation and tooling.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique view name within the config.</p> <code>sql</code> <code>Optional[str]</code> <p>Raw SQL text defining the view body.</p> <code>sql_file</code> <code>Optional[SQLFileReference]</code> <p>Direct reference to a SQL file.</p> <code>sql_template</code> <code>Optional[SQLFileReference]</code> <p>Reference to a SQL template file with variable substitution.</p> <code>source</code> <code>Optional[EnvSource]</code> <p>Source type (e.g. <code>\"parquet\"</code>, <code>\"iceberg\"</code>, <code>\"duckdb\"</code>).</p> <code>uri</code> <code>Optional[str]</code> <p>URI for file- or table-based sources (Parquet/Delta/Iceberg).</p> <code>database</code> <code>Optional[str]</code> <p>Attachment alias for attached-database sources.</p> <code>table</code> <code>Optional[str]</code> <p>Table name (optionally schema-qualified) for attached sources.</p> <code>catalog</code> <code>Optional[str]</code> <p>Iceberg catalog name for catalog-based Iceberg views.</p> <code>options</code> <code>Dict[str, Any]</code> <p>Source-specific options passed to scan functions.</p> <code>description</code> <code>Optional[str]</code> <p>Optional human-readable description of the view.</p> <code>tags</code> <code>List[str]</code> <p>Optional list of tags for classification.</p>"},{"location":"reference/#duckalog.build_catalog","title":"<code>build_catalog(config_path, db_path=None, dry_run=False, verbose=False)</code>","text":"<p>Build or update a DuckDB catalog from a configuration file.</p> <p>This function is the high-level entry point used by both the CLI and Python API. It loads the config, optionally performs a dry-run SQL generation, or otherwise connects to DuckDB, sets up attachments and Iceberg catalogs, and creates or replaces configured views.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <code>db_path</code> <code>Optional[str]</code> <p>Optional override for <code>duckdb.database</code> in the config.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If <code>True</code>, do not connect to DuckDB; instead generate and return the full SQL script for all views.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If <code>True</code>, enable more verbose logging via the standard logging module.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The generated SQL script as a string when <code>dry_run</code> is <code>True</code>,</p> <code>Optional[str]</code> <p>otherwise <code>None</code> when the catalog is applied to DuckDB.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> <code>EngineError</code> <p>If connecting to DuckDB or executing SQL fails.</p> Example <p>Build a catalog in-place::</p> <pre><code>from duckalog import build_catalog\n\nbuild_catalog(\"catalog.yaml\")\n</code></pre> <p>Generate SQL without modifying the database::</p> <pre><code>sql = build_catalog(\"catalog.yaml\", dry_run=True)\nprint(sql)\n</code></pre>"},{"location":"reference/#duckalog.generate_all_views_sql","title":"<code>generate_all_views_sql(config)</code>","text":"<p>Generate SQL for all views in a configuration.</p> <p>The output includes a descriptive header with the config version followed by a <code>CREATE OR REPLACE VIEW</code> statement for each view in the order they appear in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The validated :class:<code>Config</code> instance to render.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A multi-statement SQL script suitable for use as a catalog definition.</p>"},{"location":"reference/#duckalog.generate_sql","title":"<code>generate_sql(config_path)</code>","text":"<p>Generate a full SQL script from a config file.</p> <p>This is a convenience wrapper around :func:<code>load_config</code> and :func:<code>generate_all_views_sql</code> that does not connect to DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A multi-statement SQL script containing <code>CREATE OR REPLACE VIEW</code></p> <code>str</code> <p>statements for all configured views.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> Example <p>from duckalog import generate_sql sql = generate_sql(\"catalog.yaml\") print(\"CREATE VIEW\" in sql) True</p>"},{"location":"reference/#duckalog.generate_view_sql","title":"<code>generate_view_sql(view)</code>","text":"<p>Generate a <code>CREATE OR REPLACE VIEW</code> statement for a single view.</p> <p>Parameters:</p> Name Type Description Default <code>view</code> <code>ViewConfig</code> <p>The :class:<code>ViewConfig</code> to generate SQL for.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A single SQL statement that creates or replaces the view.</p>"},{"location":"reference/#duckalog.load_config","title":"<code>load_config(path, load_sql_files=True, sql_file_loader=None)</code>","text":"<p>Load, interpolate, and validate a Duckalog configuration file.</p> <p>This helper is the main entry point for turning a YAML or JSON file into a validated :class:<code>Config</code> instance. It applies environment-variable interpolation and enforces the configuration schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a YAML or JSON config file.</p> required <code>load_sql_files</code> <code>bool</code> <p>Whether to load and process SQL from external files.           If False, SQL file references are left as-is for later processing.</p> <code>True</code> <code>sql_file_loader</code> <code>Optional['SQLFileLoader']</code> <p>Optional SQLFileLoader instance for loading SQL files.             If None, a default loader will be created.</p> <code>None</code> <p>Returns:</p> Type Description <code>Config</code> <p>A validated :class:<code>Config</code> object.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the file cannot be read, is not valid YAML/JSON, fails schema validation, contains unresolved <code>${env:VAR_NAME}</code> placeholders, or if SQL file loading fails.</p> Example <p>Load a catalog from <code>catalog.yaml</code>::</p> <pre><code>from duckalog import load_config\n\nconfig = load_config(\"catalog.yaml\")\nprint(len(config.views))\n</code></pre>"},{"location":"reference/#duckalog.quote_ident","title":"<code>quote_ident(value)</code>","text":"<p>Quote a SQL identifier using double quotes.</p> <p>This helper wraps a string in double quotes and escapes any embedded double quotes according to SQL rules.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Identifier to quote (for example, a view or column name).</p> required <p>Returns:</p> Type Description <code>str</code> <p>The identifier wrapped in double quotes.</p> Example <p>quote_ident(\"events\") '\"events\"'</p>"},{"location":"reference/#duckalog.render_options","title":"<code>render_options(options)</code>","text":"<p>Render a mapping of options into scan-function arguments.</p> <p>The resulting string is suitable for appending to a <code>*_scan</code> function call. Keys are sorted alphabetically to keep output deterministic.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>Dict[str, Any]</code> <p>Mapping of option name to value (str, bool, int, or float).</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string that starts with <code>,</code> when options are present (for example,</p> <code>str</code> <p><code>\", hive_partitioning=TRUE\"</code>) or an empty string when no options</p> <code>str</code> <p>are provided.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If a value has a type that cannot be rendered safely.</p>"},{"location":"reference/#duckalog.validate_config","title":"<code>validate_config(config_path)</code>","text":"<p>Validate a configuration file without touching DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is missing, malformed, or does not satisfy the schema and interpolation rules.</p> Example <p>from duckalog import validate_config validate_config(\"catalog.yaml\")  # raises on invalid config</p>"},{"location":"reference/#configuration-models","title":"Configuration Models","text":""},{"location":"reference/#config","title":"Config","text":"<p>The main configuration class that represents a Duckalog configuration file.</p>"},{"location":"reference/#view","title":"View","text":"<p>Represents a view definition in the catalog.</p>"},{"location":"reference/#duckdbconfig","title":"DuckDBConfig","text":"<p>Configuration for the DuckDB database instance.</p>"},{"location":"reference/#attachment","title":"Attachment","text":"<p>Configuration for database attachments.</p>"},{"location":"reference/#icebergcatalog","title":"IcebergCatalog","text":"<p>Configuration for Iceberg catalog connections.</p>"},{"location":"reference/#utility-functions","title":"Utility Functions","text":""},{"location":"reference/#load_config","title":"load_config","text":"<p>Load a Duckalog configuration from a file path.</p>"},{"location":"reference/#build_catalog","title":"build_catalog","text":"<p>Build a DuckDB catalog from a configuration file.</p>"},{"location":"reference/#validate_config","title":"validate_config","text":"<p>Validate a Duckalog configuration without building.</p>"},{"location":"reference/#generate_sql","title":"generate_sql","text":"<p>Generate SQL statements from a configuration file.</p>"},{"location":"reference/#command-line-interface","title":"Command Line Interface","text":""},{"location":"reference/#duckalog-build","title":"duckalog build","text":"<p>Build a DuckDB catalog from a configuration file.</p>"},{"location":"reference/#duckalog-validate","title":"duckalog validate","text":"<p>Validate a configuration file.</p>"},{"location":"reference/#duckalog-generate-sql","title":"duckalog generate-sql","text":"<p>Generate SQL statements from a configuration file.</p>"},{"location":"reference/#error-handling","title":"Error Handling","text":""},{"location":"reference/#configerror","title":"ConfigError","text":"<p>Raised when there's an error in the configuration file.</p>"},{"location":"reference/#catalogerror","title":"CatalogError","text":"<p>Raised when there's an error building the catalog.</p>"},{"location":"reference/#advanced-usage","title":"Advanced Usage","text":"<p>For detailed examples and advanced usage patterns, see the User Guide and Examples.</p>"},{"location":"reference/api/","title":"API Reference","text":"<p>This section is generated from the Duckalog source code using mkdocstrings.</p>"},{"location":"reference/api/#duckalog","title":"<code>duckalog</code>","text":"<p>Duckalog public API.</p>"},{"location":"reference/api/#duckalog.AttachmentsConfig","title":"<code>AttachmentsConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Collection of attachment configurations.</p> <p>Attributes:</p> Name Type Description <code>duckdb</code> <code>List[DuckDBAttachment]</code> <p>DuckDB attachment entries.</p> <code>sqlite</code> <code>List[SQLiteAttachment]</code> <p>SQLite attachment entries.</p> <code>postgres</code> <code>List[PostgresAttachment]</code> <p>Postgres attachment entries.</p>"},{"location":"reference/api/#duckalog.Config","title":"<code>Config</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Top-level Duckalog configuration.</p> <p>Attributes:</p> Name Type Description <code>version</code> <code>int</code> <p>Positive integer describing the config schema version.</p> <code>duckdb</code> <code>DuckDBConfig</code> <p>DuckDB session and connection settings.</p> <code>views</code> <code>List[ViewConfig]</code> <p>List of view definitions to create in the catalog.</p> <code>attachments</code> <code>AttachmentsConfig</code> <p>Optional attachments to external databases.</p> <code>iceberg_catalogs</code> <code>List[IcebergCatalogConfig]</code> <p>Optional Iceberg catalog definitions.</p> <code>semantic_models</code> <code>List[SemanticModelConfig]</code> <p>Optional semantic model definitions for business metadata.</p>"},{"location":"reference/api/#duckalog.ConfigError","title":"<code>ConfigError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Configuration-related error.</p> <p>This exception is raised when a catalog configuration cannot be read, parsed, interpolated, or validated according to the Duckalog schema.</p> <p>Typical error conditions include:</p> <ul> <li>The config file does not exist or cannot be read.</li> <li>The file is not valid YAML/JSON.</li> <li>Required fields are missing or invalid.</li> <li>An environment variable placeholder cannot be resolved.</li> </ul>"},{"location":"reference/api/#duckalog.DuckDBAttachment","title":"<code>DuckDBAttachment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for attaching another DuckDB database.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>Alias under which the database will be attached.</p> <code>path</code> <code>str</code> <p>Filesystem path to the DuckDB database file.</p> <code>read_only</code> <code>bool</code> <p>Whether the attachment should be opened in read-only mode. Defaults to <code>True</code> for safety.</p>"},{"location":"reference/api/#duckalog.DuckDBConfig","title":"<code>DuckDBConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DuckDB connection and session settings.</p> <p>Attributes:</p> Name Type Description <code>database</code> <code>str</code> <p>Path to the DuckDB database file. Defaults to <code>\":memory:\"</code>.</p> <code>install_extensions</code> <code>List[str]</code> <p>Names of extensions to install before use.</p> <code>load_extensions</code> <code>List[str]</code> <p>Names of extensions to load in the session.</p> <code>pragmas</code> <code>List[str]</code> <p>SQL statements (typically <code>SET</code> pragmas) executed after connecting and loading extensions.</p> <code>settings</code> <code>Union[str, List[str], None]</code> <p>DuckDB SET statements executed after pragmas. Can be a single string or list of strings.</p> <code>secrets</code> <code>List[SecretConfig]</code> <p>List of secret definitions for external services and databases.</p>"},{"location":"reference/api/#duckalog.EngineError","title":"<code>EngineError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Engine-level error raised during catalog builds.</p> <p>This exception wraps lower-level DuckDB errors, such as failures to connect to the database, attach external systems, or execute generated SQL statements.</p>"},{"location":"reference/api/#duckalog.IcebergCatalogConfig","title":"<code>IcebergCatalogConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for an Iceberg catalog.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Catalog name referenced by Iceberg views.</p> <code>catalog_type</code> <code>str</code> <p>Backend type (for example, <code>rest</code>, <code>hive</code>, <code>glue</code>).</p> <code>uri</code> <code>Optional[str]</code> <p>Optional URI used by certain catalog types.</p> <code>warehouse</code> <code>Optional[str]</code> <p>Optional warehouse location for catalog data.</p> <code>options</code> <code>Dict[str, Any]</code> <p>Additional catalog-specific options.</p>"},{"location":"reference/api/#duckalog.PostgresAttachment","title":"<code>PostgresAttachment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for attaching a Postgres database.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>Alias used inside DuckDB to reference the Postgres database.</p> <code>host</code> <code>str</code> <p>Hostname or IP address of the Postgres server.</p> <code>port</code> <code>int</code> <p>TCP port of the Postgres server.</p> <code>database</code> <code>str</code> <p>Database name to connect to.</p> <code>user</code> <code>str</code> <p>Username for authentication.</p> <code>password</code> <code>str</code> <p>Password for authentication.</p> <code>sslmode</code> <code>Optional[str]</code> <p>Optional SSL mode (for example, <code>require</code>).</p> <code>options</code> <code>Dict[str, Any]</code> <p>Extra key/value options passed to the attachment clause.</p>"},{"location":"reference/api/#duckalog.SQLiteAttachment","title":"<code>SQLiteAttachment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for attaching a SQLite database.</p> <p>Attributes:</p> Name Type Description <code>alias</code> <code>str</code> <p>Alias under which the SQLite database will be attached.</p> <code>path</code> <code>str</code> <p>Filesystem path to the SQLite <code>.db</code> file.</p>"},{"location":"reference/api/#duckalog.ViewConfig","title":"<code>ViewConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Definition of a single catalog view.</p> <p>A view can be defined in several ways: 1. Inline SQL: Using the <code>sql</code> field with raw SQL text 2. SQL File: Using <code>sql_file</code> to reference external SQL files 3. SQL Template: Using <code>sql_template</code> for parameterized SQL files 4. Data Source: Using <code>source</code> + required fields for direct data access 5. Source + SQL: Using <code>source</code> for data access plus <code>sql</code> for transformations</p> <p>For data sources, the required fields depend on the source type: - Parquet/Delta: <code>uri</code> field is required - Iceberg: Either <code>uri</code> OR both <code>catalog</code> and <code>table</code> - DuckDB/SQLite/Postgres: Both <code>database</code> and <code>table</code> are required</p> <p>When using SQL with a data source, the SQL will be applied as a transformation over the data from the specified source.</p> <p>Additional metadata fields such as <code>description</code> and <code>tags</code> do not affect SQL generation but are preserved for documentation and tooling.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique view name within the config.</p> <code>sql</code> <code>Optional[str]</code> <p>Raw SQL text defining the view body.</p> <code>sql_file</code> <code>Optional[SQLFileReference]</code> <p>Direct reference to a SQL file.</p> <code>sql_template</code> <code>Optional[SQLFileReference]</code> <p>Reference to a SQL template file with variable substitution.</p> <code>source</code> <code>Optional[EnvSource]</code> <p>Source type (e.g. <code>\"parquet\"</code>, <code>\"iceberg\"</code>, <code>\"duckdb\"</code>).</p> <code>uri</code> <code>Optional[str]</code> <p>URI for file- or table-based sources (Parquet/Delta/Iceberg).</p> <code>database</code> <code>Optional[str]</code> <p>Attachment alias for attached-database sources.</p> <code>table</code> <code>Optional[str]</code> <p>Table name (optionally schema-qualified) for attached sources.</p> <code>catalog</code> <code>Optional[str]</code> <p>Iceberg catalog name for catalog-based Iceberg views.</p> <code>options</code> <code>Dict[str, Any]</code> <p>Source-specific options passed to scan functions.</p> <code>description</code> <code>Optional[str]</code> <p>Optional human-readable description of the view.</p> <code>tags</code> <code>List[str]</code> <p>Optional list of tags for classification.</p>"},{"location":"reference/api/#duckalog.build_catalog","title":"<code>build_catalog(config_path, db_path=None, dry_run=False, verbose=False)</code>","text":"<p>Build or update a DuckDB catalog from a configuration file.</p> <p>This function is the high-level entry point used by both the CLI and Python API. It loads the config, optionally performs a dry-run SQL generation, or otherwise connects to DuckDB, sets up attachments and Iceberg catalogs, and creates or replaces configured views.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <code>db_path</code> <code>Optional[str]</code> <p>Optional override for <code>duckdb.database</code> in the config.</p> <code>None</code> <code>dry_run</code> <code>bool</code> <p>If <code>True</code>, do not connect to DuckDB; instead generate and return the full SQL script for all views.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If <code>True</code>, enable more verbose logging via the standard logging module.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The generated SQL script as a string when <code>dry_run</code> is <code>True</code>,</p> <code>Optional[str]</code> <p>otherwise <code>None</code> when the catalog is applied to DuckDB.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> <code>EngineError</code> <p>If connecting to DuckDB or executing SQL fails.</p> Example <p>Build a catalog in-place::</p> <pre><code>from duckalog import build_catalog\n\nbuild_catalog(\"catalog.yaml\")\n</code></pre> <p>Generate SQL without modifying the database::</p> <pre><code>sql = build_catalog(\"catalog.yaml\", dry_run=True)\nprint(sql)\n</code></pre>"},{"location":"reference/api/#duckalog.generate_all_views_sql","title":"<code>generate_all_views_sql(config)</code>","text":"<p>Generate SQL for all views in a configuration.</p> <p>The output includes a descriptive header with the config version followed by a <code>CREATE OR REPLACE VIEW</code> statement for each view in the order they appear in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The validated :class:<code>Config</code> instance to render.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A multi-statement SQL script suitable for use as a catalog definition.</p>"},{"location":"reference/api/#duckalog.generate_sql","title":"<code>generate_sql(config_path)</code>","text":"<p>Generate a full SQL script from a config file.</p> <p>This is a convenience wrapper around :func:<code>load_config</code> and :func:<code>generate_all_views_sql</code> that does not connect to DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A multi-statement SQL script containing <code>CREATE OR REPLACE VIEW</code></p> <code>str</code> <p>statements for all configured views.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is invalid.</p> Example <p>from duckalog import generate_sql sql = generate_sql(\"catalog.yaml\") print(\"CREATE VIEW\" in sql) True</p>"},{"location":"reference/api/#duckalog.generate_view_sql","title":"<code>generate_view_sql(view)</code>","text":"<p>Generate a <code>CREATE OR REPLACE VIEW</code> statement for a single view.</p> <p>Parameters:</p> Name Type Description Default <code>view</code> <code>ViewConfig</code> <p>The :class:<code>ViewConfig</code> to generate SQL for.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A single SQL statement that creates or replaces the view.</p>"},{"location":"reference/api/#duckalog.load_config","title":"<code>load_config(path, load_sql_files=True, sql_file_loader=None)</code>","text":"<p>Load, interpolate, and validate a Duckalog configuration file.</p> <p>This helper is the main entry point for turning a YAML or JSON file into a validated :class:<code>Config</code> instance. It applies environment-variable interpolation and enforces the configuration schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a YAML or JSON config file.</p> required <code>load_sql_files</code> <code>bool</code> <p>Whether to load and process SQL from external files.           If False, SQL file references are left as-is for later processing.</p> <code>True</code> <code>sql_file_loader</code> <code>Optional['SQLFileLoader']</code> <p>Optional SQLFileLoader instance for loading SQL files.             If None, a default loader will be created.</p> <code>None</code> <p>Returns:</p> Type Description <code>Config</code> <p>A validated :class:<code>Config</code> object.</p> <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the file cannot be read, is not valid YAML/JSON, fails schema validation, contains unresolved <code>${env:VAR_NAME}</code> placeholders, or if SQL file loading fails.</p> Example <p>Load a catalog from <code>catalog.yaml</code>::</p> <pre><code>from duckalog import load_config\n\nconfig = load_config(\"catalog.yaml\")\nprint(len(config.views))\n</code></pre>"},{"location":"reference/api/#duckalog.quote_ident","title":"<code>quote_ident(value)</code>","text":"<p>Quote a SQL identifier using double quotes.</p> <p>This helper wraps a string in double quotes and escapes any embedded double quotes according to SQL rules.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Identifier to quote (for example, a view or column name).</p> required <p>Returns:</p> Type Description <code>str</code> <p>The identifier wrapped in double quotes.</p> Example <p>quote_ident(\"events\") '\"events\"'</p>"},{"location":"reference/api/#duckalog.render_options","title":"<code>render_options(options)</code>","text":"<p>Render a mapping of options into scan-function arguments.</p> <p>The resulting string is suitable for appending to a <code>*_scan</code> function call. Keys are sorted alphabetically to keep output deterministic.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>Dict[str, Any]</code> <p>Mapping of option name to value (str, bool, int, or float).</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string that starts with <code>,</code> when options are present (for example,</p> <code>str</code> <p><code>\", hive_partitioning=TRUE\"</code>) or an empty string when no options</p> <code>str</code> <p>are provided.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If a value has a type that cannot be rendered safely.</p>"},{"location":"reference/api/#duckalog.validate_config","title":"<code>validate_config(config_path)</code>","text":"<p>Validate a configuration file without touching DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the YAML/JSON configuration file.</p> required <p>Raises:</p> Type Description <code>ConfigError</code> <p>If the configuration file is missing, malformed, or does not satisfy the schema and interpolation rules.</p> Example <p>from duckalog import validate_config validate_config(\"catalog.yaml\")  # raises on invalid config</p>"}]}